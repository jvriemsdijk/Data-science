{
 "metadata": {
  "name": "",
  "signature": "sha256:5d7aeceb88aa7c860265258f38762927acf9fbef7f2ad572814433f61b851895"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## RSS parsing\n",
      "\n",
      "Make a notebook that performs the following steps.\n",
      "\n",
      "    1. Create a script that retrieves all urls of rss feeds from http://www.volkskrant.nl/nl/2764/rss/integration/nmc/frameset/footer/rss/rssFeeds.dhtml. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
      "\n",
      "    2. Download all rss feeds and store them on disk.\n",
      "\n",
      "    3. Parse all RSS feeds using lxml. Create a list of dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
      "\n",
      "    4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
      "\n",
      "    5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
      "\n",
      "    6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
      "\n",
      "    7. Count per day the number of words, and the number of unique words. Show this in a plot.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "from bs4 import UnicodeDammit\n",
      "from lxml import etree\n",
      "from lxml import objectify\n",
      "\n",
      "# read from the web\n",
      "url=\"http://static3.volkskrant.nl/static/nmc/nmc/frameset/footer/rss/rssFeeds.html?language=nl&pageType=integration&navigation=Voorpagina&language=nl&navigationItemId=20002\"\n",
      "f=urllib2.urlopen(url)\n",
      "\n",
      "soup = BeautifulSoup(f)\n",
      "links = []\n",
      "for link in soup.findAll(\"a\"):    \n",
      "    linktext = link.get(\"href\")\n",
      "    if linktext != None:\n",
      "        if linktext[-3:] == \"xml\":\n",
      "            links.append(UnicodeDammit(linktext).unicode_markup)\n",
      "\n",
      "\n",
      "fileNames = []            \n",
      "for linkUrl in links:\n",
      "    rssPage = urllib2.urlopen(linkUrl)\n",
      "    \n",
      "    fileName = \"Data/\"\n",
      "    fileName += linkUrl[25:-4].replace(\"/\", \"_\")\n",
      "    fileName += \".xml\"\n",
      "\n",
      "    fileNames.append(fileName)\n",
      "    \n",
      "    rssFile = open(fileName, 'w+')\n",
      "    \n",
      "    rssFile.writelines(rssPage.readlines())\n",
      "    rssFile.close()\n",
      "\n",
      "    \n",
      "xmlDictList = []\n",
      "\n",
      "for rssFileName in fileNames:\n",
      "    \n",
      "    rssFile = open(rssFileName, \"r\")\n",
      "    \n",
      "    # Now parse it to a tree\n",
      "    parsedRss = etree.parse(rssFile)\n",
      "    # use XPath to get the items\n",
      "    titles=parsed.xpath('//item//title')\n",
      "    titlestrings= [ t.text for t in titles]\n",
      "    titlestrings\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'parsed' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-b08e25d61439>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mparsedRss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrssFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# use XPath to get the items\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mtitles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparsed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//item//title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mtitlestrings\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtitles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mtitlestrings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'parsed' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## JSON parsing\n",
      "\n",
      "    1. Download http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb, remove all code blocks, and turn it into a notebook again.\n",
      "    \n",
      "    2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
      "    \n",
      "    3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb\"\n",
      "jsonfile= urllib2.urlopen(url)\n",
      "jsonfile = jsonfile.read()  # The jsonfile as a Python string\n",
      "json_as_python_object = json.loads(jsonfile) # The josnfile transformed into a Python dict\n",
      "\n",
      "# test \n",
      "jsonfile[:40],json_as_python_object.keys(),len(json_as_python_object), len(jsonfile),\n",
      "\n",
      "\n",
      "for code in soup.findAll(\"cell_type\"):\n",
      "    codetext = code.get(\"code\")\n",
      "    print codetext"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##PDF parsing\n",
      "\n",
      "    1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use difficult PDF. Why not try http://www.stdout.org/~winston/latex/latexsheet.pdf?\n",
      "\n",
      "    2. Is the word order still as it should be?\n",
      "    \n",
      "    3. What about the strange characters?\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}