{
 "metadata": {
  "name": "",
  "signature": "sha256:98caf753cfe1ee30d60628169bfb9c7711226a987f5266786847a3f93f6c77c1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## RSS parsing\n",
      "\n",
      "Make a notebook that performs the following steps.\n",
      "\n",
      "    1. Create a script that retrieves all urls of rss feeds from http://www.volkskrant.nl/nl/2764/rss/integration/nmc/frameset/footer/rss/rssFeeds.dhtml. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
      "\n",
      "    2. Download all rss feeds and store them on disk.\n",
      "\n",
      "    3. Parse all RSS feeds using lxml. Create a list of dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
      "\n",
      "    4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
      "\n",
      "    5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
      "\n",
      "    6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
      "\n",
      "    7. Count per day the number of words, and the number of unique words. Show this in a plot.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    from cStringIO import StringIO\n",
      "except:\n",
      "    from StringIO import StringIO\n",
      "import codecs\n",
      "    \n",
      "class UnicodeWriter:\n",
      "    \"\"\"\n",
      "    A CSV writer which will write rows to CSV file \"f\",\n",
      "    which is encoded in the given encoding. Heavily based\n",
      "    on the stackoverflow solution posted here:\n",
      "    \n",
      "    http://stackoverflow.com/questions/3085263/create-an-utf-8-csv-file-in-python\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, f, dialect=csv.excel, encoding=\"utf-8\", **kwds):\n",
      "        # Redirect output to a queue\n",
      "        self.queue = StringIO()\n",
      "        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)\n",
      "        self.stream = f\n",
      "        self.encoder = codecs.getincrementalencoder(encoding)()\n",
      "\n",
      "    def writerow(self, row):\n",
      "        self.writer.writerow([s.encode(\"utf-8\") for s in row])\n",
      "        # Fetch UTF-8 output from the queue ...\n",
      "        data = self.queue.getvalue()\n",
      "        data = data.decode(\"utf-8\")\n",
      "        # ... and reencode it into the target encoding\n",
      "        data = self.encoder.encode(data)\n",
      "        # write to the target stream\n",
      "        self.stream.write(data)\n",
      "        # empty queue\n",
      "        self.queue.truncate(0)\n",
      "\n",
      "    def writerows(self, rows):\n",
      "        for row in rows:\n",
      "            self.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "from bs4 import UnicodeDammit\n",
      "from lxml import etree\n",
      "from lxml import objectify\n",
      "import csv\n",
      "\n",
      "# 1.1\n",
      "url=\"http://static3.volkskrant.nl/static/nmc/nmc/frameset/footer/rss/rssFeeds.html?language=nl&pageType=integration&navigation=Voorpagina&language=nl&navigationItemId=20002\"\n",
      "f=urllib2.urlopen(url)\n",
      "\n",
      "soup = BeautifulSoup(f)\n",
      "links = []\n",
      "for link in soup.findAll(\"a\"):    \n",
      "    linktext = link.get(\"href\")\n",
      "    if linktext != None:\n",
      "        if linktext[-3:] == \"xml\":\n",
      "            links.append(UnicodeDammit(linktext).unicode_markup)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1.2            \n",
      "fileNames = []            \n",
      "for linkUrl in links:\n",
      "    rssPage = urllib2.urlopen(linkUrl)\n",
      "    \n",
      "    # write the files to the Data folder as xml and use the feedname as filename\n",
      "    fileName = \"Data/\"\n",
      "    fileName += linkUrl[25:-4].replace(\"/\", \"_\")\n",
      "    fileName += \".xml\"\n",
      "\n",
      "    fileNames.append(fileName)\n",
      "    \n",
      "    rssFile = open(fileName, 'w+')\n",
      "    \n",
      "    rssFile.writelines(rssPage.readlines())\n",
      "    rssFile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1.3    \n",
      "xmlDictList = []\n",
      "for rssFileName in fileNames:\n",
      "    \n",
      "    # open all the files\n",
      "    rssFile = open(rssFileName, \"r\")\n",
      "\n",
      "    # parse the xml\n",
      "    parsedRss = etree.parse(rssFile)\n",
      "    \n",
      "    # get the channel name from the filename\n",
      "    channel = rssFileName[5:-4]\n",
      "    \n",
      "    #get all rss items\n",
      "    items = parsedRss.xpath('//item')\n",
      "    \n",
      "    # make a dict for each rss item, and append to list\n",
      "    for item in items:\n",
      "        xmlDict = {}\n",
      "        xmlDict[\"title\"] = item.find(\"title\").text\n",
      "        xmlDict[\"channel\"] = channel\n",
      "        xmlDict[\"url\"] = item.find(\"link\").text\n",
      "        xmlDict[\"date\"] = item.find(\"pubDate\").text\n",
      "        xmlDictList.append(xmlDict)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1.4\n",
      "numberPerChannel = {}\n",
      "\n",
      "for xmlDict in xmlDictList:\n",
      "    \n",
      "    if xmlDict[\"channel\"] in numberPerChannel:\n",
      "    \n",
      "        numberPerChannel[xmlDict[\"channel\"]] += 1\n",
      "        \n",
      "    else:\n",
      "        \n",
      "        numberPerChannel[xmlDict[\"channel\"]] = 1\n",
      "\n",
      "nonDoubles = []\n",
      "doubles = 0\n",
      "for item in items:\n",
      "    if xmlDict[\"title\"] not in nonDoubles:\n",
      "        nonDoubles.append(xmlDict[\"title\"])\n",
      "        \n",
      "    else:\n",
      "        doubles += 1\n",
      "        \n",
      "    \n",
      "print \"Total number of rss items:\", len(xmlDictList)\n",
      "print numberPerChannel\n",
      "print \"Number of doubles:\", doubles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1.5\n",
      "with open('Data/xmlDictList.csv', 'w+') as csvFile:\n",
      "    \n",
      "\n",
      "    writer = UnicodeWriter(csvFile, delimiter =\",\",  quoting=csv.QUOTE_ALL)\n",
      "    writer.writerow(['title', 'channel', 'url', 'date'])\n",
      "    for xmlDict in xmlDictList:\n",
      "\n",
      "        writer.writerow([xmlDict['title'], xmlDict['channel'], xmlDict['url'], xmlDict['date']])\n",
      "    \n",
      "    csvFile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The google fusion table can be found [here](https://www.google.com/fusiontables/DataSource?docid=1dufhU66wjuY2Ufc3fvQs5mgON9Ysszhm8b8SmfbF)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1.6\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1.7\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import urllib2\n",
      "# from bs4 import BeautifulSoup\n",
      "# from bs4 import UnicodeDammit\n",
      "# from lxml import etree\n",
      "# from lxml import objectify\n",
      "# import csv\n",
      "\n",
      "# # 1.1\n",
      "# url=\"http://static3.volkskrant.nl/static/nmc/nmc/frameset/footer/rss/rssFeeds.html?language=nl&pageType=integration&navigation=Voorpagina&language=nl&navigationItemId=20002\"\n",
      "# f=urllib2.urlopen(url)\n",
      "\n",
      "# soup = BeautifulSoup(f)\n",
      "# links = []\n",
      "# for link in soup.findAll(\"a\"):    \n",
      "#     linktext = link.get(\"href\")\n",
      "#     if linktext != None:\n",
      "#         if linktext[-3:] == \"xml\":\n",
      "#             links.append(UnicodeDammit(linktext).unicode_markup)\n",
      "\n",
      "\n",
      "# # 1.2            \n",
      "# fileNames = []            \n",
      "# for linkUrl in links:\n",
      "#     rssPage = urllib2.urlopen(linkUrl)\n",
      "    \n",
      "#     # write the files to the Data folder as xml and use the feedname as filename\n",
      "#     fileName = \"Data/\"\n",
      "#     fileName += linkUrl[25:-4].replace(\"/\", \"_\")\n",
      "#     fileName += \".xml\"\n",
      "\n",
      "#     fileNames.append(fileName)\n",
      "    \n",
      "#     rssFile = open(fileName, 'w+')\n",
      "    \n",
      "#     rssFile.writelines(rssPage.readlines())\n",
      "#     rssFile.close()\n",
      "\n",
      "    \n",
      "    \n",
      "# # 1.3    \n",
      "# xmlDictList = []\n",
      "# for rssFileName in fileNames:\n",
      "    \n",
      "#     # open all the files\n",
      "#     rssFile = open(rssFileName, \"r\")\n",
      "\n",
      "#     # parse the xml\n",
      "#     parsedRss = etree.parse(rssFile)\n",
      "    \n",
      "#     # get the channel name from the filename\n",
      "#     channel = rssFileName[5:-4]\n",
      "    \n",
      "#     #get all rss items\n",
      "#     items = parsedRss.xpath('//item')\n",
      "    \n",
      "#     # make a dict for each rss item, and append to list\n",
      "#     for item in items:\n",
      "#         xmlDict = {}\n",
      "#         xmlDict[\"title\"] = item.find(\"title\").text\n",
      "#         xmlDict[\"channel\"] = channel\n",
      "#         xmlDict[\"url\"] = item.find(\"link\").text\n",
      "#         xmlDict[\"date\"] = item.find(\"pubDate\").text\n",
      "#         xmlDictList.append(xmlDict)\n",
      "\n",
      "# # 1.4\n",
      "# numberPerChannel = {}\n",
      "\n",
      "# for xmlDict in xmlDictList:\n",
      "    \n",
      "#     if xmlDict[\"channel\"] in numberPerChannel:\n",
      "    \n",
      "#         numberPerChannel[xmlDict[\"channel\"]] += 1\n",
      "        \n",
      "#     else:\n",
      "        \n",
      "#         numberPerChannel[xmlDict[\"channel\"]] = 1\n",
      "\n",
      "# nonDoubles = []\n",
      "# doubles = 0\n",
      "# for item in items:\n",
      "#     if xmlDict[\"title\"] not in nonDoubles:\n",
      "#         nonDoubles.append(xmlDict[\"title\"])\n",
      "        \n",
      "#     else:\n",
      "#         doubles += 1\n",
      "        \n",
      "    \n",
      "# print \"Total number of rss items:\", len(xmlDictList)\n",
      "# print numberPerChannel\n",
      "# print \"Number of doubles:\", doubles\n",
      "\n",
      "# #1.5\n",
      "# with open('Data/xmlDictList.csv', 'w+') as csvFile:\n",
      "    \n",
      "\n",
      "#     writer = UnicodeWriter(csvFile, delimiter =\",\",  quoting=csv.QUOTE_ALL)\n",
      "#     writer.writerow(['title', 'channel', 'url', 'date'])\n",
      "#     for xmlDict in xmlDictList:\n",
      "\n",
      "#         writer.writerow([xmlDict['title'], xmlDict['channel'], xmlDict['url'], xmlDict['date']])\n",
      "    \n",
      "#     csvFile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## JSON parsing\n",
      "\n",
      "    1. Download http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb, remove all code blocks, and turn it into a notebook again.\n",
      "    \n",
      "    2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
      "    \n",
      "    3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "import simplejson as json\n",
      "\n",
      "# 2.1 & prep for 2.3\n",
      "url = \"http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb\"\n",
      "jsonFile = urllib2.urlopen(url).read()\n",
      "jsonObj = json.loads(jsonFile)\n",
      "\n",
      "# create two lists: for code and non-code cells\n",
      "nonCodeCells = [c for c in jsonObj[\"worksheets\"][0][\"cells\"] if c[\"cell_type\"] != \"code\"]\n",
      "codeCells = [c for c in jsonObj[\"worksheets\"][0][\"cells\"] if c[\"cell_type\"] == \"code\"]\n",
      "\n",
      "# set the cells in the json object to only non code cells\n",
      "jsonObj[\"worksheets\"][0][\"cells\"] = nonCodeCells\n",
      "\n",
      "\n",
      "with open(\"JSON.ipynb\", \"w+\") as jsonIPynb:\n",
      "    json.dump(jsonObj, jsonIPynb)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2.2\n",
      "\n",
      "the code runs fine, to test for yourself open the notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 2.3\n",
      "with open(\"Data/JSONparsing.py\", \"w+\") as pythonFile:\n",
      "    for codeCell in codeCells:\n",
      "        pythonFile.writelines(codeCell[\"input\"])\n",
      "        pythonFile.write('\\n')\n",
      "    pythonFile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most of the code runs fine\n",
      "\n",
      "However at the last line we get the following:\n",
      "\n",
      ">strict_decoder\n",
      ">\n",
      ">^   \n",
      ">   \n",
      ">IndentationError: expected an indented block\n",
      "\n",
      "Ergo the code is not correct.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##PDF parsing\n",
      "\n",
      "    1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use difficult PDF. Why not try http://www.stdout.org/~winston/latex/latexsheet.pdf?\n",
      "\n",
      "    2. Is the word order still as it should be?\n",
      "    \n",
      "    3. What about the strange characters?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1.\n",
      "\n",
      "Since the following python solutions for pdf text extractions continued to result in errors:\n",
      "\n",
      "- [slate](https://pypi.python.org/pypi/slate)\n",
      "- [pdfminer](https://pypi.python.org/pypi/pdfminer/)\n",
      "- [pypdf](https://pypi.python.org/pypi/pyPdf/1.13) (discontinued and followup is a dead link)\n",
      "\n",
      "We have used the following [website](http://www.onlineocr.net/) to extract the text from the pdf files contained within our Data folder.\n",
      "\n",
      "Here the different markups were lost during the ocr as well as any linebreaks, leaving the text of the sample pdf in a single line. \n",
      "\n",
      "## 2. \n",
      "\n",
      "For latexsheet.pdf the word order was no longer as it should be. The word order was line by line with no regard for the colums.\n",
      "\n",
      "## 3.\n",
      "\n",
      "The special characters were interpreted as legal characters in utf-8, meaning they did not match the actual characters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# how it should work\n",
      "\n",
      "# import pdfminer\n",
      "# import slate\n",
      "# import io\n",
      "\n",
      "# pdfFileName = \"Data/PDF-Word.pdf\"\n",
      "\n",
      "# latexSheetFileName = \"Data/latexsheet.pdf\"\n",
      "\n",
      "# pdfFile = io.FileIO(latexSheetFileName, \"rb\")\n",
      "\n",
      "# print slate.PDF(pdfFile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# attempt 2\n",
      "\n",
      "# from pdfminer.pdfparser import PDFParser\n",
      "# from pdfminer.pdfdocument import PDFDocument\n",
      "# from pdfminer.pdfpage import PDFPage\n",
      "# from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
      "# from pdfminer.pdfinterp import PDFResourceManager\n",
      "# from pdfminer.pdfinterp import PDFPageInterpreter\n",
      "# from pdfminer.pdfdevice import PDFDevice\n",
      "# import io\n",
      "\n",
      "# # Open a PDF file.\n",
      "# fp = io.FileIO('Data/simple1.pdf', 'rb')\n",
      "# # Create a PDF parser object associated with the file object.\n",
      "# parser = PDFParser(fp)\n",
      "# # Create a PDF document object that stores the document structure.\n",
      "# # Supply the password for initialization.\n",
      "# document = PDFDocument(parser)\n",
      "# # Check if the document allows text extraction. If not, abort.\n",
      "# if not document.is_extractable:\n",
      "#     raise PDFTextExtractionNotAllowed\n",
      "# # Create a PDF resource manager object that stores shared resources.\n",
      "# rsrcmgr = PDFResourceManager()\n",
      "# # Create a PDF device object.\n",
      "# device = PDFDevice(rsrcmgr)\n",
      "# # Create a PDF interpreter object.\n",
      "# interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
      "# # Process each page contained in the document.\n",
      "\n",
      "\n",
      "# for page in PDFPage.create_pages(document):\n",
      "    \n",
      "    \n",
      "#     interpreter.process_page(page)\n",
      "    \n",
      "# print document.info\n",
      "# # print interpreter.get_current_state()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}