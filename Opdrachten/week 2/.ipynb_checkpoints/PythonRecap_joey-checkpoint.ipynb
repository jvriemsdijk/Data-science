{
 "metadata": {
  "name": "",
  "signature": "sha256:e12aec233ecf97f65369a4673e52c949669af50423e139ee9efb294bd241840b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Python Recap Exam\n",
      "\n",
      "This exam is meant to test how fluent or how rusty you are in Python. \n",
      "We do some simple things working with lists, counting things, and doing a bit of basic statistics.\n",
      "You may not remember everything at one. That is no problem, if you can reasonably fast find it back using Python's reference.\n",
      "\n",
      "Also, don't forget the great help that IPython can give you: TAB and ?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Start with running the following code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re,pprint,nltk\n",
      "\n",
      "# nltk.download()\n",
      "from __future__ import division\n",
      "from nltk.corpus import reuters \n",
      "# make a corpus of all words in the test files\n",
      "testIDs= [w for w in reuters.fileids() if w.startswith('test')]\n",
      "testWords=reuters.words(testIDs)\n",
      "testWords[:5]\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "[u'ASIAN', u'EXPORTERS', u'FEAR', u'DAMAGE', u'FROM']"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Questions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make the questions. \n",
      "Answer them  in the code block after the question. \n",
      "\n",
      "Reuse variables that you have defined in earlier questions in later questions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "(1) How many words/tokens are there in testWords? And how many unique tokens/words? What is the average frequency of a word?\n",
      "Define a variable for each subquestion.\n",
      "Then print out all variables at once. Use this style for the other exercises as well.\n",
      "So something like:\n",
      "```\n",
      "NumWords = ...\n",
      "NumUnique = ...\n",
      "AvgFreq = ....\n",
      "NumWords, NumUnique, AvgFreq\n",
      "```\n",
      "Or even better with a nicely formatted string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numWords = len(testWords)\n",
      "numUnique = len(set(testWords))\n",
      "avgFreq = numWords/numUnique\n",
      "\n",
      "print \"The total number of words is:\", numWords\n",
      "print \"The number of unique words is:\", numUnique\n",
      "print \"The average nu number of times a word occurs is:\", avgFreq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The total number of words is: 467205\n",
        "The number of unique words is: 22337\n",
        "The average nu number of times a word occurs is: 20.9161928639\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(2) How many bigrams are there in testWords? How many unique ones? What is the average bigram frequency? Explain the difference of the last two numbers with the numbers in the previous question. \n",
      "Make it easy for yourself. Just use the most \"dumb\" definition of bigram."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigrams = list(nltk.bigrams(testWords))\n",
      "numBigrams = len(bigrams)\n",
      "uniqueBigrams = len(set(bigrams))\n",
      "bigramAvgFreq = numBigrams/uniqueBigrams\n",
      "\n",
      "print numBigrams, uniqueBigrams, bigramAvgFreq\n",
      "print \"\"\"The difference in the number of unique bigrams can be explained by the number of combinations that is possible. Every word can be with another word, thus creating a larger number. The average frequency of bigrams is follows from that. As there are far more possibilities for bigrams, they are likely to come in smaller frequency.\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "467204 155763 2.99945429916\n",
        "The difference in the number of unique bigrams can be explained by the number of combinations that is possible. Every word can be with another word, thus creating a larger number. The average frequency of bigrams is follows from that. As there are far more possibilities for bigrams, they are likely to come in smaller frequency.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(3)  There are quite some tokens in testWords which are not really \"words\".\n",
      "\n",
      "Use [regular expressions](https://docs.python.org/2/library/re.html) and use the list of english stopwords which can be obtained as follows:\n",
      "\n",
      "```\n",
      "from nltk.corpus import stopwords\n",
      "# test\n",
      "stopwords.words('english')[:20]\n",
      "```\n",
      "\n",
      "Don't display too many digits behind the comma: use the `round()` function to control that.\n",
      "\n",
      "3.1. Create the list of all \"punctuation tokens\" in testWords.\n",
      "\n",
      "3.2. Create the list of all \"stopword tokens\" in testWords. Use NLTK's english stopword list.\n",
      "\n",
      "3.3. Compute the percentage of all tokens in testWords that is a punctuation and the percentage of all tokens in testWords that is a stopword.\n",
      "\n",
      "3.4. How many (as a percentage) of the UNIQUE tokens in testWords is a punctuation? How many a stopword?\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "\n",
      "stopWordsEng = stopwords.words(\"english\")\n",
      "\n",
      "punc = []\n",
      "stopWordsToken = []\n",
      "\n",
      "for word in testWords:\n",
      "    punc += re.findall(r'\\W+', word)\n",
      "    if word in stopWordsEng:\n",
      "        stopWordsToken.append(word)\n",
      "\n",
      "numTestWords = len(testWords)\n",
      "percentPunc = round((len(punc) /numTestWords), 4) * 100\n",
      "percentStop = round((len(stopWordsToken) / numTestWords), 4) * 100\n",
      "\n",
      "print \"The percentage of punctuation tokens in testWords:\", percentPunc, \"%\"\n",
      "print \"The percentage of stopwords in testWords:\", percentStop, \"%\"\n",
      "\n",
      "numUniqueTestWords = len(set(testWords))\n",
      "numUniquePunc = len(set(punc))\n",
      "numUniqueStop = len(set(stopWordsToken))\n",
      "percentUniquePunc = round(numUniquePunc / numUniqueTestWords, 4) * 100\n",
      "percentUniqueStop = round(numUniqueStop / numUniqueTestWords, 4) * 100\n",
      "\n",
      "print \"\\n\"\n",
      "print \"The percentage of punctuation in the unique tokens:\", percentUniquePunc, \"%\"\n",
      "print \"The percentage of stopwords in the unique tokens:\", percentUniqueStop, \"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The percentage of punctuation tokens in testWords: 14.82 %\n",
        "The percentage of stopwords in testWords: 22.63 %\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The percentage of punctuation in the unique tokens: 0.32 %\n",
        "The percentage of stopwords in the unique tokens: 0.55 %\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3.5 We will now start counting _how often_ words appear in the list `testWords`. Counting is a very important and often used tool. It is _expensive_ as it involves sorting.\n",
      "\n",
      "There are several ways to do counting, and we look at a few of them:\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The number of times the word \"the\" occurs\n",
      "the = [x for x in testWords if x=='the']\n",
      "len(the), len(set(the)), the[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "(15048, 1, [u'the', u'the', u'the', u'the', u'the'])"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Each list has a count method, which is ideal for counting\n",
      "testWords.count('the')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "15048"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This trick counts all items in one go and yields a dictionary\n",
      "from collections import Counter\n",
      "z = ['blue', 'red', 'blue', 'yellow', 'blue', 'red']\n",
      "Counter(z), Counter(z)['blue']\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "(Counter({'blue': 3, 'red': 2, 'yellow': 1}), 3)"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# with NLTK we can make a similar datastructure\n",
      "testfd= nltk.FreqDist(testWords)\n",
      "testfd.items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[(u'Durapipe', 1),\n",
        " (u'Irving', 17),\n",
        " (u'woods', 1),\n",
        " (u'hanging', 1),\n",
        " (u'HARDIE', 1)]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3.5.1 Use `set()` and dict comprehension to create a dict like `testfd` using the `.count()` method\n",
      "\n",
      "3.5.2 You now have 3 ways to make a wordcount dictionary. Use the timing functions to see which one is the fastest.\n",
      "\n",
      "3.5.3 Which percentage of the UNIQUE tokens in testWords is a hapax (i.e. occurs only once in testWords)?\n",
      "\n",
      "3.5.4 Which percentage of the   tokens in testWords is a hapax? \n",
      "\n",
      "3.5.5 Explain why the following test returns True: `len(testfd)==len(set(testWords))`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 3.5.1\n",
      "# commented out because the code takes WAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY too long\n",
      "# testWordsDict = {word: testWords.count(word) for word in set(testWords)}\n",
      "\n",
      "# below is a test case to show that the above code should work\n",
      "\n",
      "# testList = ['asd', 'asd', 'ewq', 'fdds', 1,2,3,4,5,6,7,8,9,0,9,8,7,6,4,6,8,6,4,34,6,8,67,4,4,1,3,2,43,6,23,765,4,4,4,656,456]\n",
      "# testDict = {word: testList.count(word) for word in set(testList)}\n",
      "\n",
      "# print testDict\n",
      "\n",
      "\n",
      "\n",
      "#3.5.2\n",
      "\n",
      "# the dict comp is not taken into account because of the reasons stated above\n",
      "\n",
      "import timeit\n",
      "\n",
      "setup = \"\"\"import re,pprint,nltk\n",
      "from collections import Counter\n",
      "from nltk.corpus import reuters \n",
      "testIDs= [w for w in reuters.fileids() if w.startswith('test')]\n",
      "testWords=reuters.words(testIDs)\"\"\"\n",
      "\n",
      "timeNLTK = timeit.timeit('nltk.FreqDist(testWords)', setup=setup, number=1)\n",
      "timeCounter = timeit.timeit('Counter(testWords)', setup=setup, number=1)\n",
      "\n",
      "print \"NLTK took:\", timeNLTK, \"seconds\"\n",
      "print \"Counter took:\", timeCounter, \"seconds\"\n",
      "print \"\\n\"\n",
      "\n",
      "\n",
      "\n",
      "#3.5.3\n",
      "numUniqueHapax = 0\n",
      "for key in testfd:\n",
      "    if testfd[key] == 1:\n",
      "        numUniqueHapax += 1\n",
      "\n",
      "print \"The percentage of UNIQUE hapax words in testWords is:\", round(numUniqueHapax / len(testfd), 4) * 100, \"%\"\n",
      "print \"\\n\"\n",
      "\n",
      "\n",
      "\n",
      "#3.5.4\n",
      "print \"The percentage of hapax words in testWords is:\", round(numUniqueHapax / len(testWords), 4) * 100, \"%\"\n",
      "print \"\\n\"\n",
      "\n",
      "\n",
      "#3.5.5\n",
      "# Because set(testWords) is all of the unique words in testWords, and testfd contains how often each word occurs in testWords. \n",
      "# Meaning the keyset of testfd is set(testWords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NLTK took: 1.30298755417 seconds\n",
        "Counter took: 1.29691643064 seconds\n",
        "\n",
        "\n",
        "The percentage of UNIQUE hapax words in testWords is: 40.11 %\n",
        "\n",
        "\n",
        "The percentage of hapax words in testWords is: 1.92 %\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(4) Count the tokens in testWords. Make an estimate of the following probability: given a string t, if we draw an arbitrary  token from testWords, what is the chance that it equals t?\n",
      "\n",
      "Program it as a function prob(str).\n",
      "Give an example of a high and of a low probability word.\n",
      "\n",
      "Does prob work on every input string?\n",
      "\n",
      "Test that 'the probabilities add up to 1'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# that depends on if t is found in testWords, if so: the chance is equal to: testWords.count(t) / len(testWords)\n",
      "\n",
      "# if not, very, very, very, very low chance, seeing that the number of possible strings is huge\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "mostLikelyWord = \"\"\n",
      "mostLikelyWordCount = 0\n",
      "\n",
      "for key in testfd:\n",
      "    if testfd[key] >= mostLikelyWordCount:\n",
      "        mostLikelyWord = key\n",
      "        mostLikelyWordCount = testfd[key]\n",
      "\n",
      "print \"The most likely token is:\", mostLikelyWord, \"and it occurs:\", mostLikelyWordCount, \"times\"\n",
      "\n",
      "\n",
      "def prob(str):\n",
      "    \"\"\"\n",
      "    Assumes that testWords is initialized\n",
      "    \"\"\"\n",
      "\n",
      "    return testWords.count(str) / len(testWords)\n",
      "\n",
      "      \n",
      "#test\n",
      "print prob(\"the\")\n",
      "print prob(\".\")\n",
      "print prob(\"keyboardfacerollstringdingeswaternietinstaat\")\n",
      "print \"\\n\"\n",
      "print prob(\" \")\n",
      "\n",
      "# prob test; takes way too long\n",
      "# totalChance = 0.0\n",
      "# for w in set(testWords):\n",
      "#     totalChance += prob(w)\n",
      "\n",
      "# print totalChance\n",
      "\n",
      "# the above should work however since the chance is testWords.count(t) / len(testWords) and the total count of each word is equal to \n",
      "# the length of testWords, this is equal to 1 / 1 = 1, hence 'the probabilities add up to 1'.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The most likely token is: . and it occurs: 27578 times\n",
        "0.0322085594118"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.0590276217078"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "\n",
        "0.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(5) Suppose we have printed out testWords  using exactly one space between each two tokens. Suppose we have covered a wall with this string. Now you throw a dart missile on this wall. Assume that it always hits exactly one character, which is either a space (inserted by our printing process) or a character in a token.\n",
      "\n",
      "5.1 What is the probability that it hits a space?\n",
      "\n",
      "5.2 Define a function prob(n) which for integer n returns the probability that the missile hits a token of length n.\n",
      "\n",
      "5.3 Nicely print out a table of the form \"n,  prob(n)\" which for each n returns   prob(n). Format it well, and truncate numbers. Bonus points for those who make a plot.\n",
      "\n",
      "5.3.1 Write a good test which indicates that prob(n) works correctly.\n",
      "\n",
      "5.4 What is the probability that it hits a stopword?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 5.1\n",
      "# Since the probability is equal to the surface area of a space character times the number of spaces (len(set(testWords)) - 1)\n",
      "# divided by the total surface area of all the characters we will have to make some assumtions because we have no knowledge\n",
      "# of the chosen font in which the characters are printed\n",
      "#\n",
      "# If however we assume that all characters have an equal surface area the chance to hit a space will become the \n",
      "# number of space characters (len(set(testWords)) - 1) / the total number of characters on the wall\n",
      "# Given this we will get:\n",
      "\n",
      "numOfSpaces = (len(set(testWords)) - 1)\n",
      "numOfChars = numOfSpaces\n",
      "for x in testWords:\n",
      "    numOfChars += len(x)\n",
      "\n",
      "print \"The chance to hit a space is:\", numOfSpaces / numOfChars\n",
      "print \"\\n\"\n",
      "\n",
      "\n",
      "# 5.2\n",
      "# Assuming that a space is not a token, and therefore not hit for n = 1\n",
      "\n",
      "# first setup a freq dist of testWords lengths; that way we dont have to remake this for each time prob(n) runs\n",
      "testWordsLengths = [len(x) for x in testWords]\n",
      "lengthsFreqDist = Counter(testWordsLengths)\n",
      "\n",
      "\n",
      "def prob(n):\n",
      "    \n",
      "    return lengthsFreqDist[n] / numOfChars\n",
      "    \n",
      "# test    \n",
      "# print prob(1)\n",
      "# print prob(2)\n",
      "# print \"\\n\"\n",
      "\n",
      "\n",
      "# 5.3\n",
      "import pandas\n",
      "% matplotlib inline\n",
      "import matplotlib.pyplot as plt  \n",
      "\n",
      "# dont forget to multiply the frequency by the number of characters\n",
      "charfd = [(i[0], ((i[1] * i[0]) / numOfChars)) for i in lengthsFreqDist.items()]\n",
      "    \n",
      "\n",
      "\n",
      "# 5.3.1\n",
      "sumProb = 0.0\n",
      "for i in charfd:\n",
      "    sumProb += i[1]\n",
      "\n",
      "print sumProb\n",
      "print (1 - numOfSpaces / numOfChars)\n",
      "\n",
      "# we round here to eliminate any float inaccuracies beyond 10 decimal places\n",
      "print round(sumProb, 10) == round((1 - numOfSpaces / numOfChars), 10)\n",
      "print \"\\n\"\n",
      "\n",
      "\n",
      "\n",
      "#5.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "print \"We print the table last because it wants that, and refuses to play otherwise\"\n",
      "pandas.DataFrame(charfd, columns=[\"n\", \"prob(n)\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The chance to hit a space is: 0.0120516641721\n",
        "\n",
        "\n",
        "0.987948335828"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.987948335828\n",
        "True\n",
        "\n",
        "\n",
        "We print the table last because it wants that, and refuses to play otherwise\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>n</th>\n",
        "      <th>prob(n)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>  1</td>\n",
        "      <td> 0.051770</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>  2</td>\n",
        "      <td> 0.079196</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>  3</td>\n",
        "      <td> 0.137742</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>  4</td>\n",
        "      <td> 0.129394</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>  5</td>\n",
        "      <td> 0.098648</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>  6</td>\n",
        "      <td> 0.097863</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>  7</td>\n",
        "      <td> 0.112205</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>  8</td>\n",
        "      <td> 0.096681</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>  9</td>\n",
        "      <td> 0.072185</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 10</td>\n",
        "      <td> 0.050481</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 11</td>\n",
        "      <td> 0.029617</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 12</td>\n",
        "      <td> 0.017165</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 13</td>\n",
        "      <td> 0.009946</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 14</td>\n",
        "      <td> 0.003256</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 15</td>\n",
        "      <td> 0.000866</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 16</td>\n",
        "      <td> 0.000328</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> 17</td>\n",
        "      <td> 0.000165</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 18</td>\n",
        "      <td> 0.000408</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> 19</td>\n",
        "      <td> 0.000010</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 20</td>\n",
        "      <td> 0.000011</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> 22</td>\n",
        "      <td> 0.000012</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "     n   prob(n)\n",
        "0    1  0.051770\n",
        "1    2  0.079196\n",
        "2    3  0.137742\n",
        "3    4  0.129394\n",
        "4    5  0.098648\n",
        "5    6  0.097863\n",
        "6    7  0.112205\n",
        "7    8  0.096681\n",
        "8    9  0.072185\n",
        "9   10  0.050481\n",
        "10  11  0.029617\n",
        "11  12  0.017165\n",
        "12  13  0.009946\n",
        "13  14  0.003256\n",
        "14  15  0.000866\n",
        "15  16  0.000328\n",
        "16  17  0.000165\n",
        "17  18  0.000408\n",
        "18  19  0.000010\n",
        "19  20  0.000011\n",
        "20  22  0.000012"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1aa2a550>"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "BONUS (if you have time left or got bored)\n",
      "\n",
      "Show that the  words in testWords have a Zipfian distribution. Count the words, order them by their frequency. Plot the log(frequency) times log(index of the word). \n",
      "\n",
      "Use `% matplotlib inline` to display the figure in the notebook.\n",
      "\n",
      "Is it a straight line?\n",
      "\n",
      "Now do the same for the unigrams and the bigrams together in one list. Is the plot \"better\"? What does 'better' mean here?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}