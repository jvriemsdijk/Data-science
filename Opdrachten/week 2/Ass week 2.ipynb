{
 "metadata": {
  "name": "",
  "signature": "sha256:e03468d218d8cee49881e09fb17c96a71f9d94665bcf1df5215c6fc23bfe20fa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## RSS parsing\n",
      "\n",
      "Make a notebook that performs the following steps.\n",
      "\n",
      "    1. Create a script that retrieves all urls of rss feeds from http://www.volkskrant.nl/nl/2764/rss/integration/nmc/frameset/footer/rss/rssFeeds.dhtml. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
      "\n",
      "    2. Download all rss feeds and store them on disk.\n",
      "\n",
      "    3. Parse all RSS feeds using lxml. Create a list of dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
      "\n",
      "    4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
      "\n",
      "    5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
      "\n",
      "    6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
      "\n",
      "    7. Count per day the number of words, and the number of unique words. Show this in a plot.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "from bs4 import UnicodeDammit\n",
      "from lxml import etree\n",
      "from lxml import objectify\n",
      "\n",
      "# 1.1\n",
      "url=\"http://static3.volkskrant.nl/static/nmc/nmc/frameset/footer/rss/rssFeeds.html?language=nl&pageType=integration&navigation=Voorpagina&language=nl&navigationItemId=20002\"\n",
      "f=urllib2.urlopen(url)\n",
      "\n",
      "soup = BeautifulSoup(f)\n",
      "links = []\n",
      "for link in soup.findAll(\"a\"):    \n",
      "    linktext = link.get(\"href\")\n",
      "    if linktext != None:\n",
      "        if linktext[-3:] == \"xml\":\n",
      "            links.append(UnicodeDammit(linktext).unicode_markup)\n",
      "\n",
      "\n",
      "# 1.2            \n",
      "fileNames = []            \n",
      "for linkUrl in links:\n",
      "    rssPage = urllib2.urlopen(linkUrl)\n",
      "    \n",
      "    # write the files to the Data folder as xml and use the feedname as filename\n",
      "    fileName = \"Data/\"\n",
      "    fileName += linkUrl[25:-4].replace(\"/\", \"_\")\n",
      "    fileName += \".xml\"\n",
      "\n",
      "    fileNames.append(fileName)\n",
      "    \n",
      "    rssFile = open(fileName, 'w+')\n",
      "    \n",
      "    rssFile.writelines(rssPage.readlines())\n",
      "    rssFile.close()\n",
      "\n",
      "    \n",
      "    \n",
      "# 1.3    \n",
      "xmlDictList = []\n",
      "for rssFileName in fileNames:\n",
      "    \n",
      "    # open all the files\n",
      "    rssFile = open(rssFileName, \"r\")\n",
      "\n",
      "    # parse the xml\n",
      "    parsedRss = etree.parse(rssFile)\n",
      "    \n",
      "    # get the channel name from the filename\n",
      "    channel = rssFileName[5:-4]\n",
      "    \n",
      "    #get all rss items\n",
      "    items = parsedRss.xpath('//item')\n",
      "    \n",
      "    # make a dict for each rss item, and append to list\n",
      "    for item in items:\n",
      "        xmlDict = {}\n",
      "        xmlDict[\"title\"] = item.find(\"title\").text\n",
      "        xmlDict[\"channel\"] = channel\n",
      "        xmlDict[\"url\"] = item.find(\"link\").text\n",
      "        xmlDict[\"date\"] = item.find(\"pubDate\").text\n",
      "        xmlDictList.append(xmlDict)\n",
      "\n",
      "        \n",
      "# 1.4\n",
      "numberPerChannel = {}\n",
      "\n",
      "for xmlDict in xmlDictList:\n",
      "    \n",
      "    if xmlDict[\"channel\"] in numberPerChannel:\n",
      "    \n",
      "        numberPerChannel[xmlDict[\"channel\"]] += 1\n",
      "        \n",
      "    else:\n",
      "        \n",
      "        numberPerChannel[xmlDict[\"channel\"]] = 1\n",
      "\n",
      "print \"Total number of rss items:\", len(xmlDictList)\n",
      "print numberPerChannel"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of rss items: 760\n",
        "{u'buitenland_rss': 40, u'rss': 40, u'video_opmerkelijk_rss': 40, u'tech-media_rss': 40, u'nieuws_reizen_rss': 40, u'video_binnenland_rss': 40, u'opinie_rss': 40, u'video_buitenland_rss': 40, u'nieuws_opmerkelijk_rss': 40, u'politiek_rss': 40, u'video_rss': 40, u'nieuws_rss': 40, u'nieuws_economie_rss': 40, u'sport_rss': 40, u'nieuws_gezondheidwetenschap_rss': 40, u'video_sport_rss': 40, u'nieuws_binnenland_rss': 40, u'video_cultuur_rss': 40, u'nieuws_cultuur_rss': 40}\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## JSON parsing\n",
      "\n",
      "    1. Download http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb, remove all code blocks, and turn it into a notebook again.\n",
      "    \n",
      "    2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
      "    \n",
      "    3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "import simplejson as json\n",
      "\n",
      "url = \"http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb\"\n",
      "jsonfile= urllib2.urlopen(url)\n",
      "jsonfile = jsonfile.read()  # The jsonfile as a Python string\n",
      "json_as_python_object = json.loads(jsonfile) # The jsonfile transformed into a Python dict\n",
      "\n",
      "# test \n",
      "#jsonfile[:80],json_as_python_object.keys(),len(json_as_python_object), len(jsonfile),\n",
      "\n",
      "\n",
      "#rint(json.dumps(json_as_python_object, sort_keys=True, indent=3)) ## Convert it back to a JSON string.\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ta\": {\n",
        "  \"name\": \"\",\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##PDF parsing\n",
      "\n",
      "    1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use difficult PDF. Why not try http://www.stdout.org/~winston/latex/latexsheet.pdf?\n",
      "\n",
      "    2. Is the word order still as it should be?\n",
      "    \n",
      "    3. What about the strange characters?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pdfFileName = \"Data/PDF-Word.pdf\"\n",
      "\n",
      "pdfFile = open(pdfFileName)\n",
      "\n",
      "print pdfFile.readlines()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['%PDF-1.4\\n', '%\\xc3\\xa4\\xc3\\xbc\\xc3\\xb6\\xc3\\x9f\\n', '2 0 obj\\n', '<</Length 3 0 R/Filter/FlateDecode>>\\n', 'stream\\n', \"x\\x9c\\xbdTMk\\xe40\\x0c\\xbd\\xe7W\\xf8\\\\H*\\xf9+1\\x04C2\\xcd,\\xf46\\xbb\\x81\\x1e\\xca\\x9e\\xb6\\x1f\\xb0l\\x0b\\xed\\xa5\\x7f\\x7fe)\\x1e'3m\\xe9\\xa9\\x0cdd[R\\xde{~\\n\", '4\\xa8\\xde\\xaa\\x17\\x05\\n', '(r\\xc15Zu\\x16\\x9bN\\xbd\\xdeW7\\x17\\xeaY\\xce\\xe8\\xf7\\xfaX\\x8ds\\xe5<\\x1d\\xb5\\xad\\xa5\\xe4\\xf9N]\\xeeQ\\xa1V\\xf3\\xc3m\\x0f\\x18k\\xdd\\x83\\x8e\\xf40\\xe9a%v\\xbcm\\xc1G\\xec\\xe5\\xd1\\xc6\\xbaM;]t=\\x04\\x18`\\xa4\\xc5N\\xce$\\x9b\\xeb\\xaf8\\x17\\xc6cv\\x9d7\\x97\\xf6\\x1cO\\xebw\\xecc\\x8d\\t\\x88\\xeb\\x118\\xb4\\x10\\x10Q\\xaf{O\\xb97j\\xee,\\x906h\\xd1\\xc4\\xdf\\xf3u5\\xcd\\xd5\\xe1]\\xf6\\xd67N\\xd8\\xeb3\\xf6\\x08\\t\\x9f\\xc4\\xb5O\\x08\\\\4\\x81O\\xe8]\\xf2G\\x028\\xa1Dia\\xc1>\\xc4.C\\xe4}\\xcf\\xe88\\x1c\\xa5lS+e\\xbb\\xb4u\\x05S\\xce\\xb3\\xf9\\x15\\xfb\\xcc\\x96\\x84 \\xaaH\\xc7\\xc4\\xf7SV\\xd85AX\\x99\\xc2*\\xb1H\\x8a\\x83\\xe5\\x7fw\\xbcD\\x8e\\xf2v\\x1b\\x19\\xb5d\\x069\\x93\\x93!\\x85#\\x87r\\xbe#\\xed?\\x83\\xe1\\x036\\xe6\\xc4ZhS\\x7f\\xba#trK\\x8c`\\x885)f\\x91\\x17\\x01\\xdb\\xad\\xc7h5\\xb1T|%\\xd8q\\xdc\\xb2u\\x96\\x05\\xe7\\x91\\x90\\xc9c\\xdb\\x8e\\x81o1\\x14{\\x98\\xe3K\\xb3wO}*f\\xdd']\n"
       ]
      }
     ],
     "prompt_number": 43
    }
   ],
   "metadata": {}
  }
 ]
}