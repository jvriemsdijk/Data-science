{
 "metadata": {
  "name": "",
  "signature": "sha256:1e2ef346534312acfa2d5e105092c7285c09968dffdfb85dd8b1f0a2ffb9824b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## RSS parsing\n",
      "\n",
      "Make a notebook that performs the following steps.\n",
      "\n",
      "    1. Create a script that retrieves all urls of rss feeds from http://www.volkskrant.nl/nl/2764/rss/integration/nmc/frameset/footer/rss/rssFeeds.dhtml. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
      "\n",
      "    2. Download all rss feeds and store them on disk.\n",
      "\n",
      "    3. Parse all RSS feeds using lxml. Create a list of dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
      "\n",
      "    4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
      "\n",
      "    5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
      "\n",
      "    6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
      "\n",
      "    7. Count per day the number of words, and the number of unique words. Show this in a plot.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "from bs4 import UnicodeDammit\n",
      "from lxml import etree\n",
      "from lxml import objectify\n",
      "\n",
      "# 1.1\n",
      "url=\"http://static3.volkskrant.nl/static/nmc/nmc/frameset/footer/rss/rssFeeds.html?language=nl&pageType=integration&navigation=Voorpagina&language=nl&navigationItemId=20002\"\n",
      "f=urllib2.urlopen(url)\n",
      "\n",
      "soup = BeautifulSoup(f)\n",
      "links = []\n",
      "for link in soup.findAll(\"a\"):    \n",
      "    linktext = link.get(\"href\")\n",
      "    if linktext != None:\n",
      "        if linktext[-3:] == \"xml\":\n",
      "            links.append(UnicodeDammit(linktext).unicode_markup)\n",
      "\n",
      "\n",
      "# 1.2            \n",
      "fileNames = []            \n",
      "for linkUrl in links:\n",
      "    rssPage = urllib2.urlopen(linkUrl)\n",
      "    \n",
      "    # write the files to the Data folder as xml and use the feedname as filename\n",
      "    fileName = \"Data/\"\n",
      "    fileName += linkUrl[25:-4].replace(\"/\", \"_\")\n",
      "    fileName += \".xml\"\n",
      "\n",
      "    fileNames.append(fileName)\n",
      "    \n",
      "    rssFile = open(fileName, 'w+')\n",
      "    \n",
      "    rssFile.writelines(rssPage.readlines())\n",
      "    rssFile.close()\n",
      "\n",
      "    \n",
      "    \n",
      "# 1.3    \n",
      "xmlDictList = []\n",
      "for rssFileName in fileNames:\n",
      "    \n",
      "    # open all the files\n",
      "    rssFile = open(rssFileName, \"r\")\n",
      "\n",
      "    # parse the xml\n",
      "    parsedRss = etree.parse(rssFile)\n",
      "    \n",
      "    # get the channel name from the filename\n",
      "    channel = rssFileName[5:-4]\n",
      "    \n",
      "    #get all rss items\n",
      "    items = parsedRss.xpath('//item')\n",
      "    \n",
      "    # make a dict for each rss item, and append to list\n",
      "    for item in items:\n",
      "        xmlDict = {}\n",
      "        xmlDict[\"title\"] = item.find(\"title\").text\n",
      "        xmlDict[\"channel\"] = channel\n",
      "        xmlDict[\"url\"] = item.find(\"link\").text\n",
      "        xmlDict[\"date\"] = item.find(\"pubDate\").text\n",
      "        xmlDictList.append(xmlDict)\n",
      "\n",
      "        \n",
      "# 1.4\n",
      "numberPerChannel = {}\n",
      "\n",
      "for xmlDict in xmlDictList:\n",
      "    \n",
      "    if xmlDict[\"channel\"] in numberPerChannel:\n",
      "    \n",
      "        numberPerChannel[xmlDict[\"channel\"]] += 1\n",
      "        \n",
      "    else:\n",
      "        \n",
      "        numberPerChannel\n",
      "\n",
      "print \"Total number of rss items:\", len(xmlDictList)\n",
      "print numberPerChannel"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "u'rss'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-39-5432016d0a52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxmlDict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxmlDictList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mnumberPerChannel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxmlDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"channel\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Total number of rss items:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxmlDictList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: u'rss'"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## JSON parsing\n",
      "\n",
      "    1. Download http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb, remove all code blocks, and turn it into a notebook again.\n",
      "    \n",
      "    2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
      "    \n",
      "    3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "import simplejson as json\n",
      "\n",
      "url = \"http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb\"\n",
      "jsonfile= urllib2.urlopen(url)\n",
      "jsonfile = jsonfile.read()  # The jsonfile as a Python string\n",
      "json_as_python_object = json.loads(jsonfile) # The jsonfile transformed into a Python dict\n",
      "\n",
      "# test \n",
      "#jsonfile[:80],json_as_python_object.keys(),len(json_as_python_object), len(jsonfile),\n",
      "\n",
      "\n",
      "#rint(json.dumps(json_as_python_object, sort_keys=True, indent=3)) ## Convert it back to a JSON string.\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ta\": {\n",
        "  \"name\": \"\",\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##PDF parsing\n",
      "\n",
      "    1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use difficult PDF. Why not try http://www.stdout.org/~winston/latex/latexsheet.pdf?\n",
      "\n",
      "    2. Is the word order still as it should be?\n",
      "    \n",
      "    3. What about the strange characters?\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}