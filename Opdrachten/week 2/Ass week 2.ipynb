{
 "metadata": {
  "name": "",
  "signature": "sha256:295182716ae423d521b42aa1ef098f09ed59a91626b792295a224cbe5f1010c5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## RSS parsing\n",
      "\n",
      "Make a notebook that performs the following steps.\n",
      "\n",
      "    1. Create a script that retrieves all urls of rss feeds from http://www.volkskrant.nl/nl/2764/rss/integration/nmc/frameset/footer/rss/rssFeeds.dhtml. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
      "\n",
      "    2. Download all rss feeds and store them on disk.\n",
      "\n",
      "    3. Parse all RSS feeds using lxml. Create a list of dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
      "\n",
      "    4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
      "\n",
      "    5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
      "\n",
      "    6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
      "\n",
      "    7. Count per day the number of words, and the number of unique words. Show this in a plot.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "from bs4 import UnicodeDammit\n",
      "from lxml import etree\n",
      "from lxml import objectify\n",
      "\n",
      "# read from the web\n",
      "url=\"http://static3.volkskrant.nl/static/nmc/nmc/frameset/footer/rss/rssFeeds.html?language=nl&pageType=integration&navigation=Voorpagina&language=nl&navigationItemId=20002\"\n",
      "f=urllib2.urlopen(url)\n",
      "\n",
      "soup = BeautifulSoup(f)\n",
      "links = []\n",
      "for link in soup.findAll(\"a\"):    \n",
      "    linktext = link.get(\"href\")\n",
      "    if linktext != None:\n",
      "        if linktext[-3:] == \"xml\":\n",
      "            links.append(UnicodeDammit(linktext).unicode_markup)\n",
      "\n",
      "\n",
      "fileNames = []            \n",
      "for linkUrl in links:\n",
      "    rssPage = urllib2.urlopen(linkUrl)\n",
      "    \n",
      "    fileName = \"Data/\"\n",
      "    fileName += linkUrl[25:-4].replace(\"/\", \"_\")\n",
      "    fileName += \".xml\"\n",
      "\n",
      "    fileNames.append(fileName)\n",
      "    \n",
      "    rssFile = open(fileName, 'w+')\n",
      "    \n",
      "    rssFile.writelines(rssPage.readlines())\n",
      "    rssFile.close()\n",
      "\n",
      "    \n",
      "xmlDictList = []\n",
      "\n",
      "for rssFileName in fileNames:\n",
      "    \n",
      "    rssFile = open(rssFileName, \"r\")\n",
      "    \n",
      "    # Now parse it to a tree\n",
      "    parsedRss = etree.parse(rssFile)\n",
      "    # use XPath to get the items\n",
      "    titles=parsed.xpath('//item//title')\n",
      "    titlestrings= [ t.text for t in titles]\n",
      "    titlestrings\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## JSON parsing\n",
      "\n",
      "    1. Download http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb, remove all code blocks, and turn it into a notebook again.\n",
      "    \n",
      "    2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
      "    \n",
      "    3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##PDF parsing\n",
      "\n",
      "    1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use difficult PDF. Why not try http://www.stdout.org/~winston/latex/latexsheet.pdf?\n",
      "\n",
      "    2. Is the word order still as it should be?\n",
      "    \n",
      "    3. What about the strange characters?\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}