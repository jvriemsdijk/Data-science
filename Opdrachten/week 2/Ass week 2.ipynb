{
 "metadata": {
  "name": "",
<<<<<<< HEAD
  "signature": "sha256:a1fde51a9f77d374971504cb8b09be0976685a82bf474941c4e0ca9028ee6f69"
=======
  "signature": "sha256:e87b93cb0124a7b15a6a967348f903083f17804b49c2897437cceeba59b85290"
>>>>>>> 9a9eb2efbcce66f72463724ed1acdd4945795053
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## RSS parsing\n",
      "\n",
      "Make a notebook that performs the following steps.\n",
      "\n",
      "    1. Create a script that retrieves all urls of rss feeds from http://www.volkskrant.nl/nl/2764/rss/integration/nmc/frameset/footer/rss/rssFeeds.dhtml. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
      "\n",
      "    2. Download all rss feeds and store them on disk.\n",
      "\n",
      "    3. Parse all RSS feeds using lxml. Create a list of dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
      "\n",
      "    4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
      "\n",
      "    5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
      "\n",
      "    6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
      "\n",
      "    7. Count per day the number of words, and the number of unique words. Show this in a plot.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "from bs4 import UnicodeDammit\n",
      "from lxml import etree\n",
      "from lxml import objectify\n",
      "import csv\n",
      "\n",
      "# 1.1\n",
      "url=\"http://static3.volkskrant.nl/static/nmc/nmc/frameset/footer/rss/rssFeeds.html?language=nl&pageType=integration&navigation=Voorpagina&language=nl&navigationItemId=20002\"\n",
      "f=urllib2.urlopen(url)\n",
      "\n",
      "soup = BeautifulSoup(f)\n",
      "links = []\n",
      "for link in soup.findAll(\"a\"):    \n",
      "    linktext = link.get(\"href\")\n",
      "    if linktext != None:\n",
      "        if linktext[-3:] == \"xml\":\n",
      "            links.append(UnicodeDammit(linktext).unicode_markup)\n",
      "\n",
      "\n",
      "# 1.2            \n",
      "fileNames = []            \n",
      "for linkUrl in links:\n",
      "    rssPage = urllib2.urlopen(linkUrl)\n",
      "    \n",
      "    # write the files to the Data folder as xml and use the feedname as filename\n",
      "    fileName = \"Data/\"\n",
      "    fileName += linkUrl[25:-4].replace(\"/\", \"_\")\n",
      "    fileName += \".xml\"\n",
      "\n",
      "    fileNames.append(fileName)\n",
      "    \n",
      "    rssFile = open(fileName, 'w+')\n",
      "    \n",
      "    rssFile.writelines(rssPage.readlines())\n",
      "    rssFile.close()\n",
      "\n",
      "    \n",
      "    \n",
      "# 1.3    \n",
      "xmlDictList = []\n",
      "for rssFileName in fileNames:\n",
      "    \n",
      "    # open all the files\n",
      "    rssFile = open(rssFileName, \"r\")\n",
      "\n",
      "    # parse the xml\n",
      "    parsedRss = etree.parse(rssFile)\n",
      "    \n",
      "    # get the channel name from the filename\n",
      "    channel = rssFileName[5:-4]\n",
      "    \n",
      "    #get all rss items\n",
      "    items = parsedRss.xpath('//item')\n",
      "    \n",
      "    # make a dict for each rss item, and append to list\n",
      "    for item in items:\n",
      "        xmlDict = {}\n",
      "        xmlDict[\"title\"] = item.find(\"title\").text\n",
      "        xmlDict[\"channel\"] = channel\n",
      "        xmlDict[\"url\"] = item.find(\"link\").text\n",
      "        xmlDict[\"date\"] = item.find(\"pubDate\").text\n",
      "        xmlDictList.append(xmlDict)\n",
      "\n",
      "# 1.4\n",
      "numberPerChannel = {}\n",
      "\n",
      "for xmlDict in xmlDictList:\n",
      "    \n",
      "    if xmlDict[\"channel\"] in numberPerChannel:\n",
      "    \n",
      "        numberPerChannel[xmlDict[\"channel\"]] += 1\n",
      "        \n",
      "    else:\n",
      "        \n",
      "        numberPerChannel[xmlDict[\"channel\"]] = 1\n",
      "\n",
      "nonDoubles = []\n",
      "doubles = 0\n",
      "for item in items:\n",
      "    if xmlDict[\"title\"] not in nonDoubles:\n",
      "        nonDoubles.append(xmlDict[\"title\"])\n",
      "        \n",
      "    else:\n",
      "        doubles += 1\n",
      "        \n",
      "    \n",
      "print \"Total number of rss items:\", len(xmlDictList)\n",
      "print numberPerChannel\n",
      "print \"Number of doubles:\", doubles\n",
      "\n",
      "#1.5\n",
      "with open('xmlDictList.csv', 'w+') as xmlDictList:\n",
      "    writer = csv.writer(xmlDictList, delimiter =\",\",  quoting=csv.QUOTE_ALL)\n",
      "    for xmlDict in xmlDictList:\n",
      "        for key in xmlDict:\n",
      "            writer.writerow([key, xmlDict[key]])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of rss items: 760\n",
        "{u'buitenland_rss': 40, u'rss': 40, u'video_opmerkelijk_rss': 40, u'tech-media_rss': 40, u'nieuws_reizen_rss': 40, u'video_binnenland_rss': 40, u'opinie_rss': 40, u'video_buitenland_rss': 40, u'nieuws_opmerkelijk_rss': 40, u'politiek_rss': 40, u'video_rss': 40, u'nieuws_rss': 40, u'nieuws_economie_rss': 40, u'sport_rss': 40, u'nieuws_gezondheidwetenschap_rss': 40, u'video_sport_rss': 40, u'nieuws_binnenland_rss': 40, u'video_cultuur_rss': 40, u'nieuws_cultuur_rss': 40}\n",
        "Number of doubles: 39\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## JSON parsing\n",
      "\n",
      "    1. Download http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb, remove all code blocks, and turn it into a notebook again.\n",
      "    \n",
      "    2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
      "    \n",
      "    3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "import simplejson as json\n",
      "\n",
      "url = \"http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb\"\n",
      "jsonfile= urllib2.urlopen(url)\n",
      "jsonfile = jsonfile.read()  # The jsonfile as a Python string\n",
      "json_as_python_object = json.loads(jsonfile) # The jsonfile transformed into a Python dict\n",
      "\n",
      "# test \n",
      "#jsonfile[:80],json_as_python_object.keys(),len(json_as_python_object), len(jsonfile),\n",
      "\n",
      "\n",
      "#rint(json.dumps(json_as_python_object, sort_keys=True, indent=3)) ## Convert it back to a JSON string.\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ta\": {\n",
        "  \"name\": \"\",\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##PDF parsing\n",
      "\n",
      "    1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use difficult PDF. Why not try http://www.stdout.org/~winston/latex/latexsheet.pdf?\n",
      "\n",
      "    2. Is the word order still as it should be?\n",
      "    \n",
      "    3. What about the strange characters?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}