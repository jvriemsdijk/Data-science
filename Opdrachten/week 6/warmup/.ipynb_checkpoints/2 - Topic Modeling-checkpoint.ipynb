{
 "metadata": {
  "name": "",
  "signature": "sha256:e5fee8384372e0dfb792aca0d2eb3927831e573e706f49a8329f00aa63650b39"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Topic Modeling for Fun and Profit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we'll\n",
      "\n",
      "* vectorize a streamed corpus\n",
      "* run topic modeling on streamed vectors, using gensim\n",
      "* explore how to choose, evaluate and tweak topic modeling parameters\n",
      "* persist trained models to disk, for later re-use"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous notebook `1 - Streamed Corpora` we used the 20newsgroups corpus to demonstrate data preprocessing and streaming.\n",
      "\n",
      "Now we'll switch to the English Wikipedia and do some topic modeling."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import and setup modules we'll be using in this notebook\n",
      "import logging\n",
      "import itertools\n",
      "\n",
      "import numpy as np\n",
      "import gensim\n",
      "\n",
      "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
      "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
      "\n",
      "def head(stream, n=10):\n",
      "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
      "    return list(itertools.islice(stream, n))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Wikipedia corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use the now-familiar pattern of streaming over an entire Wikipedia dump, without unzipping the raw file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.utils import smart_open, simple_preprocess\n",
      "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
      "from gensim.parsing.preprocessing import STOPWORDS\n",
      "\n",
      "def tokenize(text):\n",
      "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
      "\n",
      "def iter_wiki(dump_file):\n",
      "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
      "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
      "    for title, text, pageid in _extract_pages(smart_open(dump_file)):\n",
      "        text = filter_wiki(text)\n",
      "        tokens = tokenize(text)\n",
      "        if len(tokens) < 50 or any(title.startswith(ns + ':') for ns in ignore_namespaces):\n",
      "            continue  # ignore short articles and various meta-articles\n",
      "        yield title, tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls -lh data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total 1265824\r\n",
        "-rw-r--r--  1 admin  staff    14M Jan 22 17:27 20news-bydate.tar.gz\r\n",
        "-rw-r--r--  1 admin  staff   163B Jan 22 17:21 README.md\r\n",
        "-rw-r--r--  1 admin  staff   2.9M Jan 22 18:40 lda_wiki.model\r\n",
        "-rw-r--r--  1 admin  staff   2.0M Jan 22 18:40 lda_wiki.model.state\r\n",
        "-rw-r--r--  1 admin  staff   860K Jan 22 18:40 lsi_wiki.model\r\n",
        "-rw-r--r--  1 admin  staff    41M Jan 22 18:40 lsi_wiki.model.projection\r\n",
        "-rw-r--r--  1 admin  staff    95M Jan 22 17:31 simplewiki-20140623-pages-articles.xml.bz2\r\n",
        "-rw-r--r--  1 admin  staff   1.3M Jan 22 18:40 tfidf_wiki.model\r\n",
        "-rw-r--r--  1 admin  staff   860K Jan 22 18:40 wiki.dictionary\r\n",
        "-rw-r--r--  1 admin  staff    60M Jan 22 18:11 wiki_bow.mm\r\n",
        "-rw-r--r--  1 admin  staff   236K Jan 22 18:11 wiki_bow.mm.index\r\n",
        "-rw-r--r--  1 admin  staff    25M Jan 23 14:40 wiki_index.0\r\n",
        "-rw-r--r--  1 admin  staff    12M Jan 23 14:51 wiki_index.1\r\n",
        "-rw-r--r--  1 admin  staff   498B Jan 23 14:51 wiki_index.index\r\n",
        "-rw-r--r--  1 admin  staff   240M Jan 22 18:23 wiki_lsa.mm\r\n",
        "-rw-r--r--  1 admin  staff   236K Jan 22 18:23 wiki_lsa.mm.index\r\n",
        "-rw-r--r--  1 admin  staff   122M Jan 22 18:20 wiki_tfidf.mm\r\n",
        "-rw-r--r--  1 admin  staff   236K Jan 22 18:20 wiki_tfidf.mm.index\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# only use simplewiki in this tutorial (fewer documents)\n",
      "# the full wiki dump is exactly the same format, but larger\n",
      "stream = iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2')\n",
      "for title, tokens in itertools.islice(stream, 8):\n",
      "    print title, tokens[10:20]  # print the article title and its first ten tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "April [u'week', u'july', u'years', u'january', u'leap', u'years', u'april', u'flowers', u'sweet', u'pea']\n",
        "August [u'number', u'days', u'previous', u'month', u'july', u'named', u'roman', u'emperor', u'augustus', u'caesar']\n",
        "Art [u'human', u'senses', u'art', u'human', u'expresses', u'art', u'useful', u'practical', u'sense', u'sculptured']\n",
        "A [u'english', u'alphabet', u'small', u'letter', u'lower', u'case', u'vowel', u'english', u'long', u'said']\n",
        "Air [u'live', u'breathe', u'indefinite', u'shape', u'volume', u'color', u'smell', u'mass', u'weight', u'matter']\n",
        "Autonomous communities of Spain [u'executive', u'power', u'legislative', u'power', u'judicial', u'power', u'similar', u'states', u'united', u'states']\n",
        "Alan Turing [u'obe', u'frs', u'london', u'june', u'wilmslow', u'cheshire', u'june', u'english', u'mathematician', u'scientist']\n",
        "Alanis Morissette [u'singer', u'songwriter', u'born', u'ottawa', u'canada', u'began', u'singing', u'canada', u'teenager', u'popular']\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dictionaries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dictionaries are objects that map into raw text tokens (strings) from their numerical ids (integers). Example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "id2word = {0: u'word', 2: u'profit', 300: u'another_word'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This mapping step is technically (not conceptually) necessary because most algorithms rely on numerical libraries that work with vectors indexed by integers, rather than by strings, and have to know the vector/matrix dimensionality in advance.\n",
      "\n",
      "The mapping can be constructed automatically by giving `Dictionary` class a stream of tokenized documents:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_stream = (tokens for _, tokens in iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next command may take a while. Be patient, once in a while you kept some INFO output. \n",
      "We must parse 48356 documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
      "print(id2word_wiki)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary(148230 unique tokens: [u'fawn', u'\\u03c9\\u0431\\u0440\\u0430\\u0434\\u043e\\u0432\\u0430\\u043d\\u043d\\u0430\\u0467', u'refreshable', u'yollar\\u0131', u'idaira']...)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary(225175 unique tokens: [u'biennials', u'sowela', u'mdbg', u'clottes', u'idaira']...)\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dictionary object now contains all words that appeared in the corpus, along with how many times they appeared.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print the document-frequency for some words\n",
      "for i in range(10): print id2word_wiki.dfs[i], id2word_wiki.id2token[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " Let's filter out both very infrequent words and very frequent words (stopwords), to clear up resources as well as remove noise:\n",
      "\n",
      "But first let's see how many unique tokens we have at present."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(id2word_wiki)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(409123 unique tokens: [u'biennials', u'sowela', u'mdbg', u'biysk', u'sermersheim']...)\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ignore words that appear in less than 20 documents or more than 10% documents\n",
      "id2word_wiki.filter_extremes(no_below=20, no_above=0.1)\n",
      "print(id2word_wiki)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.dictionary:discarding 382482 tokens: [(u'alvares', 3), (u'ambedkar', 8), (u'american', 9760), (u'anzac', 19), (u'aperire', 1), (u'arbroath', 14), (u'born', 10651), (u'century', 4874), (u'chakri', 6), (u'city', 8139)]...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.dictionary:keeping 26641 tokens which were in no less than 20 and no more than 4832 (=10.0%) documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.dictionary:resulting dictionary: Dictionary(26641 unique tokens: [u'fawn', u'schlegel', u'sonja', u'woods', u'spiders']...)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(26641 unique tokens: [u'fawn', u'schlegel', u'sonja', u'woods', u'spiders']...)\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise (5 min)**: Print all words and their ids from `id2word_wiki` where the word starts with \"human\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " [ (c,w) for (c,w) in id2word_wiki.items() if w.startswith('human')][:10] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "[(10150, u'humanoid'),\n",
        " (10996, u'humanitarian'),\n",
        " (11915, u'humanity'),\n",
        " (13512, u'humankind'),\n",
        " (18876, u'humans'),\n",
        " (21804, u'human'),\n",
        " (21795, u'humanism'),\n",
        " (21797, u'humanist'),\n",
        " (24492, u'humanities')]"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note for advanced users**: In fully online scenarios, where the documents can only be streamed once (no repeating the stream), we can't exhaust the document stream just to build a dictionary. In this case we can map strings directly into their integer hash, using a hashing function such as MurmurHash or MD5. This is called the [\"hashing trick\"](http://en.wikipedia.org/wiki/Feature_hashing#Feature_vectorization_using_the_hashing_trick). A dictionary built this way is more difficult to debug, because there may be hash collisions: multiple words represented by a single id. See the documentation of [HashDictionary](http://radimrehurek.com/gensim/corpora/hashdictionary.html) for more details."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Vectorization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A streamed corpus and a dictionary is all we need to create [bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model) vectors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc = \"A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood.\"\n",
      "bow = id2word_wiki.doc2bow(tokenize(doc))\n",
      "print(bow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(10880, 2), (18117, 1), (21293, 1), (22824, 2)]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 5min:** What are the words in the array above? Why are the other words from the string missing?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "id2word_wiki[10882]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "u'bloom'"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's wrap the entire dump, as a stream of bag-of-word vectors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class WikiCorpus(object):\n",
      "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
      "        \"\"\"\n",
      "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
      "        Yield each document in turn, as a list of tokens (unicode strings).\n",
      "        \n",
      "        \"\"\"\n",
      "        self.dump_file = dump_file\n",
      "        self.dictionary = dictionary\n",
      "        self.clip_docs = clip_docs\n",
      "    \n",
      "    def __iter__(self):\n",
      "        self.titles = []\n",
      "        for title, tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
      "            self.titles.append(title)\n",
      "            yield self.dictionary.doc2bow(tokens)\n",
      "    \n",
      "    def __len__(self):\n",
      "        return self.clip_docs\n",
      "\n",
      "# create a stream of bag-of-words vectors\n",
      "wiki_corpus = WikiCorpus('./data/simplewiki-20140623-pages-articles.xml.bz2', id2word_wiki)\n",
      "vector = next(iter(wiki_corpus))\n",
      "print(vector)  # print the first vector in the stream"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(24, 1), (38, 1), (53, 1), (103, 1), (111, 1), (213, 3), (237, 1), (242, 2), (417, 1), (455, 3), (459, 1), (463, 1), (505, 1), (532, 1), (546, 3), (717, 1), (785, 1), (833, 1), (857, 2), (880, 1), (933, 2), (943, 1), (1190, 3), (1204, 1), (1232, 1), (1365, 1), (1425, 1), (1468, 2), (1476, 2), (1494, 2), (1539, 1), (1610, 1), (1731, 1), (1748, 1), (1822, 1), (1912, 3), (1952, 1), (2002, 1), (2041, 1), (2141, 1), (2142, 1), (2323, 1), (2335, 5), (2381, 9), (2389, 1), (2536, 1), (2575, 2), (2582, 1), (2601, 1), (2616, 1), (2656, 2), (2683, 1), (2720, 1), (2951, 1), (2965, 1), (3065, 2), (3181, 2), (3234, 1), (3330, 4), (3356, 1), (3358, 5), (3382, 1), (3411, 2), (3479, 1), (3483, 1), (3503, 1), (3541, 2), (3554, 2), (3574, 1), (3577, 1), (3597, 5), (3631, 1), (3635, 1), (3642, 1), (3667, 1), (3703, 1), (3746, 1), (3780, 1), (3792, 2), (3807, 2), (3813, 1), (3852, 1), (3946, 1), (3968, 4), (4003, 1), (4006, 1), (4017, 2), (4031, 2), (4047, 1), (4127, 1), (4196, 1), (4215, 1), (4218, 2), (4237, 1), (4253, 11), (4297, 1), (4349, 1), (4369, 1), (4387, 1), (4418, 1), (4438, 1), (4464, 1), (4495, 1), (4517, 4), (4557, 1), (4581, 2), (4586, 2), (4693, 1), (4697, 1), (4700, 1), (4702, 1), (4791, 1), (4852, 2), (4900, 8), (4906, 3), (5005, 2), (5060, 2), (5065, 1), (5115, 2), (5162, 1), (5182, 1), (5225, 2), (5261, 2), (5267, 2), (5380, 1), (5389, 1), (5430, 2), (5438, 2), (5508, 1), (5511, 1), (5525, 1), (5534, 2), (5607, 1), (5630, 1), (5640, 1), (5741, 1), (5760, 1), (5766, 3), (5772, 11), (5794, 1), (5803, 3), (5840, 2), (5851, 1), (5891, 1), (5974, 1), (6014, 1), (6120, 1), (6189, 1), (6198, 2), (6211, 1), (6226, 1), (6232, 1), (6241, 215), (6245, 1), (6246, 1), (6277, 1), (6357, 1), (6393, 1), (6543, 1), (6615, 1), (6628, 1), (6629, 1), (6714, 1), (6839, 4), (6881, 1), (6890, 2), (6906, 1), (6944, 4), (7033, 1), (7063, 1), (7082, 1), (7085, 2), (7251, 1), (7287, 1), (7342, 1), (7400, 8), (7460, 1), (7510, 1), (7541, 1), (7626, 1), (7663, 1), (7747, 1), (7838, 1), (7860, 3), (7898, 1), (8135, 3), (8174, 2), (8180, 1), (8185, 3), (8244, 1), (8254, 1), (8370, 1), (8429, 1), (8438, 1), (8466, 1), (8579, 5), (8608, 1), (8616, 1), (8621, 2), (8661, 1), (8673, 1), (8713, 1), (8721, 1), (8748, 2), (8759, 4), (8830, 1), (8879, 4), (8906, 1), (8954, 1), (8977, 1), (8979, 2), (9178, 3), (9198, 1), (9272, 1), (9378, 1), (9416, 4), (9422, 1), (9430, 1), (9431, 3), (9482, 3), (9493, 2), (9522, 1), (9544, 1), (9548, 1), (9758, 3), (9997, 1), (10066, 2), (10182, 6), (10186, 1), (10210, 1), (10269, 3), (10280, 1), (10283, 1), (10286, 1), (10306, 2), (10313, 4), (10376, 1), (10411, 4), (10482, 5), (10547, 1), (10574, 1), (10587, 1), (10588, 3), (10707, 1), (10713, 2), (10735, 1), (10779, 1), (10784, 2), (10848, 1), (10907, 1), (10958, 1), (10986, 1), (11007, 1), (11057, 1), (11058, 1), (11065, 1), (11146, 1), (11150, 1), (11164, 1), (11277, 1), (11350, 1), (11355, 1), (11465, 1), (11482, 1), (11553, 1), (11613, 1), (11722, 1), (11742, 1), (11772, 5), (11801, 1), (11828, 1), (11855, 2), (11897, 1), (11943, 1), (11968, 3), (11998, 3), (12055, 1), (12080, 1), (12091, 1), (12111, 1), (12183, 3), (12255, 6), (12286, 2), (12313, 2), (12353, 1), (12354, 6), (12373, 1), (12397, 2), (12512, 1), (12588, 1), (12597, 1), (12599, 1), (12604, 1), (12616, 1), (12617, 1), (12623, 1), (12704, 1), (12728, 2), (12768, 2), (12939, 1), (13018, 1), (13062, 1), (13161, 1), (13227, 2), (13286, 1), (13343, 1), (13448, 1), (13479, 1), (13501, 1), (13522, 3), (13545, 2), (13561, 6), (13580, 4), (13625, 1), (13628, 1), (13632, 1), (13664, 1), (13691, 2), (13713, 1), (13779, 1), (13821, 1), (13837, 1), (13854, 1), (13909, 1), (14038, 1), (14079, 3), (14143, 1), (14152, 1), (14157, 1), (14193, 1), (14318, 1), (14369, 1), (14377, 2), (14437, 3), (14453, 3), (14456, 1), (14599, 2), (14603, 1), (14680, 1), (14709, 1), (14773, 2), (14777, 1), (14813, 5), (14837, 1), (14841, 1), (14895, 1), (14906, 2), (14915, 3), (15004, 1), (15016, 1), (15037, 1), (15045, 1), (15090, 2), (15092, 1), (15135, 1), (15154, 1), (15177, 1), (15266, 2), (15273, 1), (15290, 1), (15297, 1), (15333, 4), (15376, 1), (15515, 3), (15541, 2), (15551, 1), (15610, 4), (15617, 1), (15621, 1), (15641, 2), (15764, 1), (15854, 2), (15871, 1), (15928, 2), (15979, 2), (16034, 5), (16051, 3), (16108, 1), (16253, 1), (16257, 1), (16315, 2), (16345, 1), (16385, 1), (16558, 2), (16566, 2), (16622, 2), (16685, 1), (16692, 1), (16703, 6), (16736, 1), (16884, 4), (16888, 1), (16942, 1), (16951, 1), (16996, 1), (17086, 1), (17157, 1), (17183, 1), (17188, 2), (17196, 1), (17241, 1), (17287, 1), (17361, 2), (17496, 1), (17515, 1), (17532, 1), (17703, 3), (17759, 1), (17896, 3), (17903, 8), (17915, 1), (17951, 2), (17962, 1), (17974, 1), (17979, 1), (18113, 1), (18212, 2), (18235, 1), (18319, 1), (18478, 2), (18513, 1), (18535, 3), (18538, 1), (18548, 1), (18558, 1), (18587, 1), (18590, 1), (18641, 3), (18707, 1), (18711, 1), (18721, 1), (18827, 1), (18832, 1), (18857, 1), (18868, 2), (18900, 2), (18939, 1), (18992, 1), (18997, 1), (19042, 1), (19050, 4), (19058, 1), (19094, 1), (19141, 1), (19161, 1), (19243, 1), (19253, 1), (19260, 1), (19289, 1), (19323, 2), (19374, 1), (19382, 1), (19404, 2), (19445, 2), (19576, 1), (19607, 1), (19641, 1), (19698, 3), (19720, 1), (19753, 3), (19796, 2), (19808, 1), (19826, 1), (20000, 1), (20002, 1), (20028, 1), (20065, 2), (20128, 2), (20220, 1), (20313, 1), (20378, 1), (20491, 2), (20498, 1), (20510, 1), (20570, 2), (20579, 1), (20663, 1), (20683, 2), (20859, 1), (20862, 1), (20898, 4), (20915, 1), (21037, 5), (21071, 3), (21139, 1), (21406, 3), (21448, 1), (21473, 1), (21491, 1), (21515, 2), (21526, 2), (21532, 1), (21551, 1), (21653, 1), (21684, 1), (21687, 1), (21720, 1), (21724, 2), (21731, 1), (21762, 6), (21804, 1), (21871, 3), (21906, 1), (21923, 1), (21932, 2), (21988, 1), (22022, 1), (22027, 1), (22045, 1), (22077, 1), (22079, 1), (22141, 1), (22142, 5), (22178, 1), (22346, 2), (22432, 1), (22503, 2), (22552, 1), (22572, 1), (22630, 2), (22632, 2), (22671, 1), (22676, 1), (22706, 1), (22709, 1), (22734, 1), (22780, 2), (22871, 1), (22919, 1), (22922, 2), (22945, 1), (23056, 1), (23096, 2), (23110, 1), (23185, 1), (23228, 1), (23254, 1), (23271, 4), (23351, 5), (23438, 2), (23439, 1), (23455, 1), (23487, 2), (23515, 2), (23536, 2), (23639, 1), (23649, 1), (23680, 1), (23682, 4), (23709, 1), (23719, 1), (23743, 1), (23800, 2), (23801, 2), (23977, 1), (24047, 1), (24099, 2), (24136, 1), (24150, 1), (24186, 1), (24213, 1), (24222, 1), (24240, 1), (24259, 1), (24273, 1), (24284, 1), (24340, 1), (24375, 1), (24407, 1), (24704, 1), (24718, 1), (24735, 1), (24831, 2), (24934, 1), (24967, 1), (25025, 1), (25026, 1), (25030, 1), (25032, 3), (25059, 1), (25083, 14), (25093, 2), (25107, 1), (25138, 1), (25177, 2), (25185, 3), (25281, 1), (25398, 1), (25408, 1), (25425, 3), (25427, 3), (25469, 10), (25547, 3), (25625, 2), (25626, 2), (25643, 1), (25661, 1), (25709, 1), (25729, 1), (25732, 2), (25745, 6), (25770, 1), (25784, 1), (25995, 1), (26014, 2), (26093, 1), (26159, 1), (26161, 1), (26248, 1), (26258, 1), (26283, 2), (26292, 2), (26341, 1), (26362, 1), (26368, 1), (26431, 1), (26548, 1), (26568, 1), (26576, 4), (26590, 2), (26622, 2)]\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print the same vector, but now with words instead of the word_ids\n",
      "print [(id2word_wiki.id2token[i],c) for i,c in vector]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'selassie', 1), (u'china', 1), (u'dna', 1), (u'effects', 1), (u'telescope', 1), (u'xvi', 3), (u'basque', 1), (u'union', 2), (u'catalonia', 1), (u'sierra', 3), (u'originally', 1), (u'following', 1), (u'drilling', 1), (u'snooker', 1), (u'saving', 3), (u'thailand', 1), (u'causes', 1), (u'november', 1), (u'beatrix', 2), (u'huge', 1), (u'occurs', 2), (u'benedict', 1), (u'land', 3), (u'sumter', 1), (u'derby', 1), (u'worldwide', 1), (u'nuclear', 1), (u'indonesia', 2), (u'clocks', 2), (u'zimbabwe', 2), (u'placed', 1), (u'referring', 1), (u'timothy', 1), (u'historical', 1), (u'climate', 1), (u'night', 3), (u'conservation', 1), (u'battle', 1), (u'franklin', 1), (u'fixed', 1), (u'transition', 1), (u'aphrodite', 1), (u'start', 5), (u'month', 9), (u'corps', 1), (u'horse', 1), (u'celebrated', 2), (u'rotterdam', 1), (u'culture', 1), (u'league', 1), (u'grand', 2), (u'showing', 1), (u'bangladesh', 1), (u'faroe', 1), (u'tens', 1), (u'lincoln', 2), (u'confederate', 2), (u'berlin', 1), (u'begins', 4), (u'marie', 1), (u'zealand', 5), (u'iceberg', 1), (u'russia', 2), (u'royal', 1), (u'overthrown', 1), (u'dead', 1), (u'landing', 2), (u'marking', 2), (u'father', 1), (u'sovereignty', 1), (u'international', 5), (u'aries', 1), (u'faith', 1), (u'making', 1), (u'lsd', 1), (u'tiki', 1), (u'nazi', 1), (u'sites', 1), (u'awareness', 2), (u'london', 2), (u'declares', 1), (u'declaration', 1), (u'arranged', 1), (u'founded', 4), (u'malaria', 1), (u'newspaper', 1), (u'africa', 2), (u'hour', 2), (u'robinson', 1), (u'showers', 1), (u'tennessee', 1), (u'elections', 1), (u'portugal', 2), (u'traffic', 1), (u'easter', 11), (u'planes', 1), (u'vietnamese', 1), (u'copenhagen', 1), (u'alois', 1), (u'holds', 1), (u'wedding', 1), (u'belgrade', 1), (u'reached', 1), (u'sweden', 4), (u'discovers', 1), (u'monarchs', 2), (u'armenia', 2), (u'introduces', 1), (u'commonwealth', 1), (u'female', 1), (u'waco', 1), (u'philippe', 1), (u'elizabeth', 2), (u'independence', 8), (u'days', 3), (u'discovery', 2), (u'ocean', 2), (u'proposed', 1), (u'duke', 2), (u'vietnam', 1), (u'row', 1), (u'birthstone', 2), (u'fourth', 2), (u'islamic', 2), (u'gulf', 1), (u'jazz', 1), (u'turkey', 2), (u'gagarin', 2), (u'luxembourg', 1), (u'buddha', 1), (u'rising', 1), (u'cultures', 2), (u'invention', 1), (u'christian', 1), (u'tragedy', 1), (u'friday', 1), (u'leading', 1), (u'saint', 3), (u'king', 11), (u'traditional', 1), (u'gains', 3), (u'sweet', 2), (u'flight', 1), (u'prince', 1), (u'peace', 1), (u'include', 1), (u'holiday', 1), (u'moveable', 1), (u'senegal', 2), (u'monuments', 1), (u'favor', 1), (u'opening', 1), (u'april', 215), (u'unclear', 1), (u'tunisia', 1), (u'buddhism', 1), (u'carries', 1), (u'theatre', 1), (u'truman', 1), (u'letter', 1), (u'bomb', 1), (u'reactor', 1), (u'chile', 1), (u'republic', 4), (u'bonfire', 1), (u'current', 2), (u'love', 1), (u'date', 4), (u'jefferson', 1), (u'carolina', 1), (u'affecting', 1), (u'uk', 2), (u'tanganyika', 1), (u'expedition', 1), (u'autumn', 1), (u'week', 8), (u'marriage', 1), (u'sex', 1), (u'hitting', 1), (u'bc', 1), (u'jewish', 1), (u'lee', 1), (u'merge', 1), (u'spill', 3), (u'muir', 1), (u'francisco', 3), (u'near', 2), (u'sinks', 1), (u'celebration', 3), (u'greek', 1), (u'saigon', 1), (u'dc', 1), (u'voice', 1), (u'confirmed', 1), (u'reichstag', 1), (u'flag', 5), (u'revolution', 1), (u'explosion', 1), (u'change', 2), (u'december', 1), (u'means', 1), (u'norway', 1), (u'half', 1), (u'fires', 2), (u'space', 4), (u'abraham', 1), (u'comes', 4), (u'kenya', 1), (u'martyrs', 1), (u'brighton', 1), (u'central', 2), (u'robert', 3), (u'ash', 1), (u'switzerland', 1), (u'finnish', 1), (u'france', 4), (u'heritage', 1), (u'mozambique', 1), (u'willem', 3), (u'juliana', 3), (u'titanic', 2), (u'cloud', 1), (u'founding', 1), (u'poetry', 1), (u'texas', 3), (u'partisans', 1), (u'swaziland', 2), (u'ii', 6), (u'il', 1), (u'rome', 1), (u'islands', 3), (u'form', 1), (u'ford', 1), (u'fort', 1), (u'ship', 2), (u'dies', 4), (u'wilkes', 1), (u'sunday', 4), (u'language', 5), (u'cambodian', 1), (u'calendar', 1), (u'jr', 1), (u'killing', 3), (u'broken', 1), (u'island', 2), (u'autism', 1), (u'rwanda', 1), (u'fall', 2), (u'town', 1), (u'late', 1), (u'kon', 1), (u'province', 1), (u'thomas', 1), (u'ireland', 1), (u'laos', 1), (u'realms', 1), (u'gambia', 1), (u'germany', 1), (u'forward', 1), (u'regions', 1), (u'red', 1), (u'likelihood', 1), (u'napoleon', 1), (u'shuttle', 1), (u'memphis', 1), (u'estimated', 1), (u'hirohito', 1), (u'got', 1), (u'western', 5), (u'passover', 1), (u'countries', 1), (u'nunavut', 2), (u'carnation', 1), (u'cooper', 1), (u'genocide', 3), (u'diamond', 3), (u'roosevelt', 1), (u'dublin', 1), (u'purchases', 1), (u'local', 1), (u'theory', 3), (u'falls', 6), (u'rms', 2), (u'pole', 2), (u'buys', 1), (u'events', 6), (u'harrison', 1), (u'pacific', 2), (u'worst', 1), (u'september', 1), (u'argentina', 1), (u'argentine', 1), (u'bigger', 1), (u'nepal', 1), (u'health', 1), (u'wisconsin', 1), (u'trafalgar', 1), (u'korea', 2), (u'togo', 2), (u'depicted', 1), (u'ramadan', 1), (u'boring', 1), (u'radiation', 1), (u'emperor', 2), (u'sinking', 1), (u'example', 1), (u'tree', 1), (u'came', 1), (u'radium', 1), (u'country', 3), (u'viii', 2), (u'european', 6), (u'previous', 4), (u'son', 1), (u'raises', 1), (u'america', 1), (u'taurus', 1), (u'pea', 2), (u'adolf', 1), (u'st', 1), (u'square', 1), (u'siege', 1), (u'open', 1), (u'liberation', 1), (u'cannabis', 1), (u'october', 3), (u'diary', 1), (u'harry', 1), (u'samoa', 1), (u'damien', 1), (u'jacinto', 1), (u'thor', 1), (u'church', 2), (u'australia', 3), (u'spring', 3), (u'palm', 1), (u'resulting', 2), (u'innocence', 1), (u'legendary', 1), (u'shortest', 1), (u'painting', 2), (u'bring', 1), (u'ends', 5), (u'louisiana', 1), (u'spain', 1), (u'commemoration', 1), (u'fools', 2), (u'daylight', 3), (u'pedro', 1), (u'jacob', 1), (u'frank', 1), (u'daughter', 1), (u'japan', 2), (u'bombing', 1), (u'created', 1), (u'albert', 1), (u'iceland', 1), (u'air', 2), (u'launched', 1), (u'birth', 1), (u'board', 1), (u'months', 4), (u'rwandan', 1), (u'january', 3), (u'carl', 2), (u'british', 1), (u'hemisphere', 4), (u'german', 1), (u'maine', 1), (u'celebrate', 2), (u'equivalent', 1), (u'iran', 2), (u'aimed', 1), (u'held', 2), (u'syria', 2), (u'northern', 5), (u'alexander', 3), (u'hoax', 1), (u'booth', 1), (u'mussolini', 1), (u'winter', 2), (u'serbia', 1), (u'gallipoli', 1), (u'freedom', 2), (u'washington', 2), (u'asian', 2), (u'newfoundland', 1), (u'bonaparte', 1), (u'president', 6), (u'mean', 1), (u'san', 4), (u'mcveigh', 1), (u'assassination', 1), (u'destroy', 1), (u'surrender', 1), (u'patriots', 1), (u'tbilisi', 1), (u'zurich', 1), (u'canada', 2), (u'dutch', 1), (u'broadcast', 1), (u'shot', 1), (u'oil', 2), (u'sung', 1), (u'dictatorship', 1), (u'jackie', 1), (u'common', 3), (u'replaces', 1), (u'pope', 3), (u'queen', 8), (u'married', 1), (u'festivals', 2), (u'iii', 1), (u'ascension', 1), (u'democracy', 1), (u'frederick', 1), (u'soviet', 2), (u'italian', 1), (u'bicycle', 1), (u'tanzania', 2), (u'report', 1), (u'southeast', 3), (u'goddess', 1), (u'hitler', 1), (u'kentucky', 1), (u'mother', 1), (u'seasonal', 1), (u'civil', 3), (u'cook', 1), (u'hawaii', 1), (u'invaded', 1), (u'benito', 1), (u'oklahoma', 1), (u'given', 1), (u'bounty', 2), (u'marathon', 2), (u'mormon', 1), (u'minister', 1), (u'mount', 1), (u'alphabetical', 1), (u'kills', 4), (u'falkland', 1), (u'signs', 1), (u'thursday', 1), (u'racing', 1), (u'parts', 1), (u'friendship', 1), (u'duchess', 1), (u'pablo', 1), (u'eruption', 2), (u'latin', 1), (u'roman', 1), (u'georgia', 2), (u'workers', 2), (u'settlers', 1), (u'patron', 1), (u'lech', 1), (u'july', 3), (u'emancipation', 1), (u'europe', 3), (u'present', 2), (u'unity', 1), (u'utrecht', 1), (u'holy', 1), (u'georgian', 1), (u'beauty', 1), (u'angola', 2), (u'boston', 2), (u'eastern', 1), (u'joseph', 1), (u'falling', 1), (u'massive', 2), (u'spanish', 1), (u'proving', 1), (u'going', 2), (u'prime', 1), (u'summer', 1), (u'meaning', 2), (u'plant', 1), (u'plane', 1), (u'england', 4), (u'major', 1), (u'kingdom', 5), (u'leone', 3), (u'morocco', 1), (u'denmark', 3), (u'chernobyl', 1), (u'paul', 1), (u'belgium', 1), (u'cove', 2), (u'catherine', 2), (u'walpole', 1), (u'planting', 1), (u'india', 1), (u'order', 1), (u'office', 1), (u'abdicated', 1), (u'abdicates', 2), (u'kim', 1), (u'forces', 6), (u'human', 1), (u'independent', 3), (u'picasso', 1), (u'baha', 1), (u'horizon', 2), (u'dance', 1), (u'assistance', 1), (u'inauguration', 1), (u'sighted', 1), (u'southern', 1), (u'heroes', 1), (u'thai', 1), (u'flowers', 5), (u'fletcher', 1), (u'massachusetts', 2), (u'zanzibar', 1), (u'daisy', 2), (u'scotland', 1), (u'mobile', 1), (u'mutiny', 2), (u'territory', 2), (u'poland', 1), (u'coup', 1), (u'pierre', 1), (u'erupts', 1), (u'cabral', 1), (u'william', 2), (u'sardinia', 1), (u'hubble', 1), (u'florida', 2), (u'exactly', 1), (u'began', 1), (u'martin', 2), (u'monday', 1), (u'astrological', 1), (u'force', 1), (u'luther', 1), (u'army', 4), (u'starts', 5), (u'brazil', 2), (u'said', 1), (u'aragon', 1), (u'mexico', 2), (u'claims', 2), (u'gustaf', 2), (u'suicide', 1), (u'word', 1), (u'french', 1), (u'earthquake', 4), (u'peru', 1), (u'fly', 1), (u'falklands', 1), (u'california', 2), (u'yuri', 2), (u'pan', 1), (u'spreading', 1), (u'children', 2), (u'movement', 1), (u'published', 1), (u'polynesian', 1), (u'good', 1), (u'exiled', 1), (u'commits', 1), (u'elba', 1), (u'youth', 1), (u'aged', 1), (u'barbados', 1), (u'italy', 1), (u'big', 1), (u'observed', 1), (u'demonstration', 1), (u'selection', 1), (u'orthodox', 2), (u'deaths', 1), (u'paris', 1), (u'phone', 1), (u'hofmann', 1), (u'henri', 1), (u'henry', 3), (u'executed', 1), (u'netherlands', 14), (u'celebrating', 2), (u'free', 1), (u'gabon', 1), (u'curie', 2), (u'earth', 3), (u'trivia', 1), (u'cyclone', 1), (u'baseball', 1), (u'memorial', 3), (u'australian', 3), (u'birthday', 10), (u'george', 3), (u'cities', 2), (u'come', 2), (u'peaceful', 1), (u'ethiopia', 1), (u'madrid', 1), (u'haile', 1), (u'cambridge', 2), (u'christianity', 6), (u'britain', 1), (u'championship', 1), (u'reach', 1), (u'hit', 2), (u'rig', 1), (u'anne', 1), (u'eggs', 1), (u'ukraine', 1), (u'apple', 1), (u'women', 2), (u'tax', 2), (u'crash', 1), (u'poets', 1), (u'york', 1), (u'thousands', 1), (u'branch', 1), (u'lead', 1), (u'leap', 4), (u'throne', 2), (u'book', 2)]\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what is the most common word in that first article?\n",
      "most_index, most_count = max(vector, key=lambda (word_index, count): count)\n",
      "print(id2word_wiki[most_index], most_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'april', 215)\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's store all those bag-of-words vectors into a file, so we don't have to parse the bzipped Wikipedia XML every time over and over:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time gensim.corpora.MmCorpus.serialize('./data/wiki_bow.mm', wiki_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.mmcorpus:storing corpus in Matrix Market format to ./data/wiki_bow.mm\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:saving sparse matrix to ./data/wiki_bow.mm\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #2000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #3000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #5000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #6000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #7000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #8000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #9000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #10000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #11000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #12000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #13000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #14000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #15000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #16000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #17000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #18000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #19000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #21000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #22000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #23000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #24000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #25000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #26000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #27000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #28000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #29000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #30000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #31000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #32000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #33000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #34000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #35000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #36000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #37000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #38000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #39000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #40000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #41000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #42000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #43000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #44000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #45000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #46000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #47000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #48000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:saved 48321x26641 matrix, density=0.368% (4739580/1287319761)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.indexedcorpus:saving MmCorpus index to ./data/wiki_bow.mm.index\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 5min 44s, sys: 2.87 s, total: 5min 46s\n",
        "Wall time: 5min 52s\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mm_corpus = gensim.corpora.MmCorpus('./data/wiki_bow.mm')\n",
      "print(mm_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.indexedcorpus:loaded corpus index from ./data/wiki_bow.mm.index\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:initializing corpus reader from ./data/wiki_bow.mm\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:accepted corpus with 48321 documents, 26641 features, 4739580 non-zero entries\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MmCorpus(48321 documents, 26641 features, 4739580 non-zero entries)\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`mm_corpus` now contains exactly the same bag-of-words vectors as `wiki_corpus` before, but they are backed by the `.mm` file, rather than extracted on the fly from the `xml.bz2` file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(next(iter(mm_corpus)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(24, 1.0), (38, 1.0), (53, 1.0), (103, 1.0), (111, 1.0), (213, 3.0), (237, 1.0), (242, 2.0), (417, 1.0), (455, 3.0), (459, 1.0), (463, 1.0), (505, 1.0), (532, 1.0), (546, 3.0), (717, 1.0), (785, 1.0), (833, 1.0), (857, 2.0), (880, 1.0), (933, 2.0), (943, 1.0), (1190, 3.0), (1204, 1.0), (1232, 1.0), (1365, 1.0), (1425, 1.0), (1468, 2.0), (1476, 2.0), (1494, 2.0), (1539, 1.0), (1610, 1.0), (1731, 1.0), (1748, 1.0), (1822, 1.0), (1912, 3.0), (1952, 1.0), (2002, 1.0), (2041, 1.0), (2141, 1.0), (2142, 1.0), (2323, 1.0), (2335, 5.0), (2381, 9.0), (2389, 1.0), (2536, 1.0), (2575, 2.0), (2582, 1.0), (2601, 1.0), (2616, 1.0), (2656, 2.0), (2683, 1.0), (2720, 1.0), (2951, 1.0), (2965, 1.0), (3065, 2.0), (3181, 2.0), (3234, 1.0), (3330, 4.0), (3356, 1.0), (3358, 5.0), (3382, 1.0), (3411, 2.0), (3479, 1.0), (3483, 1.0), (3503, 1.0), (3541, 2.0), (3554, 2.0), (3574, 1.0), (3577, 1.0), (3597, 5.0), (3631, 1.0), (3635, 1.0), (3642, 1.0), (3667, 1.0), (3703, 1.0), (3746, 1.0), (3780, 1.0), (3792, 2.0), (3807, 2.0), (3813, 1.0), (3852, 1.0), (3946, 1.0), (3968, 4.0), (4003, 1.0), (4006, 1.0), (4017, 2.0), (4031, 2.0), (4047, 1.0), (4127, 1.0), (4196, 1.0), (4215, 1.0), (4218, 2.0), (4237, 1.0), (4253, 11.0), (4297, 1.0), (4349, 1.0), (4369, 1.0), (4387, 1.0), (4418, 1.0), (4438, 1.0), (4464, 1.0), (4495, 1.0), (4517, 4.0), (4557, 1.0), (4581, 2.0), (4586, 2.0), (4693, 1.0), (4697, 1.0), (4700, 1.0), (4702, 1.0), (4791, 1.0), (4852, 2.0), (4900, 8.0), (4906, 3.0), (5005, 2.0), (5060, 2.0), (5065, 1.0), (5115, 2.0), (5162, 1.0), (5182, 1.0), (5225, 2.0), (5261, 2.0), (5267, 2.0), (5380, 1.0), (5389, 1.0), (5430, 2.0), (5438, 2.0), (5508, 1.0), (5511, 1.0), (5525, 1.0), (5534, 2.0), (5607, 1.0), (5630, 1.0), (5640, 1.0), (5741, 1.0), (5760, 1.0), (5766, 3.0), (5772, 11.0), (5794, 1.0), (5803, 3.0), (5840, 2.0), (5851, 1.0), (5891, 1.0), (5974, 1.0), (6014, 1.0), (6120, 1.0), (6189, 1.0), (6198, 2.0), (6211, 1.0), (6226, 1.0), (6232, 1.0), (6241, 215.0), (6245, 1.0), (6246, 1.0), (6277, 1.0), (6357, 1.0), (6393, 1.0), (6543, 1.0), (6615, 1.0), (6628, 1.0), (6629, 1.0), (6714, 1.0), (6839, 4.0), (6881, 1.0), (6890, 2.0), (6906, 1.0), (6944, 4.0), (7033, 1.0), (7063, 1.0), (7082, 1.0), (7085, 2.0), (7251, 1.0), (7287, 1.0), (7342, 1.0), (7400, 8.0), (7460, 1.0), (7510, 1.0), (7541, 1.0), (7626, 1.0), (7663, 1.0), (7747, 1.0), (7838, 1.0), (7860, 3.0), (7898, 1.0), (8135, 3.0), (8174, 2.0), (8180, 1.0), (8185, 3.0), (8244, 1.0), (8254, 1.0), (8370, 1.0), (8429, 1.0), (8438, 1.0), (8466, 1.0), (8579, 5.0), (8608, 1.0), (8616, 1.0), (8621, 2.0), (8661, 1.0), (8673, 1.0), (8713, 1.0), (8721, 1.0), (8748, 2.0), (8759, 4.0), (8830, 1.0), (8879, 4.0), (8906, 1.0), (8954, 1.0), (8977, 1.0), (8979, 2.0), (9178, 3.0), (9198, 1.0), (9272, 1.0), (9378, 1.0), (9416, 4.0), (9422, 1.0), (9430, 1.0), (9431, 3.0), (9482, 3.0), (9493, 2.0), (9522, 1.0), (9544, 1.0), (9548, 1.0), (9758, 3.0), (9997, 1.0), (10066, 2.0), (10182, 6.0), (10186, 1.0), (10210, 1.0), (10269, 3.0), (10280, 1.0), (10283, 1.0), (10286, 1.0), (10306, 2.0), (10313, 4.0), (10376, 1.0), (10411, 4.0), (10482, 5.0), (10547, 1.0), (10574, 1.0), (10587, 1.0), (10588, 3.0), (10707, 1.0), (10713, 2.0), (10735, 1.0), (10779, 1.0), (10784, 2.0), (10848, 1.0), (10907, 1.0), (10958, 1.0), (10986, 1.0), (11007, 1.0), (11057, 1.0), (11058, 1.0), (11065, 1.0), (11146, 1.0), (11150, 1.0), (11164, 1.0), (11277, 1.0), (11350, 1.0), (11355, 1.0), (11465, 1.0), (11482, 1.0), (11553, 1.0), (11613, 1.0), (11722, 1.0), (11742, 1.0), (11772, 5.0), (11801, 1.0), (11828, 1.0), (11855, 2.0), (11897, 1.0), (11943, 1.0), (11968, 3.0), (11998, 3.0), (12055, 1.0), (12080, 1.0), (12091, 1.0), (12111, 1.0), (12183, 3.0), (12255, 6.0), (12286, 2.0), (12313, 2.0), (12353, 1.0), (12354, 6.0), (12373, 1.0), (12397, 2.0), (12512, 1.0), (12588, 1.0), (12597, 1.0), (12599, 1.0), (12604, 1.0), (12616, 1.0), (12617, 1.0), (12623, 1.0), (12704, 1.0), (12728, 2.0), (12768, 2.0), (12939, 1.0), (13018, 1.0), (13062, 1.0), (13161, 1.0), (13227, 2.0), (13286, 1.0), (13343, 1.0), (13448, 1.0), (13479, 1.0), (13501, 1.0), (13522, 3.0), (13545, 2.0), (13561, 6.0), (13580, 4.0), (13625, 1.0), (13628, 1.0), (13632, 1.0), (13664, 1.0), (13691, 2.0), (13713, 1.0), (13779, 1.0), (13821, 1.0), (13837, 1.0), (13854, 1.0), (13909, 1.0), (14038, 1.0), (14079, 3.0), (14143, 1.0), (14152, 1.0), (14157, 1.0), (14193, 1.0), (14318, 1.0), (14369, 1.0), (14377, 2.0), (14437, 3.0), (14453, 3.0), (14456, 1.0), (14599, 2.0), (14603, 1.0), (14680, 1.0), (14709, 1.0), (14773, 2.0), (14777, 1.0), (14813, 5.0), (14837, 1.0), (14841, 1.0), (14895, 1.0), (14906, 2.0), (14915, 3.0), (15004, 1.0), (15016, 1.0), (15037, 1.0), (15045, 1.0), (15090, 2.0), (15092, 1.0), (15135, 1.0), (15154, 1.0), (15177, 1.0), (15266, 2.0), (15273, 1.0), (15290, 1.0), (15297, 1.0), (15333, 4.0), (15376, 1.0), (15515, 3.0), (15541, 2.0), (15551, 1.0), (15610, 4.0), (15617, 1.0), (15621, 1.0), (15641, 2.0), (15764, 1.0), (15854, 2.0), (15871, 1.0), (15928, 2.0), (15979, 2.0), (16034, 5.0), (16051, 3.0), (16108, 1.0), (16253, 1.0), (16257, 1.0), (16315, 2.0), (16345, 1.0), (16385, 1.0), (16558, 2.0), (16566, 2.0), (16622, 2.0), (16685, 1.0), (16692, 1.0), (16703, 6.0), (16736, 1.0), (16884, 4.0), (16888, 1.0), (16942, 1.0), (16951, 1.0), (16996, 1.0), (17086, 1.0), (17157, 1.0), (17183, 1.0), (17188, 2.0), (17196, 1.0), (17241, 1.0), (17287, 1.0), (17361, 2.0), (17496, 1.0), (17515, 1.0), (17532, 1.0), (17703, 3.0), (17759, 1.0), (17896, 3.0), (17903, 8.0), (17915, 1.0), (17951, 2.0), (17962, 1.0), (17974, 1.0), (17979, 1.0), (18113, 1.0), (18212, 2.0), (18235, 1.0), (18319, 1.0), (18478, 2.0), (18513, 1.0), (18535, 3.0), (18538, 1.0), (18548, 1.0), (18558, 1.0), (18587, 1.0), (18590, 1.0), (18641, 3.0), (18707, 1.0), (18711, 1.0), (18721, 1.0), (18827, 1.0), (18832, 1.0), (18857, 1.0), (18868, 2.0), (18900, 2.0), (18939, 1.0), (18992, 1.0), (18997, 1.0), (19042, 1.0), (19050, 4.0), (19058, 1.0), (19094, 1.0), (19141, 1.0), (19161, 1.0), (19243, 1.0), (19253, 1.0), (19260, 1.0), (19289, 1.0), (19323, 2.0), (19374, 1.0), (19382, 1.0), (19404, 2.0), (19445, 2.0), (19576, 1.0), (19607, 1.0), (19641, 1.0), (19698, 3.0), (19720, 1.0), (19753, 3.0), (19796, 2.0), (19808, 1.0), (19826, 1.0), (20000, 1.0), (20002, 1.0), (20028, 1.0), (20065, 2.0), (20128, 2.0), (20220, 1.0), (20313, 1.0), (20378, 1.0), (20491, 2.0), (20498, 1.0), (20510, 1.0), (20570, 2.0), (20579, 1.0), (20663, 1.0), (20683, 2.0), (20859, 1.0), (20862, 1.0), (20898, 4.0), (20915, 1.0), (21037, 5.0), (21071, 3.0), (21139, 1.0), (21406, 3.0), (21448, 1.0), (21473, 1.0), (21491, 1.0), (21515, 2.0), (21526, 2.0), (21532, 1.0), (21551, 1.0), (21653, 1.0), (21684, 1.0), (21687, 1.0), (21720, 1.0), (21724, 2.0), (21731, 1.0), (21762, 6.0), (21804, 1.0), (21871, 3.0), (21906, 1.0), (21923, 1.0), (21932, 2.0), (21988, 1.0), (22022, 1.0), (22027, 1.0), (22045, 1.0), (22077, 1.0), (22079, 1.0), (22141, 1.0), (22142, 5.0), (22178, 1.0), (22346, 2.0), (22432, 1.0), (22503, 2.0), (22552, 1.0), (22572, 1.0), (22630, 2.0), (22632, 2.0), (22671, 1.0), (22676, 1.0), (22706, 1.0), (22709, 1.0), (22734, 1.0), (22780, 2.0), (22871, 1.0), (22919, 1.0), (22922, 2.0), (22945, 1.0), (23056, 1.0), (23096, 2.0), (23110, 1.0), (23185, 1.0), (23228, 1.0), (23254, 1.0), (23271, 4.0), (23351, 5.0), (23438, 2.0), (23439, 1.0), (23455, 1.0), (23487, 2.0), (23515, 2.0), (23536, 2.0), (23639, 1.0), (23649, 1.0), (23680, 1.0), (23682, 4.0), (23709, 1.0), (23719, 1.0), (23743, 1.0), (23800, 2.0), (23801, 2.0), (23977, 1.0), (24047, 1.0), (24099, 2.0), (24136, 1.0), (24150, 1.0), (24186, 1.0), (24213, 1.0), (24222, 1.0), (24240, 1.0), (24259, 1.0), (24273, 1.0), (24284, 1.0), (24340, 1.0), (24375, 1.0), (24407, 1.0), (24704, 1.0), (24718, 1.0), (24735, 1.0), (24831, 2.0), (24934, 1.0), (24967, 1.0), (25025, 1.0), (25026, 1.0), (25030, 1.0), (25032, 3.0), (25059, 1.0), (25083, 14.0), (25093, 2.0), (25107, 1.0), (25138, 1.0), (25177, 2.0), (25185, 3.0), (25281, 1.0), (25398, 1.0), (25408, 1.0), (25425, 3.0), (25427, 3.0), (25469, 10.0), (25547, 3.0), (25625, 2.0), (25626, 2.0), (25643, 1.0), (25661, 1.0), (25709, 1.0), (25729, 1.0), (25732, 2.0), (25745, 6.0), (25770, 1.0), (25784, 1.0), (25995, 1.0), (26014, 2.0), (26093, 1.0), (26159, 1.0), (26161, 1.0), (26248, 1.0), (26258, 1.0), (26283, 2.0), (26292, 2.0), (26341, 1.0), (26362, 1.0), (26368, 1.0), (26431, 1.0), (26548, 1.0), (26568, 1.0), (26576, 4.0), (26590, 2.0), (26622, 2.0)]\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Semantic transformations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Topic modeling in gensim is realized via transformations. A transformation is something that takes a corpus and spits out another corpus on output, using `corpus_out = transformation_object[corpus_in]` syntax. What exactly happens in between is determined by what kind of transformation we're using -- options are Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), Random Projections (RP) etc.\n",
      "\n",
      "Some transformations need to be initialized (=trained) before they can be used. For example, let's train an LDA transformation model, using our bag-of-words WikiCorpus as training data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000)  # use fewer documents during training, LDA is slow\n",
      "# ClippedCorpus new in gensim 0.10.1\n",
      "# copy&paste it from https://github.com/piskvorky/gensim/blob/0.10.1/gensim/utils.py#L467 if necessary (or upgrade your gensim)\n",
      "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=10, id2word=id2word_wiki, passes=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:using symmetric alpha at 0.1\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:using serial LDA version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:running online LDA training, 10 topics, 4 passes over the supplied corpus of 4000 documents, updating model once every 2000 documents, evaluating perplexity every 4000 documents, iterating 50x with a convergence threshold of 0.001000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #2000/4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 4000 documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #0 (0.100): 0.004*bridge + 0.004*september + 0.003*countries + 0.003*example + 0.003*october + 0.002*france + 0.002*december + 0.002*league + 0.002*president + 0.002*actress\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.007*tower + 0.006*mast + 0.006*transmission + 0.003*country + 0.003*mario + 0.002*example + 0.002*things + 0.002*person + 0.002*language + 0.002*november\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.003*president + 0.003*actor + 0.003*british + 0.003*january + 0.003*french + 0.002*actress + 0.002*german + 0.002*king + 0.002*country + 0.002*government\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.005*lake + 0.004*bc + 0.003*president + 0.003*british + 0.003*singer + 0.002*ii + 0.002*actor + 0.002*england + 0.002*light + 0.002*language\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.004*july + 0.003*left + 0.003*align + 0.003*things + 0.003*language + 0.003*rgb + 0.003*hex + 0.002*word + 0.002*estimate + 0.002*league\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.004*british + 0.003*president + 0.003*government + 0.003*country + 0.003*countries + 0.003*tower + 0.003*party + 0.002*french + 0.002*king + 0.002*river\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.008*rgb + 0.007*hex + 0.004*april + 0.003*february + 0.003*color + 0.003*china + 0.003*light + 0.002*countries + 0.002*country + 0.002*things\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.003*french + 0.003*british + 0.003*german + 0.002*actress + 0.002*actor + 0.002*country + 0.002*president + 0.002*language + 0.002*king + 0.002*live\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.004*water + 0.003*december + 0.003*river + 0.003*music + 0.002*actor + 0.002*things + 0.002*jpg + 0.002*country + 0.002*german + 0.002*countries\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.003*august + 0.002*player + 0.002*person + 0.002*president + 0.002*god + 0.002*king + 0.002*country + 0.002*moon + 0.002*countries + 0.002*german\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic diff=3.363468, rho=1.000000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:-9.386 per-word bound, 669.3 perplexity estimate based on a held-out corpus of 2000 documents with 648119 words\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #4000/4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 4000 documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #0 (0.100): 0.004*september + 0.003*league + 0.003*bridge + 0.003*december + 0.003*october + 0.002*february + 0.002*august + 0.002*example + 0.002*penis + 0.002*park\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.005*tower + 0.005*country + 0.003*island + 0.003*game + 0.003*transmission + 0.003*mast + 0.003*population + 0.002*capital + 0.002*largest + 0.002*language\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.009*actor + 0.007*writer + 0.007*german + 0.007*british + 0.007*singer + 0.007*french + 0.007*president + 0.007*footballer + 0.006*actress + 0.006*politician\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.007*singer + 0.006*british + 0.006*player + 0.005*actor + 0.005*president + 0.005*footballer + 0.005*german + 0.004*actress + 0.004*politician + 0.004*writer\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.004*means + 0.003*word + 0.003*blood + 0.003*things + 0.003*language + 0.003*left + 0.003*person + 0.003*windows + 0.003*example + 0.002*body\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.006*president + 0.005*rural + 0.005*party + 0.004*british + 0.003*government + 0.003*country + 0.003*germany + 0.002*king + 0.002*countries + 0.002*kingdom\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.013*rgb + 0.013*hex + 0.007*color + 0.004*light + 0.004*blue + 0.003*red + 0.003*pink + 0.003*green + 0.003*april + 0.003*china\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.008*german + 0.008*french + 0.007*actor + 0.007*british + 0.007*actress + 0.006*singer + 0.006*footballer + 0.006*politician + 0.005*writer + 0.005*player\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.005*music + 0.004*water + 0.003*jpg + 0.003*river + 0.002*band + 0.002*things + 0.002*country + 0.002*december + 0.002*sea + 0.002*image\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.005*game + 0.003*player + 0.003*games + 0.003*fish + 0.003*person + 0.003*species + 0.002*live + 0.002*rock + 0.002*released + 0.002*players\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic diff=1.530483, rho=0.707107\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:PROGRESS: pass 1, at document #2000/4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 4000 documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #0 (0.100): 0.007*bridge + 0.006*league + 0.005*september + 0.004*december + 0.004*october + 0.004*park + 0.003*energy + 0.003*award + 0.003*august + 0.003*light\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.008*tower + 0.007*country + 0.006*mast + 0.006*transmission + 0.004*mario + 0.004*island + 0.003*capital + 0.003*countries + 0.003*government + 0.003*population\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.009*actor + 0.007*president + 0.007*singer + 0.007*british + 0.007*writer + 0.006*actress + 0.006*german + 0.006*footballer + 0.006*french + 0.006*politician\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.006*lake + 0.005*bc + 0.005*singer + 0.005*british + 0.004*president + 0.004*player + 0.004*ii + 0.004*actor + 0.004*german + 0.004*actress\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.005*word + 0.005*language + 0.005*things + 0.004*example + 0.004*means + 0.004*words + 0.003*person + 0.003*left + 0.003*earth + 0.003*windows\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.006*party + 0.006*government + 0.006*president + 0.005*countries + 0.005*country + 0.004*kingdom + 0.004*germany + 0.003*communist + 0.003*british + 0.003*union\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.017*rgb + 0.017*hex + 0.008*color + 0.006*light + 0.006*china + 0.005*blue + 0.005*april + 0.005*green + 0.004*february + 0.004*red\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.008*french + 0.007*german + 0.006*british + 0.006*actor + 0.006*actress + 0.005*singer + 0.005*politician + 0.005*footballer + 0.004*king + 0.004*italian\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.006*music + 0.006*water + 0.004*jpg + 0.004*river + 0.003*image + 0.003*things + 0.002*energy + 0.002*sea + 0.002*birds + 0.002*theory\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.004*game + 0.004*games + 0.004*god + 0.003*person + 0.003*species + 0.003*player + 0.003*cells + 0.003*fish + 0.003*cell + 0.002*live\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic diff=1.059690, rho=0.577350\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:-8.695 per-word bound, 414.4 perplexity estimate based on a held-out corpus of 2000 documents with 648119 words\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:PROGRESS: pass 1, at document #4000/4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 4000 documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #0 (0.100): 0.005*league + 0.005*bridge + 0.004*award + 0.004*september + 0.003*movie + 0.003*park + 0.003*film + 0.003*december + 0.003*october + 0.003*series\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.008*country + 0.005*tower + 0.005*island + 0.005*capital + 0.004*population + 0.004*largest + 0.004*mast + 0.003*transmission + 0.003*countries + 0.003*east\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.011*actor + 0.009*singer + 0.008*british + 0.008*writer + 0.008*german + 0.008*actress + 0.008*footballer + 0.008*president + 0.007*french + 0.007*politician\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.006*bc + 0.005*lake + 0.005*singer + 0.005*british + 0.004*ii + 0.004*player + 0.004*president + 0.004*movie + 0.004*german + 0.004*actor\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.005*means + 0.005*word + 0.005*things + 0.004*example + 0.004*person + 0.004*language + 0.003*words + 0.003*blood + 0.003*body + 0.003*windows\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.007*president + 0.006*party + 0.006*government + 0.005*rural + 0.004*countries + 0.004*germany + 0.004*country + 0.003*said + 0.003*kingdom + 0.003*house\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.020*rgb + 0.020*hex + 0.011*color + 0.007*light + 0.006*blue + 0.006*china + 0.005*green + 0.005*red + 0.005*pink + 0.004*purple\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.009*french + 0.009*german + 0.007*british + 0.007*actor + 0.007*actress + 0.006*footballer + 0.006*singer + 0.006*politician + 0.006*italian + 0.005*writer\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.008*music + 0.005*water + 0.005*jpg + 0.004*river + 0.004*band + 0.003*rock + 0.003*image + 0.003*file + 0.003*album + 0.002*live\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.006*game + 0.004*species + 0.004*games + 0.004*person + 0.004*player + 0.003*god + 0.003*fish + 0.003*live + 0.003*body + 0.003*cells\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic diff=0.756529, rho=0.500000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:PROGRESS: pass 2, at document #2000/4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 4000 documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #0 (0.100): 0.008*bridge + 0.007*league + 0.005*award + 0.005*september + 0.004*park + 0.004*movie + 0.004*energy + 0.003*december + 0.003*club + 0.003*october\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.008*country + 0.007*tower + 0.006*mast + 0.005*transmission + 0.005*capital + 0.005*island + 0.004*population + 0.004*largest + 0.004*countries + 0.004*mario\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.010*actor + 0.008*singer + 0.008*british + 0.008*writer + 0.008*actress + 0.008*president + 0.007*german + 0.007*footballer + 0.007*politician + 0.007*french\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.009*lake + 0.008*bc + 0.005*germany + 0.004*ii + 0.004*british + 0.004*france + 0.004*usa + 0.004*russia + 0.004*singer + 0.003*president\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.005*example + 0.005*things + 0.005*word + 0.005*language + 0.005*means + 0.004*person + 0.004*words + 0.003*earth + 0.003*internet + 0.003*windows\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.007*government + 0.007*president + 0.006*party + 0.006*countries + 0.005*country + 0.004*germany + 0.004*kingdom + 0.003*union + 0.003*said + 0.003*political\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.023*rgb + 0.022*hex + 0.012*color + 0.008*light + 0.008*china + 0.007*blue + 0.006*green + 0.005*red + 0.005*april + 0.005*japan\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.009*french + 0.008*german + 0.007*british + 0.006*actress + 0.006*actor + 0.006*king + 0.005*italian + 0.005*singer + 0.005*footballer + 0.005*politician\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.008*music + 0.006*water + 0.005*jpg + 0.004*river + 0.003*image + 0.003*energy + 0.003*file + 0.003*band + 0.003*rock + 0.002*things\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.005*game + 0.004*god + 0.004*games + 0.004*species + 0.004*person + 0.003*cells + 0.003*player + 0.003*body + 0.003*cell + 0.003*animals\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic diff=0.665657, rho=0.447214\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:-8.558 per-word bound, 377.0 perplexity estimate based on a held-out corpus of 2000 documents with 648119 words\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:PROGRESS: pass 2, at document #4000/4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 4000 documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #0 (0.100): 0.006*league + 0.006*award + 0.006*movie + 0.005*bridge + 0.004*film + 0.004*movies + 0.004*september + 0.004*series + 0.003*park + 0.003*star\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.008*country + 0.006*island + 0.006*capital + 0.005*tower + 0.005*population + 0.004*largest + 0.004*east + 0.004*countries + 0.004*language + 0.004*mast\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.011*actor + 0.009*singer + 0.009*british + 0.009*writer + 0.009*german + 0.008*actress + 0.008*footballer + 0.008*president + 0.008*politician + 0.007*french\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.008*lake + 0.008*bc + 0.004*germany + 0.004*ii + 0.004*british + 0.003*france + 0.003*russia + 0.003*movie + 0.003*usa + 0.003*al\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.005*example + 0.005*means + 0.005*things + 0.005*word + 0.005*person + 0.004*language + 0.004*words + 0.003*blood + 0.003*body + 0.003*windows\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.008*president + 0.007*government + 0.006*party + 0.005*countries + 0.005*rural + 0.005*germany + 0.004*country + 0.004*said + 0.003*kingdom + 0.003*political\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.025*rgb + 0.024*hex + 0.014*color + 0.008*light + 0.007*blue + 0.007*china + 0.007*green + 0.007*red + 0.006*pink + 0.005*purple\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.010*french + 0.009*german + 0.007*british + 0.006*king + 0.006*italian + 0.006*actress + 0.006*actor + 0.005*footballer + 0.005*politician + 0.005*ii\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.009*music + 0.006*jpg + 0.006*water + 0.005*band + 0.004*rock + 0.004*river + 0.004*album + 0.003*image + 0.003*file + 0.003*guitar\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.006*game + 0.005*species + 0.004*games + 0.004*person + 0.004*god + 0.004*player + 0.003*body + 0.003*live + 0.003*cells + 0.003*fish\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic diff=0.496846, rho=0.408248\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:PROGRESS: pass 3, at document #2000/4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 4000 documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #0 (0.100): 0.007*bridge + 0.007*league + 0.006*award + 0.006*movie + 0.004*september + 0.004*movies + 0.004*film + 0.004*series + 0.004*park + 0.004*club\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.009*country + 0.007*tower + 0.006*capital + 0.005*island + 0.005*mast + 0.005*transmission + 0.005*largest + 0.005*population + 0.004*countries + 0.004*east\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.011*actor + 0.009*singer + 0.009*british + 0.008*actress + 0.008*writer + 0.008*german + 0.008*president + 0.008*footballer + 0.007*politician + 0.007*french\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.012*lake + 0.010*bc + 0.006*germany + 0.006*usa + 0.005*russia + 0.005*france + 0.005*ii + 0.004*poland + 0.004*british + 0.003*russian\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.006*example + 0.005*things + 0.005*word + 0.005*means + 0.005*language + 0.005*person + 0.004*words + 0.003*earth + 0.003*internet + 0.003*windows\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.007*government + 0.007*president + 0.006*countries + 0.006*party + 0.005*country + 0.004*germany + 0.004*kingdom + 0.004*said + 0.003*political + 0.003*union\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.026*rgb + 0.025*hex + 0.014*color + 0.009*light + 0.009*china + 0.008*blue + 0.008*green + 0.006*red + 0.006*japan + 0.006*chinese\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.009*french + 0.008*german + 0.007*british + 0.007*king + 0.006*italian + 0.005*france + 0.005*actress + 0.005*ii + 0.005*actor + 0.005*henry\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.009*music + 0.007*water + 0.006*jpg + 0.004*river + 0.004*band + 0.004*rock + 0.003*image + 0.003*file + 0.003*energy + 0.003*album\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.006*game + 0.005*god + 0.004*species + 0.004*games + 0.004*person + 0.004*cells + 0.004*body + 0.003*animals + 0.003*player + 0.003*live\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic diff=0.455086, rho=0.377964\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:-8.496 per-word bound, 361.0 perplexity estimate based on a held-out corpus of 2000 documents with 648119 words\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:PROGRESS: pass 3, at document #4000/4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 4000 documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #0 (0.100): 0.007*movie + 0.006*award + 0.006*league + 0.005*bridge + 0.004*movies + 0.004*series + 0.004*film + 0.004*september + 0.004*album + 0.003*star\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.009*country + 0.006*capital + 0.006*island + 0.005*tower + 0.005*population + 0.005*largest + 0.004*east + 0.004*language + 0.004*cities + 0.004*countries\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.012*actor + 0.010*singer + 0.009*british + 0.009*writer + 0.009*actress + 0.009*german + 0.008*footballer + 0.008*politician + 0.008*president + 0.008*french\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.010*lake + 0.010*bc + 0.006*germany + 0.005*usa + 0.004*ii + 0.004*russia + 0.004*france + 0.004*al + 0.003*rape + 0.003*poland\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.006*example + 0.005*means + 0.005*things + 0.005*word + 0.005*person + 0.004*language + 0.004*words + 0.003*blood + 0.003*body + 0.003*windows\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.008*president + 0.007*government + 0.006*party + 0.005*countries + 0.004*germany + 0.004*country + 0.004*rural + 0.004*said + 0.003*political + 0.003*kingdom\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.027*rgb + 0.027*hex + 0.016*color + 0.009*light + 0.008*china + 0.008*blue + 0.008*green + 0.007*red + 0.006*pink + 0.005*purple\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.010*french + 0.009*german + 0.007*king + 0.007*british + 0.006*italian + 0.006*france + 0.005*ii + 0.005*henry + 0.005*actress + 0.004*actor\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.010*music + 0.007*band + 0.006*jpg + 0.006*water + 0.005*rock + 0.004*album + 0.004*river + 0.003*file + 0.003*image + 0.003*guitar\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.007*game + 0.005*species + 0.004*games + 0.004*god + 0.004*person + 0.004*body + 0.004*player + 0.003*cells + 0.003*animals + 0.003*live\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic diff=0.348969, rho=0.353553\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 3min 46s, sys: 1.58 s, total: 3min 48s\n",
        "Wall time: 3min 37s\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_ = lda_model.print_topics(-1)  # print a few most important words for each LDA topic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #0 (0.100): 0.007*movie + 0.006*award + 0.006*league + 0.005*bridge + 0.004*movies + 0.004*series + 0.004*film + 0.004*september + 0.004*album + 0.003*star\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.009*country + 0.006*capital + 0.006*island + 0.005*tower + 0.005*population + 0.005*largest + 0.004*east + 0.004*language + 0.004*cities + 0.004*countries\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.012*actor + 0.010*singer + 0.009*british + 0.009*writer + 0.009*actress + 0.009*german + 0.008*footballer + 0.008*politician + 0.008*president + 0.008*french\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.010*lake + 0.010*bc + 0.006*germany + 0.005*usa + 0.004*ii + 0.004*russia + 0.004*france + 0.004*al + 0.003*rape + 0.003*poland\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.006*example + 0.005*means + 0.005*things + 0.005*word + 0.005*person + 0.004*language + 0.004*words + 0.003*blood + 0.003*body + 0.003*windows\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.008*president + 0.007*government + 0.006*party + 0.005*countries + 0.004*germany + 0.004*country + 0.004*rural + 0.004*said + 0.003*political + 0.003*kingdom\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.027*rgb + 0.027*hex + 0.016*color + 0.009*light + 0.008*china + 0.008*blue + 0.008*green + 0.007*red + 0.006*pink + 0.005*purple\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.010*french + 0.009*german + 0.007*king + 0.007*british + 0.006*italian + 0.006*france + 0.005*ii + 0.005*henry + 0.005*actress + 0.004*actor\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.010*music + 0.007*band + 0.006*jpg + 0.006*water + 0.005*rock + 0.004*album + 0.004*river + 0.003*file + 0.003*image + 0.003*guitar\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.007*game + 0.005*species + 0.004*games + 0.004*god + 0.004*person + 0.004*body + 0.004*player + 0.003*cells + 0.003*animals + 0.003*live\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More info on model parameters in [gensim docs](http://radimrehurek.com/gensim/models/lsimodel.html)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Transformation can be stacked. For example, here we'll train a TFIDF model, and then train [Latent Semantic Analysis](http://en.wikipedia.org/wiki/Latent_semantic_analysis) on top of TFIDF:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time tfidf_model = gensim.models.TfidfModel(mm_corpus, id2word=id2word_wiki)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.tfidfmodel:collecting document frequencies\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #10000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #30000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #40000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.tfidfmodel:calculating IDF weights for 48321 documents and 26640 features (4739580 matrix non-zeros)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 38.7 s, sys: 264 ms, total: 39 s\n",
        "Wall time: 39.2 s\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The TFIDF transformation only modifies feature weights of each word. Its input and output dimensionality are identical (=the dictionary size)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time lsi_model = gensim.models.LsiModel(tfidf_model[mm_corpus], id2word=id2word_wiki, num_topics=200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:using serial LSI version on this node\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:updating model with new documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:1st phase: constructing (26641, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:orthonormalizing (26641, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (300, 20000) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:keeping 200 factors (discarding 15.269% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:processed documents up to #20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #0(15.142): 0.195*\"footballer\" + 0.181*\"actor\" + 0.172*\"german\" + 0.163*\"actress\" + 0.157*\"writer\" + 0.156*\"politician\" + 0.155*\"singer\" + 0.154*\"french\" + 0.146*\"british\" + 0.133*\"president\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #1(10.333): 0.206*\"footballer\" + -0.163*\"music\" + 0.161*\"actor\" + 0.156*\"politician\" + 0.148*\"actress\" + 0.136*\"writer\" + 0.118*\"singer\" + -0.110*\"band\" + -0.108*\"album\" + -0.106*\"district\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #2(8.766): -0.376*\"district\" + 0.248*\"music\" + -0.226*\"coat\" + -0.213*\"arms\" + 0.209*\"band\" + 0.207*\"album\" + -0.172*\"municipalities\" + -0.165*\"county\" + -0.160*\"river\" + -0.136*\"towns\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #3(7.974): 0.387*\"district\" + 0.269*\"coat\" + 0.253*\"arms\" + 0.232*\"band\" + 0.226*\"album\" + 0.204*\"music\" + 0.184*\"municipalities\" + 0.126*\"towns\" + 0.120*\"districts\" + 0.113*\"guitar\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #4(7.499): 0.327*\"league\" + 0.198*\"team\" + -0.182*\"music\" + -0.166*\"king\" + 0.162*\"football\" + 0.161*\"division\" + 0.140*\"nhl\" + 0.138*\"game\" + 0.136*\"season\" + 0.135*\"cup\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:1st phase: constructing (26641, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:orthonormalizing (26641, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (300, 20000) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:keeping 200 factors (discarding 13.799% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:merging projections: (26641, 200) + (26641, 200)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:keeping 200 factors (discarding 13.432% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:processed documents up to #40000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #0(18.623): 0.117*\"actor\" + 0.110*\"german\" + 0.106*\"music\" + 0.104*\"british\" + 0.102*\"actress\" + 0.101*\"french\" + 0.101*\"singer\" + 0.100*\"footballer\" + 0.097*\"league\" + 0.096*\"king\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #1(12.816): -0.569*\"league\" + -0.268*\"football\" + -0.233*\"team\" + -0.179*\"club\" + -0.164*\"premier\" + -0.162*\"division\" + -0.150*\"cup\" + -0.149*\"nhl\" + -0.129*\"championship\" + -0.128*\"played\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #2(12.588): 0.330*\"album\" + 0.256*\"band\" + 0.199*\"music\" + 0.163*\"released\" + 0.163*\"song\" + -0.159*\"footballer\" + 0.140*\"albums\" + 0.134*\"chart\" + -0.133*\"politician\" + 0.132*\"guitar\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #3(12.292): 0.391*\"river\" + 0.268*\"county\" + 0.168*\"district\" + -0.147*\"actor\" + -0.143*\"album\" + 0.142*\"province\" + -0.133*\"footballer\" + -0.131*\"actress\" + -0.130*\"singer\" + 0.110*\"jpg\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #4(10.995): -0.600*\"river\" + -0.322*\"county\" + 0.179*\"emperor\" + 0.131*\"month\" + 0.122*\"era\" + -0.121*\"album\" + 0.116*\"period\" + 0.102*\"library\" + 0.092*\"neng\u014d\" + -0.091*\"band\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:1st phase: constructing (26641, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:orthonormalizing (26641, 300) action matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (300, 8321) matrix\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:computing the final decomposition\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:keeping 200 factors (discarding 15.205% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:merging projections: (26641, 200) + (26641, 200)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:keeping 200 factors (discarding 9.895% of energy spectrum)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:processed documents up to #48321\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #0(20.126): 0.110*\"actor\" + 0.108*\"album\" + 0.105*\"music\" + 0.105*\"movie\" + 0.095*\"german\" + 0.095*\"british\" + 0.094*\"king\" + 0.092*\"actress\" + 0.091*\"singer\" + 0.089*\"league\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #1(13.799): 0.407*\"album\" + 0.288*\"band\" + -0.236*\"league\" + 0.220*\"released\" + 0.196*\"music\" + 0.190*\"song\" + 0.154*\"albums\" + 0.150*\"guitar\" + 0.144*\"chart\" + 0.126*\"vocals\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #2(13.716): -0.399*\"league\" + -0.222*\"team\" + -0.198*\"nhl\" + -0.186*\"football\" + -0.181*\"championship\" + -0.163*\"played\" + -0.149*\"hockey\" + -0.129*\"cup\" + -0.127*\"club\" + -0.123*\"album\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #3(12.901): 0.327*\"river\" + 0.246*\"county\" + -0.190*\"actor\" + -0.166*\"footballer\" + -0.163*\"actress\" + -0.138*\"politician\" + 0.137*\"district\" + -0.134*\"writer\" + -0.127*\"singer\" + 0.121*\"jpg\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #4(11.650): 0.339*\"wrestling\" + 0.319*\"championship\" + 0.267*\"match\" + 0.262*\"wwe\" + -0.234*\"river\" + -0.230*\"county\" + -0.222*\"league\" + 0.184*\"defeated\" + 0.173*\"tag\" + -0.130*\"album\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1min 44s, sys: 2.42 s, total: 1min 46s\n",
        "Wall time: 1min 34s\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi_model.print_topics()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #0(20.126): 0.110*\"actor\" + 0.108*\"album\" + 0.105*\"music\" + 0.105*\"movie\" + 0.095*\"german\" + 0.095*\"british\" + 0.094*\"king\" + 0.092*\"actress\" + 0.091*\"singer\" + 0.089*\"league\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #1(13.799): 0.407*\"album\" + 0.288*\"band\" + -0.236*\"league\" + 0.220*\"released\" + 0.196*\"music\" + 0.190*\"song\" + 0.154*\"albums\" + 0.150*\"guitar\" + 0.144*\"chart\" + 0.126*\"vocals\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #2(13.716): -0.399*\"league\" + -0.222*\"team\" + -0.198*\"nhl\" + -0.186*\"football\" + -0.181*\"championship\" + -0.163*\"played\" + -0.149*\"hockey\" + -0.129*\"cup\" + -0.127*\"club\" + -0.123*\"album\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #3(12.901): 0.327*\"river\" + 0.246*\"county\" + -0.190*\"actor\" + -0.166*\"footballer\" + -0.163*\"actress\" + -0.138*\"politician\" + 0.137*\"district\" + -0.134*\"writer\" + -0.127*\"singer\" + 0.121*\"jpg\"\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.models.lsimodel:topic #4(11.650): 0.339*\"wrestling\" + 0.319*\"championship\" + 0.267*\"match\" + 0.262*\"wwe\" + -0.234*\"river\" + -0.230*\"county\" + -0.222*\"league\" + 0.184*\"defeated\" + 0.173*\"tag\" + -0.130*\"album\"\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "[u'0.110*\"actor\" + 0.108*\"album\" + 0.105*\"music\" + 0.105*\"movie\" + 0.095*\"german\" + 0.095*\"british\" + 0.094*\"king\" + 0.092*\"actress\" + 0.091*\"singer\" + 0.089*\"league\"',\n",
        " u'0.407*\"album\" + 0.288*\"band\" + -0.236*\"league\" + 0.220*\"released\" + 0.196*\"music\" + 0.190*\"song\" + 0.154*\"albums\" + 0.150*\"guitar\" + 0.144*\"chart\" + 0.126*\"vocals\"',\n",
        " u'-0.399*\"league\" + -0.222*\"team\" + -0.198*\"nhl\" + -0.186*\"football\" + -0.181*\"championship\" + -0.163*\"played\" + -0.149*\"hockey\" + -0.129*\"cup\" + -0.127*\"club\" + -0.123*\"album\"',\n",
        " u'0.327*\"river\" + 0.246*\"county\" + -0.190*\"actor\" + -0.166*\"footballer\" + -0.163*\"actress\" + -0.138*\"politician\" + 0.137*\"district\" + -0.134*\"writer\" + -0.127*\"singer\" + 0.121*\"jpg\"',\n",
        " u'0.339*\"wrestling\" + 0.319*\"championship\" + 0.267*\"match\" + 0.262*\"wwe\" + -0.234*\"river\" + -0.230*\"county\" + -0.222*\"league\" + 0.184*\"defeated\" + 0.173*\"tag\" + -0.130*\"album\"']"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The LSI transformation goes from a space of high dimensionality (~TFIDF, tens of thousands) into a space of low dimensionality (a few hundreds; here 200). For this reason it can also seen as **dimensionality reduction**.\n",
      "\n",
      "As always, the transformations are applied \"lazily\", so the resulting output corpus is streamed as well:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(next(iter(lsi_model[tfidf_model[mm_corpus]])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.16645828715954131), (1, -0.036528886709846692), (2, 0.048606972979281605), (3, -0.04291054082082954), (4, 0.010676148240765413), (5, -0.0097369629015960912), (6, -0.048792873386556926), (7, -0.0010467738734802326), (8, 0.042209093588417811), (9, -0.0226104153965801), (10, 0.003908673757353774), (11, 0.028489910224010439), (12, 0.0015119361502735535), (13, -0.027741531715974651), (14, -0.055003086181626422), (15, -0.0041684997033909314), (16, 0.034289824777900151), (17, 0.03900500211436838), (18, 0.0019870836521118243), (19, -0.016673752549155735), (20, -0.0054021656979816989), (21, 0.025298313053604686), (22, -0.025339002729333485), (23, -0.03854843943197088), (24, -0.059685170311517197), (25, 0.031062084574129636), (26, 0.028735578782274323), (27, 0.050792732438823561), (28, -0.0058326954761527116), (29, -0.083506208021071046), (30, 0.045667411313380826), (31, 0.057059033685801612), (32, 0.038569600646630432), (33, 0.0027543807031154923), (34, -0.050189600482334604), (35, -0.061642045264637527), (36, -0.0074897695288217818), (37, -0.02358306709771351), (38, -0.052368365041281041), (39, -0.013540960098038908), (40, -0.020168542058167444), (41, 0.067244944714236268), (42, -0.012884098406377744), (43, -0.036588258036710918), (44, -0.022088414915462217), (45, -0.030191663213487457), (46, -0.00033755599272608104), (47, -0.045172789066461598), (48, -0.036004519205358), (49, -0.011009781422804048), (50, 0.018943683838415198), (51, 0.0053437104754067962), (52, 0.036627833606423327), (53, 0.028477350152056189), (54, -0.015994294747627737), (55, -0.039145302287964936), (56, -0.0082612297049230541), (57, 0.049994505315530569), (58, 0.025993049962877438), (59, -0.018704168620375737), (60, 0.014880963771845702), (61, -0.021721237189144614), (62, 0.0039976957536522967), (63, 0.043387972482296165), (64, 0.034214052183914274), (65, 0.038974102606136617), (66, 0.021877319637720539), (67, -0.014887822484568883), (68, 0.023921282302511851), (69, -0.013686980454564816), (70, 0.00050086627569834554), (71, -0.010204614100947492), (72, -0.010862523252916652), (73, -0.015638084108285946), (74, -0.012218478215420055), (75, 0.039752672599680336), (76, -0.011406119897919183), (77, 0.00831622506877246), (78, -0.017040374845611835), (79, -0.010241873726213246), (80, 0.017435918379066401), (81, -0.027292341543153593), (82, -0.023025107266090753), (83, -0.0021892401746491964), (84, -0.0055398998597568135), (85, 0.023182186768404146), (86, 0.038752361548497975), (87, -0.0084480548155517407), (88, -0.020806823647103238), (89, -0.019677186543207392), (90, -0.0097613960496529817), (91, 0.011569479056623564), (92, 0.022422512107486869), (93, 0.038007817987305119), (94, -0.034482949503871395), (95, -0.023855408026762191), (96, -0.003487437665702153), (97, -0.017213544279613847), (98, -0.010484383198744906), (99, -0.002870217834004938), (100, 0.0037366848534289591), (101, 0.0074412338537837106), (102, 0.017612295879007311), (103, -0.012479487284519015), (104, 0.018686673853586577), (105, -0.0177602288924841), (106, -0.043823928133654468), (107, 0.012903996999171771), (108, 0.0071442482954594362), (109, -0.015072471163020133), (110, 0.01104940035411028), (111, 0.0066321684203351251), (112, -0.018145300658772307), (113, -0.026766724628452289), (114, 0.0082107950023034212), (115, 0.0076083034101620045), (116, -0.008493735715271046), (117, -0.045612497229710544), (118, -0.0004983287751953307), (119, 0.00019617488849961272), (120, -0.022183279320049744), (121, 0.012276885704566209), (122, -0.025250660843856548), (123, -0.0090503815484492406), (124, -0.010377266741942863), (125, -0.037125738025727828), (126, -0.045172905723156909), (127, -0.022268152179869782), (128, 0.020894938720307125), (129, 0.0030367653810833366), (130, 0.01565495776306482), (131, -0.022941086644352737), (132, -0.021586435845444289), (133, -0.0029717400352341269), (134, 0.010549948470399502), (135, 0.0082231182053056941), (136, 0.023696840261601872), (137, -0.0082583722446698746), (138, 0.010110015234767343), (139, 0.0032956070273354311), (140, -0.0081058374540818687), (141, -0.028870560701897523), (142, -0.0059769429852777993), (143, 0.02331050452699265), (144, -0.022686880856468986), (145, -0.0095871358541318071), (146, 0.0059297395285211867), (147, 0.041158958868468321), (148, -0.033641650627379442), (149, 0.026116569480402703), (150, -0.018773241106205721), (151, 0.0089227933210324541), (152, 0.014959893224293806), (153, -0.0022724301299286848), (154, -0.0014927208389285773), (155, -0.040348115649308042), (156, 0.011561010740160105), (157, 0.021507504537273764), (158, 0.051816408081825686), (159, -0.0069307436558428068), (160, -0.047788456181688282), (161, -0.0071024522620179087), (162, -0.022457984394316073), (163, 0.017826356030962401), (164, 0.030407273361791099), (165, 0.0051341975391375923), (166, -0.0055267678687277808), (167, -0.02611194246766909), (168, 0.01483569190284347), (169, -0.009358950673593323), (170, -0.028299534212784042), (171, 0.019652164361056556), (172, 0.033211404470960965), (173, -0.02216890453713154), (174, -0.0031060103780204198), (175, -0.021584387990036071), (176, -0.0025401596594897945), (177, -0.0050205918556739636), (178, 0.015999254092103746), (179, 0.015333264845890584), (180, 0.012167383307294838), (181, 0.015237257265728561), (182, -0.047413609492910506), (183, -0.0088158104265316194), (184, -0.012501888687913751), (185, 0.01594945609431337), (186, 0.0010302469314966856), (187, -0.041283860918469591), (188, -0.0086968052453208142), (189, -0.029906394751002433), (190, -0.013119659056611334), (191, -0.023287513256258673), (192, -0.022418895954017234), (193, -0.015582974695005884), (194, 0.015958583014668782), (195, -0.01530162997177748), (196, -0.015708207778789712), (197, 0.013370658668240522), (198, -0.018386179125739026), (199, 0.021500126938414341)]\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can store this \"LSA via TFIDF via bag-of-words\" corpus the same way again:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cache the transformed corpora to disk, for use in later notebooks\n",
      "%time gensim.corpora.MmCorpus.serialize('./data/wiki_tfidf.mm', tfidf_model[mm_corpus])\n",
      "%time gensim.corpora.MmCorpus.serialize('./data/wiki_lsa.mm', lsi_model[tfidf_model[mm_corpus]])\n",
      "# gensim.corpora.MmCorpus.serialize('./data/wiki_lda.mm', lda_model[mm_corpus])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.mmcorpus:storing corpus in Matrix Market format to ./data/wiki_tfidf.mm\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:saving sparse matrix to ./data/wiki_tfidf.mm\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #2000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #3000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #5000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #6000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #7000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #8000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #9000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #10000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #11000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #12000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #13000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #14000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #15000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #16000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #17000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #18000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #19000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #21000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #22000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #23000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #24000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #25000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #26000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #27000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #28000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #29000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #30000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #31000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #32000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #33000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #34000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #35000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #36000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #37000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #38000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #39000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #40000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #41000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #42000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #43000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #44000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #45000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #46000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #47000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #48000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:saved 48321x26641 matrix, density=0.368% (4739580/1287319761)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.indexedcorpus:saving MmCorpus index to ./data/wiki_tfidf.mm.index\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.mmcorpus:storing corpus in Matrix Market format to ./data/wiki_lsa.mm\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:saving sparse matrix to ./data/wiki_lsa.mm\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #2000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #3000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #5000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #6000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #7000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #8000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #9000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #10000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #11000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #12000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #13000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #14000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #15000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #16000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #17000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #18000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #19000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #21000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #22000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #23000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #24000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #25000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #26000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #27000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #28000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #29000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #30000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #31000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #32000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #33000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #34000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #35000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #36000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #37000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #38000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #39000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #40000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #41000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #42000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #43000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #44000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #45000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #46000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #47000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:PROGRESS: saving document #48000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:saved 48321x200 matrix, density=100.000% (9664199/9664200)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.indexedcorpus:saving MmCorpus index to ./data/wiki_lsa.mm.index\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1min 32s, sys: 1.29 s, total: 1min 33s\n",
        "Wall time: 1min 35s\n",
        "CPU times: user 3min 6s, sys: 2.58 s, total: 3min 9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 3min 13s\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(you can also gzip/bzip2 these `.mm` files to save space, as gensim can work with zipped input transparently)\n",
      "\n",
      "Persisting a transformed corpus to disk makes sense if we want to iterate over it multiple times and the transformation is costly. As before, the saved result is indistinguishable from when it's computed on the fly, so this is effectively a form of \"corpus caching\":"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_corpus = gensim.corpora.MmCorpus('./data/wiki_tfidf.mm')\n",
      "# `tfidf_corpus` is now exactly the same as `tfidf_model[wiki_corpus]`\n",
      "print(tfidf_corpus)\n",
      "\n",
      "lsi_corpus = gensim.corpora.MmCorpus('./data/wiki_lsa.mm')\n",
      "# and `lsi_corpus` now equals `lsi_model[tfidf_model[wiki_corpus]]` = `lsi_model[tfidf_corpus]`\n",
      "print(lsi_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.indexedcorpus:loaded corpus index from ./data/wiki_tfidf.mm.index\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:initializing corpus reader from ./data/wiki_tfidf.mm\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:accepted corpus with 48321 documents, 26641 features, 4739580 non-zero entries\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.corpora.indexedcorpus:loaded corpus index from ./data/wiki_lsa.mm.index\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:initializing corpus reader from ./data/wiki_lsa.mm\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.matutils:accepted corpus with 48321 documents, 200 features, 9664199 non-zero entries\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MmCorpus(48321 documents, 26641 features, 4739580 non-zero entries)\n",
        "MmCorpus(48321 documents, 200 features, 9664199 non-zero entries)\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Transforming unseen documents"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the trained models to transform new, unseen documents into the semantic space:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = \"A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood.\"\n",
      "\n",
      "# transform text into the bag-of-words space\n",
      "bow_vector = id2word_wiki.doc2bow(tokenize(text))\n",
      "print([(id2word_wiki[id], count) for id, count in bow_vector])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'blood', 2), (u'normally', 1), (u'produced', 1), (u'cell', 2)]\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# how to print a topic \n",
      "lda_model.print_topic(9)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# transform into LDA space\n",
      "lda_vector = lda_model[bow_vector]\n",
      "print(lda_vector)\n",
      "# print the document's single most prominent LDA topic\n",
      "print(lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.014287864806863332), (1, 0.014286263065643781), (2, 0.014285879665365256), (3, 0.014286059775723304), (4, 0.31258757490542338), (5, 0.014286128271546196), (6, 0.014287038771997048), (7, 0.014285866032082617), (8, 0.014288794812994349), (9, 0.57311852989236078)]\n",
        "0.007*game + 0.005*species + 0.004*games + 0.004*god + 0.004*person + 0.004*body + 0.004*player + 0.003*cells + 0.003*animals + 0.003*live\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise (5 min)**: print `text` transformed into TFIDF space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "([(10880, 0.6014402368248175),\n",
        "  (18117, 0.3177624409550837),\n",
        "  (21293, 0.24689166054779982),\n",
        "  (22824, 0.6901747464217481)],\n",
        " [(u'blood', 0.6014402368248175),\n",
        "  (u'normally', 0.3177624409550837),\n",
        "  (u'produced', 0.24689166054779982),\n",
        "  (u'cell', 0.6901747464217481)])"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For stacked transformations, apply the same stack during transformation as was applied during training:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# transform into LSI space\n",
      "lsi_vector = lsi_model[tfidf_model[bow_vector]]\n",
      "print(lsi_vector[:10])\n",
      "# print the document's single most prominent LSI topic (not interpretable like LDA!)\n",
      "print(lsi_model.print_topic(max(lsi_vector, key=lambda item: abs(item[1]))[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.023163499634850114), (1, 0.014208138960130777), (2, 0.004123496185336771), (3, 0.024165441160752348), (4, 0.030462234595637037), (5, 0.032573493377941277), (6, 0.0065658401741766539), (7, 0.0003012663597034232), (8, -0.016339102078397401), (9, 0.0018327825657752403)]\n",
        "0.384*\"bridge\" + 0.218*\"song\" + 0.195*\"cells\" + 0.161*\"cell\" + 0.157*\"party\" + -0.137*\"orchestra\" + -0.121*\"god\" + 0.101*\"language\" + 0.097*\"pope\" + -0.096*\"married\"\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model persistence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gensim objects have `save/load` methods for persisting a model to disk, so it can be re-used later (or sent over network to a different computer, or whatever):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# store all trained models to disk\n",
      "lda_model.save('./data/lda_wiki.model')\n",
      "lsi_model.save('./data/lsi_wiki.model')\n",
      "tfidf_model.save('./data/tfidf_wiki.model')\n",
      "id2word_wiki.save('./data/wiki.dictionary')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:saving LdaState object under ./data/lda_wiki.model.state, separately None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:saving LdaModel object under ./data/lda_wiki.model, separately None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:not storing attribute state\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:not storing attribute dispatcher\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:saving Projection object under ./data/lsi_wiki.model.projection, separately None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:saving LsiModel object under ./data/lsi_wiki.model, separately None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:not storing attribute projection\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:not storing attribute dispatcher\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:saving TfidfModel object under ./data/tfidf_wiki.model, separately None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:saving Dictionary object under ./data/wiki.dictionary, separately None\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the same model back; the result is equal to `lda_model`\n",
      "same_lda_model = gensim.models.LdaModel.load('./data/lda_wiki.model')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:loading LdaModel object from ./data/lda_wiki.model\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:setting ignored attribute state to None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:setting ignored attribute dispatcher to None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:gensim.utils:loading LdaModel object from ./data/lda_wiki.model.state\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These methods are optimized for storing large models; internal matrices that consume a lot of RAM are [mmap](http://en.wikipedia.org/wiki/Mmap)'ed in read-only mode. This allows \"sharing\" a single model between several processes, through the OS's virtual memory management."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Topic modeling is an **unsupervised task**; we do not know in advance what the topics ought to look like. This makes evaluation tricky: whereas in supervised learning (classification, regression) we simply compare predicted labels to expected labels, there are no \"expected labels\" in topic modeling.\n",
      "\n",
      "Each topic modeling method (LSI, LDA...) its own way of measuring internal quality (perplexity, reconstruction error...). But these are an artifact of the particular approach taken (bayesian training, matrix factorization...), and mostly of academic interest. There's no way to compare such scores across different types of topic models, either. The best way to really evaluate quality of unsupervised tasks is to **evaluate how they improve the superordinate task, the one we're actually training them for**.\n",
      "\n",
      "For example, when the ultimate goal is to retrieve semantically similar documents, we manually tag a set of similar documents and then see how well a given semantic model maps those similar documents together.\n",
      "\n",
      "Such manual tagging can be resource intensive, so people hae been looking for clever ways to automate it. In [Reading tea leaves: How humans interpret topic models](http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf), Wallach&al suggest a \"word intrusion\" method that works well for models where the topics are meant to be \"human interpretable\", such as LDA. For each trained topic, they take its first ten words, then substitute one of them with another, randomly chosen word (intruder!) and see whether a human can reliably tell which one it was. If so, the trained topic is **topically coherent** (good); if not, the topic has no discernible theme (bad):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# select top 50 words for each of the 20 LDA topics\n",
      "top_words = [[word for _, word in lda_model.show_topic(topicno, topn=50)] for topicno in range(lda_model.num_topics)]\n",
      "print(top_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[u'movie', u'award', u'league', u'bridge', u'movies', u'series', u'film', u'september', u'album', u'star', u'man', u'love', u'released', u'park', u'school', u'children', u'television', u'won', u'club', u'december', u'awards', u'season', u'men', u'october', u'woman', u'penis', u'episode', u'women', u'story', u'light', u'energy', u'nominated', u'doctor', u'premier', u'role', u'played', u'august', u'division', u'sex', u'november', u'sexual', u'black', u'golden', u'million', u'home', u'play', u'stars', u'disney', u'father', u'books'], [u'country', u'capital', u'island', u'tower', u'population', u'largest', u'east', u'language', u'cities', u'countries', u'mast', u'transmission', u'government', u'republic', u'sea', u'land', u'town', u'mario', u'africa', u'million', u'india', u'islands', u'america', u'central', u'river', u'region', u'football', u'county', u'live', u'union', u'western', u'europe', u'languages', u'empire', u'coast', u'school', u'eastern', u'northern', u'spanish', u'team', u'european', u'mountains', u'church', u'divided', u'built', u'international', u'today', u'province', u'southern', u'rivers'], [u'actor', u'singer', u'british', u'writer', u'actress', u'german', u'footballer', u'politician', u'president', u'french', u'player', u'italian', u'musician', u'composer', u'minister', u'prime', u'king', u'ii', u'canadian', u'japanese', u'director', u'january', u'russian', u'george', u'poet', u'movie', u'governor', u'killing', u'general', u'william', u'charles', u'battle', u'james', u'songwriter', u'february', u'robert', u'emperor', u'april', u'painter', u'spanish', u'producer', u'december', u'physicist', u'august', u'france', u'paul', u'leader', u'november', u'july', u'october'], [u'lake', u'bc', u'germany', u'usa', u'ii', u'russia', u'france', u'al', u'rape', u'poland', u'british', u'russian', u'nudity', u'republic', u'german', u'britain', u'islands', u'police', u'europe', u'movie', u'sea', u'independence', u'international', u'lakes', u'president', u'ep', u'king', u'england', u'nude', u'tropical', u'image', u'polish', u'french', u'indonesia', u'singer', u'saint', u'czech', u'kubrick', u'song', u'raped', u'louis', u'names', u'player', u'sri', u'william', u'japan', u'director', u'samurai', u'prison', u'la'], [u'example', u'means', u'things', u'word', u'person', u'language', u'words', u'blood', u'body', u'windows', u'earth', u'change', u'common', u'makes', u'internet', u'water', u'left', u'types', u'uses', u'information', u'right', u'type', u'food', u'human', u'languages', u'object', u'light', u'written', u'computers', u'study', u'temperature', u'form', u'software', u'microsoft', u'data', u'good', u'making', u'help', u'need', u'air', u'milk', u'think', u'heart', u'numbers', u'web', u'comes', u'parts', u'sound', u'program', u'special'], [u'president', u'government', u'party', u'countries', u'germany', u'country', u'rural', u'said', u'political', u'kingdom', u'king', u'house', u'army', u'union', u'book', u'law', u'wanted', u'death', u'took', u'power', u'rights', u'went', u'father', u'york', u'wrote', u'communist', u'empire', u'money', u'al', u'members', u'london', u'election', u'england', u'married', u'children', u'church', u'europe', u'nations', u'democratic', u'began', u'killed', u'elected', u'military', u'lost', u'soviet', u'bush', u'france', u'urban', u'thought', u'ii'], [u'rgb', u'hex', u'color', u'light', u'china', u'blue', u'green', u'red', u'pink', u'purple', u'japan', u'web', u'chinese', u'colors', u'crayola', u'ff', u'yellow', u'april', u'korea', u'violet', u'com', u'japanese', u'orange', u'taiwan', u'fruit', u'february', u'plant', u'brown', u'dark', u'magenta', u'cm', u'white', u'calendar', u'days', u'leaves', u'flowers', u'mol', u'medium', u'http', u'mao', u'plants', u'bear', u'black', u'indigo', u'xona', u'korean', u'caesar', u'lavender', u'flower', u'text'], [u'french', u'german', u'king', u'british', u'italian', u'france', u'ii', u'henry', u'actress', u'actor', u'russian', u'footballer', u'politician', u'roman', u'pope', u'singer', u'england', u'player', u'writer', u'battle', u'composer', u'spanish', u'queen', u'church', u'emperor', u'st', u'prime', u'australia', u'william', u'president', u'dutch', u'saint', u'minister', u'independence', u'poet', u'charles', u'australian', u'governor', u'iii', u'mary', u'scottish', u'musician', u'louis', u'canadian', u'ice', u'forces', u'catholic', u'general', u'anne', u'army'], [u'music', u'band', u'jpg', u'water', u'rock', u'album', u'river', u'file', u'image', u'guitar', u'songs', u'live', u'metal', u'popular', u'song', u'energy', u'lead', u'bass', u'things', u'style', u'birds', u'big', u'air', u'chemical', u'food', u'black', u'vocals', u'albums', u'compounds', u'form', u'sea', u'built', u'good', u'instruments', u'modern', u'making', u'piano', u'wrote', u'white', u'lot', u'art', u'electric', u'pop', u'played', u'common', u'electrons', u'theory', u'play', u'bands', u'type'], [u'game', u'species', u'games', u'god', u'person', u'body', u'player', u'cells', u'animals', u'live', u'fish', u'cell', u'things', u'players', u'living', u'evolution', u'believe', u'human', u'example', u'plants', u'order', u'water', u'abortion', u'types', u'woman', u'means', u'eat', u'play', u'team', u'brain', u'version', u'released', u'earth', u'ball', u'mammals', u'common', u'form', u'disease', u'money', u'dna', u'insects', u'good', u'usb', u'groups', u'women', u'think', u'birth', u'include', u'boy', u'help']]\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get all top 50 words in all 20 topics, as one large set\n",
      "all_words = set(itertools.chain.from_iterable(top_words))\n",
      "\n",
      "print(\"Can you spot the misplaced word in each topic?\")\n",
      "\n",
      "# for each topic, replace a word at a different index, to make it more interesting\n",
      "replace_index = np.random.randint(0, 10, lda_model.num_topics)\n",
      "\n",
      "replacements = []\n",
      "for topicno, words in enumerate(top_words):\n",
      "    other_words = all_words.difference(words)\n",
      "    replacement = np.random.choice(list(other_words))\n",
      "    replacements.append((words[replace_index[topicno]], replacement))\n",
      "    words[replace_index[topicno]] = replacement\n",
      "    print(\"%i: %s\" % (topicno, ' '.join(words[:10])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Can you spot the misplaced word in each topic?\n",
        "0: movie award league bridge movies series film september album ii"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1: country capital island movies population largest east language cities countries\n",
        "2: actor country british writer actress german footballer politician president french\n",
        "3: lake bc northern usa ii russia france al rape poland\n",
        "4: example means things word person al words blood body windows\n",
        "5: president government party countries germany country song said political kingdom\n",
        "6: rgb hex color light china ice green red pink purple\n",
        "7: french german king british italian france ii henry actress coast\n",
        "8: music heart jpg water rock album river file image guitar\n",
        "9: game species games god person body player cells killing live\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Actual replacements were:\")\n",
      "print(list(enumerate(replacements)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Actual replacements were:\n",
        "[(0, (u'star', u'ii')), (1, (u'tower', u'movies')), (2, (u'singer', u'country')), (3, (u'germany', u'northern')), (4, (u'language', u'al')), (5, (u'rural', u'song')), (6, (u'blue', u'ice')), (7, (u'actor', u'coast')), (8, (u'band', u'heart')), (9, (u'animals', u'killing'))]\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also use a different trick, one which doesn't require manual tagging or \"eyeballing\" (resource intensive) and doesn't limit the evaluation to only interpretable models. We'll split each document into two parts, and check that 1) topics of the first half are similar to topics of the second 2) halves of different documents are mostly dissimilar:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# evaluate on 1k documents **not** used in LDA training\n",
      "doc_stream = (tokens for _, tokens in iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2'))  # generator\n",
      "test_docs = list(itertools.islice(doc_stream, 8000, 9000))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def intra_inter(model, test_docs, num_pairs=10000):\n",
      "    # split each test document into two halves and compute topics for each half\n",
      "    part1 = [model[id2word_wiki.doc2bow(tokens[: len(tokens) / 2])] for tokens in test_docs]\n",
      "    part2 = [model[id2word_wiki.doc2bow(tokens[len(tokens) / 2 :])] for tokens in test_docs]\n",
      "    \n",
      "    # print computed similarities (uses cossim)\n",
      "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
      "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
      "\n",
      "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
      "    print(\"average cosine similarity between 10,000 random parts (lower is better):\")    \n",
      "    print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"LDA results:\")\n",
      "intra_inter(lda_model, test_docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LDA results:\n",
        "average cosine similarity between corresponding parts (higher is better):"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.780567068675\n",
        "average cosine similarity between 10,000 random parts (lower is better):\n",
        "0.278323715449"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"LSI results:\")\n",
      "intra_inter(lsi_model, test_docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LSI results:\n",
        "average cosine similarity between corresponding parts (higher is better):"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.607285383134"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "average cosine similarity between 10,000 random parts (lower is better):\n",
        "0.0749900716859"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/sparse/compressed.py:124: UserWarning: indices array has non-integer dtype (float64)\n",
        "  % self.indices.dtype.name )\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook, we saw how to:\n",
      "\n",
      "* create an id => word mapping, aka dictionary\n",
      "* transform a document into a bag-of-word vector, using a dictionary\n",
      "* transform a stream of documents into a stream of vectors\n",
      "* transform between vector streams, using topic models\n",
      "* store and save trained models, for persistency\n",
      "* use manual and semi-automated methods to evaluate quality of a topic model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook, we've used a smallish `simplewiki-20140623-pages-articles.xml.bz2` file, for time reasons. You can run exactly the same code on the full Wikipedia dump too [[BZ2 10.2GB](http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2)] -- the same format is the same. Our streamed approach ensures that RAM footprint of the processing stays constant. There's actually a script in gensim that does all these steps for you, and uses parallelization (multiprocessing) for faster execution, see [Experiments on the English Wikipedia](http://radimrehurek.com/gensim/wiki.html)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Next"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the next notebook, we'll see how to the index semantically transformed corpora and run queries against the index.\n",
      "\n",
      "Continue by opening the next ipython notebook, `3 - Indexing and Retrieval`."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}