{
 "metadata": {
  "name": "",
  "signature": "sha256:0b0cc20682b0092360f0cec52542773f912244e5efc55e33ac912c184ae94337"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Topic Modeling for Fun and Profit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook:\n",
      "\n",
      "* you learn efficient patterns for processing large corpora (streamed processing)\n",
      "* I try to convince you iterators and generators are a useful (and joyful!) tool, not black magic\n",
      "* you write your own streamed processing pipeline for large corpora, incl. basic NLP: collocation detection, lemmatization, stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import and setup modules we'll be using in this notebook\n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "import re\n",
      "import tarfile\n",
      "import itertools\n",
      "\n",
      "import nltk\n",
      "from nltk.collocations import TrigramCollocationFinder\n",
      "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
      "\n",
      "import gensim\n",
      "from gensim.parsing.preprocessing import STOPWORDS\n",
      "\n",
      "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
      "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/admin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/lib/_util.py:34: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data preprocessing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously, we have downloaded the 20newsgroups dataset and left it under `./data/` as a zipped tarball:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls -l ./data/20news-bydate.tar.gz"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-rw-r--r--  1 admin  staff  14464277 Jan 22 17:27 ./data/20news-bydate.tar.gz\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use this file for some topic modeling. Instead of decompressing its files, let's access them directly from Python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with tarfile.open('./data/20news-bydate.tar.gz', 'r:gz') as tf:\n",
      "    # get information (metadata) about all files in the tarball\n",
      "    file_infos = [file_info for file_info in tf if file_info.isfile()]\n",
      "    \n",
      "    # print one of them; for example, the first one\n",
      "    message = tf.extractfile(file_infos[0]).read()\n",
      "    print(message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From: Nanci Ann Miller <nm0w+@andrew.cmu.edu>\n",
        "Subject: Re: Amusing atheists and agnostics\n",
        "Organization: Sponsored account, School of Computer Science, Carnegie Mellon, Pittsburgh, PA\n",
        "Lines: 33\n",
        "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
        "In-Reply-To: <timmbake.735196735@mcl>\n",
        "\n",
        "timmbake@mcl.ucsb.edu (Bake Timmons) writes:\n",
        "> There lies the hypocrisy, dude.  Atheism takes as much faith as theism.  \n",
        "> Admit it!\n",
        "\n",
        "Some people might think it takes faith to be an atheist... but faith in\n",
        "what?  Does it take some kind of faith to say that the Great Invisible Pink\n",
        "Unicorn does not exist?  Does it take some kind of faith to say that Santa\n",
        "Claus does not exist?  If it does (and it may for some people I suppose) it\n",
        "certainly isn't as big a leap of faith to say that these things (and god)\n",
        "DO exist.  (I suppose it depends on your notion and definition of \"faith\".)\n",
        "\n",
        "Besides... not believing in a god means one doesn't have to deal with all\n",
        "of the extra baggage that comes with it!  This leaves a person feeling\n",
        "wonderfully free, especially after beaten over the head with it for years!\n",
        "I agree that religion and belief is often an important psychological healer\n",
        "for many people and for that reason I think it's important.  However,\n",
        "trying to force a psychological fantasy (I don't mean that in a bad way,\n",
        "but that's what it really is) on someone else who isn't interested is\n",
        "extremely rude.  What if I still believed in Santa Claus and said that my\n",
        "belief in Santa did wonderful things for my life (making me a better\n",
        "person, allowing me to live without guilt, etc...) and then tried to get\n",
        "you to believe in Santa too just 'cuz he did so much for me?  You'd call\n",
        "the men in white coats as soon as you could get to a phone.\n",
        "\n",
        "> --\n",
        "> Bake Timmons, III\n",
        "\n",
        "Nanci  (just babbling... :-))\n",
        ".........................................................................\n",
        "If you know (and are SURE of) the author of this quote, please send me\n",
        "email (nm0w+@andrew.cmu.edu):\n",
        "Spring is nature's way of saying, 'Let's party!'\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This text is typical of real-world data. It contains a mix of relevant text, metadata (email headers), and downright noise. Even its relevant content is unstructured, with email addresses, people's names, quotations etc.\n",
      "\n",
      "Most machine learning methods, topic modeling included, are only as good as the data you give it. At this point, we generally want to clean the data as much as possible. While the subsequent steps in the machine learning pipeline are more or less automated, handling the raw data should reflect the intended purpose of the application, its business logic, idiosyncracies, sanity check (aren't we accidentally receiving and parsing *image data* instead of *plain text*?). As always with automated processing it's [garbage in, garbage out](http://en.wikipedia.org/wiki/Garbage_in,_garbage_out)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As an example, let's write a function that aims to extract only the chunk of relevant text from each message, ignoring email headers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_message(message):\n",
      "    \"\"\"\n",
      "    Preprocess a single 20newsgroups message, returning the result as\n",
      "    a unicode string.\n",
      "    \n",
      "    \"\"\"\n",
      "    message = gensim.utils.to_unicode(message, 'latin1').strip()\n",
      "    blocks = message.split(u'\\n\\n')\n",
      "    # skip email headers (first block) and footer (last block)\n",
      "    content = u'\\n\\n'.join(blocks[1:])\n",
      "    return content\n",
      "\n",
      "print process_message(message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "timmbake@mcl.ucsb.edu (Bake Timmons) writes:\n",
        "> There lies the hypocrisy, dude.  Atheism takes as much faith as theism.  \n",
        "> Admit it!\n",
        "\n",
        "Some people might think it takes faith to be an atheist... but faith in\n",
        "what?  Does it take some kind of faith to say that the Great Invisible Pink\n",
        "Unicorn does not exist?  Does it take some kind of faith to say that Santa\n",
        "Claus does not exist?  If it does (and it may for some people I suppose) it\n",
        "certainly isn't as big a leap of faith to say that these things (and god)\n",
        "DO exist.  (I suppose it depends on your notion and definition of \"faith\".)\n",
        "\n",
        "Besides... not believing in a god means one doesn't have to deal with all\n",
        "of the extra baggage that comes with it!  This leaves a person feeling\n",
        "wonderfully free, especially after beaten over the head with it for years!\n",
        "I agree that religion and belief is often an important psychological healer\n",
        "for many people and for that reason I think it's important.  However,\n",
        "trying to force a psychological fantasy (I don't mean that in a bad way,\n",
        "but that's what it really is) on someone else who isn't interested is\n",
        "extremely rude.  What if I still believed in Santa Claus and said that my\n",
        "belief in Santa did wonderful things for my life (making me a better\n",
        "person, allowing me to live without guilt, etc...) and then tried to get\n",
        "you to believe in Santa too just 'cuz he did so much for me?  You'd call\n",
        "the men in white coats as soon as you could get to a phone.\n",
        "\n",
        "> --\n",
        "> Bake Timmons, III\n",
        "\n",
        "Nanci  (just babbling... :-))\n",
        ".........................................................................\n",
        "If you know (and are SURE of) the author of this quote, please send me\n",
        "email (nm0w+@andrew.cmu.edu):\n",
        "Spring is nature's way of saying, 'Let's party!'\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Feel free to modify this function and test out other ideas for clean up. The flexibility Python gives you in processing text is superb -- [it'd be a crime](http://radimrehurek.com/2014/03/data-streaming-in-python-generators-iterators-iterables/) to hide the processing behind opaque APIs, exposing only one or two tweakable parameters.\n",
      "\n",
      "There are a handful of handy Python libraries for text cleanup: [jusText](https://github.com/miso-belica/jusText) removes HTML boilerplate and extracts \"main text\" of a web page. [NLTK](http://www.nltk.org/), [Pattern](http://www.clips.ua.ac.be/pattern) and [TextBlob](http://textblob.readthedocs.org/en/dev/) are good for tokenization, POS tagging, sentence splitting and generic NLP, with a nice Pythonic interface. None of them scales very well though, so keep the inputs small."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise (5 min)**: Modify the `process_message` function to ignore message footers, too."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's a good practice to inspect your data visually, at each point as it passes through your data processing pipeline. Simple printing (logging) a few arbitrary entries, ala UNIX `head`, does wonders for spotting unexpected bugs. *Oh, bad encoding! What is Chinese doing there, we were told all texts are English only? Do these rubbish tokens come from embedded images? How come everything's empty?* Taking a text with \"hey, let's tokenize it into a bag of words blindly, like they do in the tutorials, push it through this magical unicorn machine learning library and hope for the best\" is ill advised.\n",
      "\n",
      "Another good practice is to keep internal strings as Unicode, and only encode/decode on IO (preferably using UTF8). As of Python 3.3, there is practically no memory penalty for using Unicode over UTF8 byte strings ([PEP 393](http://legacy.python.org/dev/peps/pep-0393/))."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data streaming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Let's write a function to go over all messages in the 20newsgroups archive:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def iter_20newsgroups(fname, log_every=None):\n",
      "    \"\"\"\n",
      "    Yield plain text of each 20 newsgroups message, as a unicode string.\n",
      "\n",
      "    The messages are read from raw tar.gz file `fname` on disk (e.g. `./data/20news-bydate.tar.gz`)\n",
      "\n",
      "    \"\"\"\n",
      "    extracted = 0\n",
      "    with tarfile.open(fname, 'r:gz') as tf:\n",
      "        for file_number, file_info in enumerate(tf):\n",
      "            if file_info.isfile():\n",
      "                if log_every and extracted % log_every == 0:\n",
      "                    logging.info(\"extracting 20newsgroups file #%i: %s\" % (extracted, file_info.name))\n",
      "                content = tf.extractfile(file_info).read()\n",
      "                yield process_message(content)\n",
      "                extracted += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This uses the `process_message()` we wrote above, to process each message in turn. The messages are extracted on-the-fly, one after another, using a generator.\n",
      "\n",
      "Such **data streaming** is a very important pattern: real data is typically too large to fit into RAM, and we don't need all of it in RAM at the same time anyway -- that's just wasteful. With streamed data, we can process arbitrarily large input, reading the data from a file on disk, SQL database, shared network disk, or even more exotic remote network protocols."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# itertools is an inseparable friend with data streaming (Python built-in library)\n",
      "import itertools\n",
      "\n",
      "# let's only parse and print the first three messages, lazily\n",
      "# `list(stream)` materializes the stream elements into a plain Python list\n",
      "message_stream = iter_20newsgroups('./data/20news-bydate.tar.gz', log_every=2)\n",
      "print(list(itertools.islice(message_stream, 3)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:extracting 20newsgroups file #0: 20news-bydate-test/alt.atheism/53265\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:extracting 20newsgroups file #2: 20news-bydate-test/alt.atheism/53260\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'timmbake@mcl.ucsb.edu (Bake Timmons) writes:\\n> There lies the hypocrisy, dude.  Atheism takes as much faith as theism.  \\n> Admit it!\\n\\nSome people might think it takes faith to be an atheist... but faith in\\nwhat?  Does it take some kind of faith to say that the Great Invisible Pink\\nUnicorn does not exist?  Does it take some kind of faith to say that Santa\\nClaus does not exist?  If it does (and it may for some people I suppose) it\\ncertainly isn\\'t as big a leap of faith to say that these things (and god)\\nDO exist.  (I suppose it depends on your notion and definition of \"faith\".)\\n\\nBesides... not believing in a god means one doesn\\'t have to deal with all\\nof the extra baggage that comes with it!  This leaves a person feeling\\nwonderfully free, especially after beaten over the head with it for years!\\nI agree that religion and belief is often an important psychological healer\\nfor many people and for that reason I think it\\'s important.  However,\\ntrying to force a psychological fantasy (I don\\'t mean that in a bad way,\\nbut that\\'s what it really is) on someone else who isn\\'t interested is\\nextremely rude.  What if I still believed in Santa Claus and said that my\\nbelief in Santa did wonderful things for my life (making me a better\\nperson, allowing me to live without guilt, etc...) and then tried to get\\nyou to believe in Santa too just \\'cuz he did so much for me?  You\\'d call\\nthe men in white coats as soon as you could get to a phone.\\n\\n> --\\n> Bake Timmons, III\\n\\nNanci  (just babbling... :-))\\n.........................................................................\\nIf you know (and are SURE of) the author of this quote, please send me\\nemail (nm0w+@andrew.cmu.edu):\\nSpring is nature\\'s way of saying, \\'Let\\'s party!\\'', u'Did that FAQ ever got modified to re-define strong atheists as not those who\\nassert the nonexistence of God, but as those who assert that they BELIEVE in \\nthe nonexistence of God?  There was a thread on this earlier, but I didn\\'t get\\nthe outcome...\\n\\n-- Adam \"No Nickname\" Cooper\\n\\n\\n\\n********************************************************************************\\n* Adam John Cooper\\t\\t\"Verily, often have I laughed at the weaklings *\\n* (612) 696-7521\\t\\t   who thought themselves good simply because  *\\n* acooper@macalstr.edu\\t\\t\\t\\tthey had no claws.\"\\t       *\\n********************************************************************************', u'jaeger@buphy.bu.edu (Gregg Jaeger) writes:\\n>In article <11847@vice.ICO.TEK.COM> bobbe@vice.ICO.TEK.COM (Robert\\n>Beauchaine) writes:\\n>>Bennett, Neil.  \"How BCCI adapted the Koran rules of banking\".  The \\n>>Times.  August 13, 1991.\\n> \\n> So, let\\'s see. If some guy writes a piece with a title that implies\\n> something is the case then it must be so, is that it?\\n\\nGregg, you haven\\'t provided even a title of an article to support *your*\\ncontention.\\n\\n>>  This is how you support a position if you intend to have anyone\\n>>  respect it, Gregg.  Any questions?  And I even managed to include\\n>>  the above reference with my head firmly engaged in my ass.  What\\'s\\n>>  your excuse?\\n> \\n> This supports nothing. I have no reason to believe that this is \\n> piece is anything other than another anti-Islamic slander job.\\n\\nYou also have no reason to believe it *is* an anti-Islamic slander job, apart\\nfrom your own prejudices.\\n\\n> I have no respect for titles, only for real content. I can look\\n> up this article if I want, true. But I can tell you BCCI was _not_\\n> an Islamic bank.\\n\\nWhy, yes.  What\\'s a mere report in The Times stating that BCCI followed\\nIslamic banking rules?  Gregg *knows* Islam is good, and he *knows* BCCI were\\nbad, therefore BCCI *cannot* have been Islamic.  Anyone who says otherwise is\\nobviously spreading slanderous propaganda.\\n\\n>                                      If someone wants to discuss\\n> the issue more seriously then I\\'d be glad to have a real discussion,\\n> providing references, etc.\\n\\nI see.  If someone wants to provide references to articles you agree with,\\nyou will also respond with references to articles you agree with?  Mmm, yes,\\nthat would be a very intellectually stimulating debate.  Doubtless that\\'s how\\nyou spend your time in soc.culture.islam.\\n\\nI\\'ve got a special place for you in my...\\n\\x0c\\n...kill file.  Right next to Bobby.  Want to join him?\\n\\nThe more you post, the more I become convinced that it is simply a waste of\\ntime to try and reason with Moslems.  Is that what you are hoping to achieve?\\n\\n\\nmathew']\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note:** Data streaming saves memory, but does nothing to save time. A common pattern for speeding up data processing is parallelization, via Python's multi-processing and multi-threading support. We don't have time to cover processing parallelization or cluster distribution in this tutorial. For an example, see [this Wikipedia parsing code](https://github.com/piskvorky/sim-shootout/blob/master/prepare_shootout.py#L48) from my [Similarity Shootout benchmark](http://radimrehurek.com/2013/12/performance-shootout-of-nearest-neighbours-contestants/)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A generator only gives us a single pass through the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print the next two messages; the three messages printed above are already gone\n",
      "print(list(itertools.islice(message_stream, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:root:extracting 20newsgroups file #4: 20news-bydate-test/alt.atheism/53333\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'In article <93Apr20.035421edt.47719@neat.cs.toronto.edu>, tgk@cs.toronto.edu (Todd Kelley) writes:\\n> In light of what happened in Waco, I need to get something of my\\n> chest.\\n\\nSadly understandable...\\n\\n> \\n> Faith and dogma are dangerous.  \\n\\nYes.\\n\\n> \\n> Religion inherently encourages the implementation of faith and dogma, and\\n> for that reason, I scorn religion.\\n> \\nTo be fair, you should really qualify this as semitic-western religions, but\\nyou basically go ahead and do this later on anyway.\\n\\n> I have expressed this notion in the past.  Some Christians debated\\n> with me whether Christianity leaves any room for reasoning.  I claimed\\n> rationality is quelled out of Christianity by faith and dogma.\\n\\nAgain, this should really be evaluated at a personal level.  For example, there\\nwas only one Jesus (presumably), and he probably didn\\'t say all that many\\nthings, and yet (seemingly) billions and billions of Christian sects have\\narisen.  Perhaps there is one that is totally dedicated to rationalism and\\nbelieves in Christ as in pantheism.  It would seem to go against the Bible, but\\nit is amazing what people come up with under the guise of \"personal\\ninterpretation\".\\n\\n> A philosopher cannot be a Christian because a philosopher can change his mind,\\n> whereas a Christian cannot, due to the nature of faith and dogma present\\n> in any religion.\\n\\nThis is a good point.  We have here the quintessential Christian: he sets up a\\nsystem of values/beliefs for himself, which work very well, and every\\nevent/experience is understandable and deablable within the framework of this\\nsystem.  However, we also have an individual who has the inability (at least\\nnot without some difficulty) to change, which is important, because the problem\\nwith such a system is the same as with any system: one cannot be open minded to\\nthe point of \"testing hypotheses\" against the basic premise of the system\\nwithout destroying whatever faith is invested therein, unless of course, all\\nthe tests fail.  In other words, the *fairer* way would be to test and evaluate\\nmoralities without the bias/responsibility of losing/retaining a system.\\n\\n> \\n> I claimed that a ``Christian philosopher\\'\\' is not a Christian,\\n> but is a person whose beliefs at the moment correspond with those\\n> of Christianity. Consider that a person visiting or guarding a prison\\n> is not a prisoner, unless you define a prisoner simply to be someone\\n> in a prison.\\n> Can we define a prisoner to be someone who at the moment is in a prison?\\n> Can we define a Christian to be someone who at the moment has Christian\\n> beliefs?  No, because if a person is free to go, he is not a prisoner.\\n> Similarly, if a person is not constrained by faith and dogma, he is not\\n> a Christian.\\n\\nInteresting, but again, when it seems to basically boil down to individual\\nnuances (although not always, I will admit, and probably it is the\\nmass-oriented divisions which are the most appalling), it becomes irrelevant,\\nunfortunately.\\n\\n> \\n> I admit it\\'s a word game.\\n> I\\'m going by the dictionary definition of religion:\\n>    ``religion n. 1. concern over what exists beyond the visible world,\\n>      differentiated from philosophy in that it operates through faith\\n>      or intuition rather than reason, ...\\'\\'\\n>                                    --Webster\\'s\\n> \\n> Now let\\'s go beyond the word game.  I don\\'t claim that religion\\n> causes genocide.  I think that if all humans were atheist, there\\n> would still be genocide.  There will always be humans who don\\'t think.\\n> There will always be humans who don\\'t ask themselves what is\\n> the REAL difference between themselves and people with different\\n> colored skin, or a different language, or different beliefs.\\n> \\n\\nGranted\\n\\n> Religion is like the gun that doesn\\'t kill anybody.  Religion encourages\\n> faith and dogma and although it doesn\\'t directly condemn people,\\n> it encourages the use of ``just because\\'\\' thinking.  It is\\n> ``just because\\'\\' thinking that kills people.\\n> \\n\\nIn which case the people become the bullets, and the religion, as the gun,\\nmerely offers them a way to more adequately do some harm with themselves, if I\\nmay be so bold as to extend your similie?\\n\\n> Sure, religion has many good qualities.  It encourages benevolence\\n> and philanthropy.  OK, so take out only the bad things: like faith,\\n> dogma, and tradition.  Put in the good things, like careful reasoning,\\n> and science.  The result is secular humanism.  Wouldn\\'t it\\n> be nice if everyone were a secular humanist?   To please the\\n> supernaturalists, you might even leave God in there, but the secular\\n> emphasis would cause the supernaturalists to start thinking, and\\n> they too would realize that a belief in a god really doesn\\'t put\\n> anyone further ahead in understanding the universe (OK, I\\'m just\\n> poking fun at the supernaturalists :-).\\n\\nAlso understandable... ;)\\n\\n> \\n> Of course, not all humans are capable of thought, and we\\'d still\\n> have genocide and maybe even some mass suicide...but not as much.\\n> I\\'m willing to bet on that.\\n> \\n> Todd\\n> -- \\n> Todd Kelley                       tgk@cs.toronto.edu\\n> Department of Computer Science\\n> University of Toronto\\n-- \\n\\nbest regards,\\n\\n\\n********************************************************************************\\n* Adam John Cooper\\t\\t\"Verily, often have I laughed at the weaklings *\\n* (612) 696-7521\\t\\t   who thought themselves good simply because  *\\n* acooper@macalstr.edu\\t\\t\\t\\tthey had no claws.\"\\t       *\\n********************************************************************************', u'In article <930420.113512.1V3.rusnews.w165w@mantis.co.uk>, mathew <mathew@mantis.co.uk> writes:\\n> mccullou@snake2.cs.wisc.edu (Mark McCullough) writes:\\n\\nFrom a parallel thread.  Much about definitions of bombs, etc. deleted.\\n[...]\\n\\n> \\n>> Aaaahhh.  Tell me, how many innocents were killed in concentration camps?\\n>> mm-hmm.  Now, how many more were scheduled to enter concentration camps\\n>> had they not been shut down because they were captured by the allies?\\n>> mm-hmm.  Now, civilians died in that war.  So no matter what you do,\\n>> civilians die.  What is the proper course?\\n> \\n> Don\\'t sell the bastard arms and information in the first place.  Ruthlessly\\n> hunt down those who do.  Especially if they\\'re in positions of power.\\n> \\n\\nMathew, I agree.  This, it seems, is the crux of your whole position,\\nisn\\'t it?  That the US shouldn\\'t have supported Hussein and sold him arms\\nto fight Iran?  I agree.  And I agree in ruthlessly hunting down those\\nwho did or do.  But we *did* sell arms to Hussein, and it\\'s a done deal.\\nNow he invades Kuwait.  So do we just sit back and say, \"Well, we sold\\nhim all those arms, I suppose he just wants to use them now.  Too bad\\nfor Kuwait.\"  No, unfortunately, sitting back and \"letting things be\"\\nis not the way to correct a former mistake.  Destroying Hussein\\'s\\nmilitary potential as we did was the right move.  But I agree with\\nyour statement, Reagan and Bush made a grave error in judgment to\\nsell arms to Hussein.  So it\\'s really not the Gulf War you abhor\\nso much, it was the U.S.\\'s and the West\\'s shortsightedness in selling\\narms to Hussein which ultimately made the war inevitable, right?\\n\\nIf so, then I agree.\\n\\n[more deleted.]\\n> \\n> mathew\\n\\nRegards,\\n\\nJim B.']\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's wrap the generator inside an object's `__iter__` method, so we can iterate over the stream multiple times:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Corpus20News(object):\n",
      "    def __init__(self, fname):\n",
      "        self.fname = fname\n",
      "\n",
      "    def __iter__(self):\n",
      "        for text in iter_20newsgroups(self.fname):\n",
      "            # tokenize each message; simply lowercase & match alphabetic chars, for now\n",
      "            yield list(gensim.utils.tokenize(text, lower=True))\n",
      "\n",
      "tokenized_corpus = Corpus20News('./data/20news-bydate.tar.gz')\n",
      "\n",
      "# print the first two tokenized messages\n",
      "print(list(itertools.islice(tokenized_corpus, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the same two tokenized messages (not the next two!)\n",
      "# each call to __iter__ \"resets\" the stream, by creating a new generator object internally\n",
      "print(list(itertools.islice(tokenized_corpus, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Text processing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Lemmatization, stemming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Lemmatization](http://en.wikipedia.org/wiki/Lemmatisation) is type of normalization that treats different inflected forms of a word as a single unit (\"work\", \"working\", \"works\", \"worked\", \"working\" => same lemma: \"work\"):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "\n",
      "print(gensim.utils.lemmatize(\"worked\"))\n",
      "print(gensim.utils.lemmatize(\"working\"))\n",
      "print(gensim.utils.lemmatize(\"I was working with a working class hero.\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['work/VB']\n",
        "['work/VB']\n",
        "['be/VB', 'work/VB', 'working/JJ', 'class/NN', 'hero/NN']\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There's a part of speech (POS) tag included in each token: lemma/POS. Note how articles and prepositions, such as \"The\", \"a\", or \"over\", have been filtered out from the result. Only word categories that traditionally carry the most meaning, such as nouns, adjectives and verbs, are left. Gensim uses the [pattern](http://www.clips.ua.ac.be/pattern) library internally, because its lemmatization performs (much) better than alternatives such as NLTK."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Corpus20News_Lemmatize(object):\n",
      "    def __init__(self, fname):\n",
      "        self.fname = fname\n",
      "\n",
      "    def __iter__(self):\n",
      "        for message in iter_20newsgroups(self.fname):\n",
      "            yield self.tokenize(message)\n",
      "\n",
      "    def tokenize(self, text):\n",
      "        \"\"\"Break text into a list of lemmatized words.\"\"\"\n",
      "        return gensim.utils.lemmatize(text)\n",
      "    \n",
      "lemmatized_corpus = Corpus20News_Lemmatize('./data/20news-bydate.tar.gz')\n",
      "print(list(itertools.islice(lemmatized_corpus, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['timmbake/NN', 'mcl/NN', 'ucsb/NN', 'edu/NN', 'bake/JJ', 'timmon/NN', 'write/VB', 'there/RB', 'lie/VB', 'hypocrisy/NN', 'dude/NN', 'atheism/NN', 'take/VB', 'as/RB', 'much/JJ', 'faith/NN', 'theism/NN', 'admit/VB', 'person/NN', 'think/VB', 'take/VB', 'faith/NN', 'be/VB', 'atheist/JJ', 'faith/NN', 'do/VB', 'take/VB', 'kind/NN', 'faith/NN', 'say/VB', 'great/JJ', 'invisible/JJ', 'pink/JJ', 'unicorn/NN', 'do/VB', 'not/RB', 'exist/NN', 'do/VB', 'take/VB', 'kind/NN', 'faith/NN', 'say/VB', 'santa/NN', 'claus/NN', 'do/VB', 'not/RB', 'exist/VB', 'do/VB', 'person/NN', 'suppose/VB', 'certainly/RB', 'isn/JJ', 'big/JJ', 'leap/NN', 'faith/NN', 'say/VB', 'thing/NN', 'god/NN', 'do/VB', 'exist/VB', 'suppose/VB', 'depend/VB', 'notion/NN', 'definition/NN', 'faith/NN', 'not/RB', 'believe/VB', 'god/NN', 'mean/VB', 'doesn/JJ', 'have/VB', 'deal/VB', 'extra/JJ', 'baggage/NN', 'come/VB', 'leave/VB', 'person/NN', 'feel/VB', 'wonderfully/RB', 'free/JJ', 'especially/RB', 'beaten/JJ', 'head/NN', 'year/NN', 'agree/VB', 'religion/NN', 'belief/NN', 'be/VB', 'often/RB', 'important/JJ', 'psychological/JJ', 'healer/NN', 'many/JJ', 'person/NN', 'reason/NN', 'think/VB', 'important/JJ', 'however/RB', 'try/VB', 'force/VB', 'psychological/JJ', 'fantasy/NN', 'don/VB', 'mean/VB', 'bad/JJ', 'way/NN', 'really/RB', 'be/VB', 'someone/NN', 'else/RB', 'isn/VB', 'interested/JJ', 'be/VB', 'extremely/RB', 'rude/JJ', 'still/RB', 'believe/VB', 'santa/NN', 'claus/NN', 'say/VB', 'belief/NN', 'santa/NN', 'do/VB', 'wonderful/JJ', 'thing/NN', 'life/NN', 'make/VB', 'better/JJ', 'person/NN', 'allow/VB', 'live/VB', 'guilt/NN', 'etc/NN', 'then/RB', 'try/VB', 'get/VB', 'believe/VB', 'santa/NN', 'too/RB', 'just/RB', 'do/VB', 'so/RB', 'much/JJ', 'call/VB', 'man/NN', 'white/JJ', 'coat/NN', 'soon/RB', 'get/VB', 'phone/NN', 'bake/JJ', 'timmon/NN', 'iii/NN', 'nanci/RB', 'just/RB', 'babble/VB', 'know/VB', 'be/VB', 'sure/JJ', 'author/NN', 'please/VB', 'send/VB', 'email/NN', 'nm/NN', 'andrew/NN', 'cmu/VB', 'edu/JJ', 'spring/NN', 'be/VB', 'nature/NN', 'way/NN', 'say/VB', 'let/VB', 'party/NN'], ['do/VB', 'faq/NN', 'ever/RB', 'get/VB', 'modify/VB', 're/NN', 'define/VB', 'strong/JJ', 'atheist/NN', 'not/RB', 'assert/VB', 'nonexistence/NN', 'god/NN', 'assert/VB', 'believe/VB', 'nonexistence/NN', 'god/NN', 'be/VB', 'thread/NN', 'earlier/RB', 'didn/VB', 'get/VB', 'outcome/NN', 'nickname/NN', 'cooper/NN', 'adam/NN', 'john/NN', 'cooper/NN', 'verily/NN', 'often/RB', 'have/VB', 'laugh/VB', 'weakling/NN', 'think/VB', 'good/JJ', 'simply/RB', 'acooper/NN', 'macalstr/NN', 'edu/NN', 'have/VB', 'claw/NN']]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise (10 min)**: Modify `tokenize()` to ignore (=not return) generic words, such as \"do\", \"then\", \"be\", \"as\"... These are called stopwords and we may want to remove them because some topic modeling algorithms are sensitive to their presence. An example of common stopwords set for English is in `from gensim.parsing.preprocessing import STOPWORDS`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Collocations and Named Entity Recognition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Collocation](http://en.wikipedia.org/wiki/Collocation) is a \"sequence of words or terms that co-occur more often than would be expected by chance.\"\n",
      "\n",
      "[Named entity recognition (NER)](http://en.wikipedia.org/wiki/Named-entity_recognition) is the task of locating chunks of text that refer to people, locations, organizations etc.\n",
      "\n",
      "Detecting collocations and named entities often has a significant business value: \"General Electric\" stays a single entity (token), rather than two words \"general\" and \"electric\". Same with \"Marathon Petroleum\", \"George Bush\" etc -- a topic model doesn't confuse its topics via words coming from unrelated entities, such as \"Korea\" and \"Carolina\" via \"North\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.collocations import TrigramCollocationFinder\n",
      "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
      "\n",
      "def best_ngrams(words, top_n=1000, min_freq=100):\n",
      "    \"\"\"\n",
      "    Extract `top_n` most salient collocations (bigrams and trigrams),\n",
      "    from a stream of words. Ignore collocations with frequency\n",
      "    lower than `min_freq`.\n",
      "\n",
      "    This fnc uses NLTK for the collocation detection itself -- not very scalable!\n",
      "\n",
      "    Return the detected ngrams as compiled regular expressions, for their faster\n",
      "    detection later on.\n",
      "\n",
      "    \"\"\"\n",
      "    tcf = TrigramCollocationFinder.from_words(words)\n",
      "    tcf.apply_freq_filter(min_freq)\n",
      "    trigrams = [' '.join(w) for w in tcf.nbest(TrigramAssocMeasures.chi_sq, top_n)]\n",
      "    logging.info(\"%i trigrams found: %s...\" % (len(trigrams), trigrams[:20]))\n",
      "\n",
      "    bcf = tcf.bigram_finder()\n",
      "    bcf.apply_freq_filter(min_freq)\n",
      "    bigrams = [' '.join(w) for w in bcf.nbest(BigramAssocMeasures.pmi, top_n)]\n",
      "    logging.info(\"%i bigrams found: %s...\" % (len(bigrams), bigrams[:20]))\n",
      "\n",
      "    pat_gram2 = re.compile('(%s)' % '|'.join(bigrams), re.UNICODE)\n",
      "    pat_gram3 = re.compile('(%s)' % '|'.join(trigrams), re.UNICODE)\n",
      "\n",
      "    return pat_gram2, pat_gram3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.parsing.preprocessing import STOPWORDS\n",
      "\n",
      "class Corpus20News_Collocations(object):\n",
      "    def __init__(self, fname):\n",
      "        self.fname = fname\n",
      "        logging.info(\"collecting ngrams from %s\" % self.fname)\n",
      "        # generator of documents; one element = list of words\n",
      "        documents = (self.split_words(text) for text in iter_20newsgroups(self.fname, log_every=1000))\n",
      "        # generator: concatenate (chain) all words into a single sequence, lazily\n",
      "        words = itertools.chain.from_iterable(documents)\n",
      "        self.bigrams, self.trigrams = best_ngrams(words)\n",
      "\n",
      "    def split_words(self, text, stopwords=STOPWORDS):\n",
      "        \"\"\"\n",
      "        Break text into a list of single words. Ignore any token that falls into\n",
      "        the `stopwords` set.\n",
      "\n",
      "        \"\"\"\n",
      "        return [word\n",
      "                for word in gensim.utils.tokenize(text, lower=True)\n",
      "                if word not in STOPWORDS and len(word) > 3]\n",
      "\n",
      "    def tokenize(self, message):\n",
      "        \"\"\"\n",
      "        Break text (string) into a list of Unicode tokens.\n",
      "        \n",
      "        The resulting tokens can be longer phrases (collocations) too,\n",
      "        e.g. `new_york`, `real_estate` etc.\n",
      "\n",
      "        \"\"\"\n",
      "        text = u' '.join(self.split_words(message))\n",
      "        text = re.sub(self.trigrams, lambda match: match.group(0).replace(u' ', u'_'), text)\n",
      "        text = re.sub(self.bigrams, lambda match: match.group(0).replace(u' ', u'_'), text)\n",
      "        return text.split()\n",
      "\n",
      "    def __iter__(self):\n",
      "        for message in iter_20newsgroups(self.fname):\n",
      "            yield self.tokenize(message)\n",
      "\n",
      "%time collocations_corpus = Corpus20News_Collocations('./data/20news-bydate.tar.gz')\n",
      "print(list(itertools.islice(collocations_corpus, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead of detecting collocations by frequency, we can run (shallow) syntactic parsing. This tags each word with its part-of-speech (POS) category, and suggests phrases based on chunks of \"noun phrases\":"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def head(stream, n=10):\n",
      "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
      "    return list(itertools.islice(stream, n))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# if you get an error about textblob missing data files, run the following:\n",
      "# `import nltk; nltk.download('brown')`\n",
      "\n",
      "from textblob import TextBlob\n",
      "\n",
      "def best_phrases(document_stream, top_n=1000, prune_at=50000):\n",
      "    \"\"\"Return a set of `top_n` most common noun phrases.\"\"\"\n",
      "    np_counts = {}\n",
      "    for docno, doc in enumerate(document_stream):\n",
      "        # prune out infrequent phrases from time to time, to save RAM.\n",
      "        # the result may not be completely accurate because of this step\n",
      "        if docno % 1000 == 0:\n",
      "            sorted_phrases = sorted(np_counts.iteritems(), key=lambda item: -item[1])\n",
      "            np_counts = dict(sorted_phrases[:prune_at])\n",
      "            logging.info(\"at document #%i, considering %i phrases: %s...\" %\n",
      "                         (docno, len(np_counts), head(sorted_phrases)))\n",
      "        \n",
      "        # how many times have we seen each noun phrase?\n",
      "        for np in TextBlob(doc).noun_phrases:\n",
      "            # only consider multi-word NEs where each word contains at least one letter\n",
      "            if u' ' not in np:\n",
      "                continue\n",
      "            # ignore phrases that contain too short/non-alphabetic words\n",
      "            if all(word.isalpha() and len(word) > 2 for word in np.split()):\n",
      "                np_counts[np] = np_counts.get(np, 0) + 1\n",
      "\n",
      "    sorted_phrases = sorted(np_counts, key=lambda np: -np_counts[np])\n",
      "    return set(head(sorted_phrases, top_n))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Corpus20News_NE(object):\n",
      "    def __init__(self, fname):\n",
      "        self.fname = fname\n",
      "        logging.info(\"collecting entities from %s\" % self.fname)\n",
      "        doc_stream = itertools.islice(iter_20newsgroups(self.fname), 10000)\n",
      "        self.entities = best_phrases(doc_stream)\n",
      "        logging.info(\"selected %i entities: %s...\" %\n",
      "                     (len(self.entities), list(self.entities)[:10]))\n",
      "\n",
      "    def __iter__(self):\n",
      "        for message in iter_20newsgroups(self.fname):\n",
      "            yield self.tokenize(message)\n",
      "\n",
      "    def tokenize(self, message, stopwords=STOPWORDS):\n",
      "        \"\"\"\n",
      "        Break text (string) into a list of Unicode tokens.\n",
      "        \n",
      "        The resulting tokens can be longer phrases (named entities) too,\n",
      "        e.g. `new_york`, `real_estate` etc.\n",
      "\n",
      "        \"\"\"\n",
      "        result = []\n",
      "        for np in TextBlob(message).noun_phrases:\n",
      "            if u' ' in np and np not in self.entities:\n",
      "                # only consider multi-word phrases we detected in the constructor\n",
      "                continue\n",
      "            token = u'_'.join(part for part in gensim.utils.tokenize(np) if len(part) > 2)\n",
      "            if len(token) < 4 or token in stopwords:\n",
      "                # ignore very short phrases and stop words\n",
      "                continue\n",
      "            result.append(token)\n",
      "        return result\n",
      "\n",
      "%time ne_corpus = Corpus20News_NE('./data/20news-bydate.tar.gz')\n",
      "print(head(ne_corpus, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Too much work?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What we've just done, cleaning up raw input as the first step to more advanced processing, is actually the most \"varied\" and the most challenging part of building up machine learning pipelines (along with the last step on the other end of the pipeline: evaluation).\n",
      "\n",
      "You typically have to know what overall goal you're trying to achieve to choose the correct preprocessing+evaluation approach. There is no \"one best way\" to preprocess text -- different applications require different steps, all the way down to custom tokenizers and lemmatizers. This is especially true for other (non-English) languages. Always log liberally and check the output coming out of your pipeline at various steps, to spot potential unforeseen problems.\n",
      "\n",
      "Now that we have the data in a common format & ready to be vectorized, subsequent notebooks will be more straightforward & run of the mill."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Flow of data preparation:\n",
      "\n",
      "* extract a text stream from (raw) data\n",
      "* clean up texts, depending on business logic\n",
      "* break sanitized text into features of interest: words, collocations, detect named entities...\n",
      "* keep the data flow streamed and flexible\n",
      "* sprinkle code with sanity prints & checks (DEBUG/INFO logs)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Next"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the next notebook, we'll learn how to plug such preprocessed data streams into gensim, a library for topic modeling and information retrieval.\n",
      "\n",
      "Continue with opening the next ipython notebook, `2 - Topic Modeling`."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}